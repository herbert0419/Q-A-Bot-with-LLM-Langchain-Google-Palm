{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc355d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.0.284\n",
    "# !pip install google-generativeai\n",
    "# !pip install pypdf\n",
    "# !pip install InstructorEmbedding\n",
    "# !pip install transformers\n",
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64e75642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "api_key = \"AIzaSyCS-4Z5Enu_t4umJN1uopHPv5b9ZB8snZQ\"\n",
    "\n",
    "llm = GooglePalm(google_api_key=api_key, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe85cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = llm(\"What is the importance of statistics in Data Science?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96009b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### The Importance of Statistics in Data Science\\n\\nData science is a field that is rapidly growing in importance, as businesses and organizations of all types are increasingly looking to make use of the vast amounts of data that they collect. In order to make effective use of data, it is necessary to have a strong understanding of statistics.\\n\\nStatistics is the science of collecting, organizing, and analyzing data in order to draw conclusions about the population from which the data was collected. This can be used to make predictions about future events, identify trends, and understand the relationships between different variables.\\n\\nStatistics is essential for data science because it provides the tools and techniques that are needed to make sense of the vast amounts of data that are collected. Without statistics, it would be impossible to identify the patterns and trends that are hidden in the data, and it would be difficult to make informed decisions about how to use the data.\\n\\nHere are some of the specific ways that statistics is used in data science:\\n\\n* **To identify patterns and trends:** Statistics can be used to identify patterns and trends in data, which can then be used to make predictions about future events. For example, statistics can be used to identify the factors that are most likely to lead to a customer buying a product, or the factors that are most likely to lead to a patient being diagnosed with a particular disease.\\n* **To test hypotheses:** Statistics can be used to test hypotheses about the relationship between different variables. For example, statistics can be used to test the hypothesis that there is a relationship between the amount of money that a person spends on advertising and the number of sales that they make.\\n* **To make decisions:** Statistics can be used to make decisions about how to use data. For example, statistics can be used to determine the best way to allocate resources, or the best way to design a marketing campaign.\\n\\nStatistics is a powerful tool that can be used to make sense of the vast amounts of data that are collected in today's world. By understanding statistics, data scientists can gain valuable insights into the data that they are working with, and they can make better decisions about how to use the data.\\n\\n### Key Takeaways\\n\\n* Statistics is the science of collecting, organizing, and analyzing data in order to draw conclusions about the population from which the data was collected.\\n* Statistics is essential for data science because it provides the tools and techniques that are needed to make sense of the vast amounts of data that are collected.\\n* Statistics can be used to identify patterns and trends, test hypotheses, and make decisions.\\n* By understanding statistics, data scientists can gain valuable insights into the data that they are working with, and they can make better decisions about how to use the data.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96cd1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path = \"ML questions dump.pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "# from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# loader = CSVLoader(file_path='codebasics_faqs.csv', source_column=\"prompt\")\n",
    "\n",
    "# # Store the loaded data in the 'data' variable\n",
    "# data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea83dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='STATISTICS ........................................................................................................................................................... 6  \\nQ1. WHAT IS THE CENTRAL LIMIT THEOREM AND WHY IS IT IMPORTANT ? ........................................................................ 6  \\nQ2. WHAT IS SAMPLING ? HOW MANY SAMPLING METHODS DO YOU KNOW ? ................................................................... 7  \\nQ3. WHAT IS THE DIFFERENCE BETWEEN TYPE I VS TYPE II ERROR? .................................................................................. 9  \\nQ4. WHAT IS LINEAR REGRESSION ? WHAT DO THE TERMS P -VALUE, COEFFICIENT , AND R-SQUARED VALUE MEAN ? WHAT IS THE \\nSIGNIFICANCE OF EACH OF THESE COMPONENTS ? ................................................................................................................. 9  \\nQ5. WHAT ARE THE ASSUMPTIONS REQUIRED FOR LINEAR REGRESSION ? ........................................................................ 10  \\nQ6. WHAT IS A STATISTICAL INTERACTION ? .............................................................................................................. 10  \\nQ7. WHAT IS SELECTION BIAS ? .............................................................................................................................. 11  \\nQ8. WHAT IS AN EXAMPLE OF A DATA SET WITH A NON -GAUSSIAN DISTRIBUTION ? .......................................................... 11  \\nDATA SCIENCE .................................................................................................................................................... 12  \\nQ1. WHAT IS DATA SCIENCE? LIST THE DIFFERENCES BETWEEN SUPERVISED AND UNSUPERVISED LEARNING . ......................... 12  \\nQ2. WHAT IS SELECTION BIAS? ............................................................................................................................. 12  \\nQ3. WHAT IS BIAS -VARIANCE TRADE -OFF? ............................................................................................................... 12  \\nQ4. WHAT IS A CONFUSION MATRIX ? ..................................................................................................................... 13  \\nQ5. WHAT IS THE DIFFERENCE BETWEEN “LONG” AND “WIDE” FORMAT DATA ? ............................................................... 14  \\nQ6. WHAT DO YOU UNDERSTAND BY THE TERM NORMAL DISTRIBUTION ? ...................................................................... 15  \\nQ7. WHAT IS CORRELATION AND COVARIANCE IN STATISTICS ? ...................................................................................... 15  \\nQ8. WHAT IS THE DIFFERENCE BETWEEN POINT ESTIMATES AND CONFIDENCE INTERVAL? ................................................. 16  \\nQ9. WHAT IS THE GOAL OF A/B TESTING? ............................................................................................................... 16  \\nQ10. WHAT IS P-VALUE? ....................................................................................................................................... 16  \\nQ11. IN ANY 15-MINUTE INTERVAL , THERE IS A 20% PROBABILITY THAT YOU WILL SEE AT LEAST ONE SHOOTING STAR . WHAT IS THE \\nPROBABILITY THAT YOU SEE AT LEAST ONE SHOOTING STAR IN THE PERIOD OF AN HOUR ? ........................................................... 16  \\nQ12. HOW CAN YOU GENERATE A RANDOM NUMBER BETWEEN 1 – 7 WITH ONLY A DIE ? .................................................... 17  \\nQ13. A CERTAIN COUPLE TELLS YOU THAT THEY HAVE TWO CHILDREN , AT LEAST ONE OF WHICH IS A GIRL . WHAT IS THE \\nPROBABILITY THAT THEY HAVE TWO GIRLS ? ....................................................................................................................... 17  \\nQ14. A JAR HAS 1000 COINS, OF WHICH 999 ARE FAIR AND 1 IS DOUBLE HEADED . PICK A COIN AT RANDOM AND TOSS IT 10 \\nTIMES. GIVEN THAT YOU SEE 10 HEADS, WHAT IS THE PROBABILITY THAT THE NEXT TOSS OF THAT COIN IS ALSO A HEAD ? ................. 17  \\nQ15. WHAT DO YOU UNDERSTAND BY STATISTICAL POWER OF SENSITIVITY AND HOW DO YOU CALCULATE IT ? ......................... 18  \\nQ16. WHY IS RE-SAMPLING DONE ? ......................................................................................................................... 18  \\nQ17. WHAT ARE THE DIFFERENCES BETWEEN OVER -FITTING AND UNDER -FITTING? ............................................................ 19  \\nQ18. HOW TO COMBAT OVERFITTING AND UNDERFITTING ? ......................................................................................... 19  \\nQ19. WHAT IS REGULARIZATION ? WHY IS IT USEFUL ? .................................................................................................. 20  \\nQ20. WHAT IS THE LAW OF LARGE NUMBERS? .......................................................................................................... 20  \\nQ21. WHAT ARE CONFOUNDING VARIABLES? ........................................................................................................... 20  \\nQ22. WHAT ARE THE TYPES OF BIASES THAT CAN OCCUR DURING SAMPLING? ............................................................... 20  \\nQ23. WHAT IS SURVIVORSHIP BIAS? ........................................................................................................................ 20  \\nQ24. WHAT IS SELECTION BIAS? WHAT IS UNDER COVERAGE BIAS ? ............................................................................... 21  \\nQ25. EXPLAIN HOW A ROC CURVE WORKS ? .............................................................................................................. 21  \\nQ26. WHAT IS TF/IDF VECTORIZATION ? .................................................................................................................. 22  \\nQ27. WHY WE GENERALLY USE SOFT-MAX (OR SIGMOID ) NON-LINEARITY FUNCTION AS LAST OPERATION IN -NETWORK? WHY \\nRELU IN AN INNER LAYER ? ............................................................................................................................................ 22  \\nDATA ANALYSIS .................................................................................................................................................. 23  \\nQ1. PYTHON OR R – WHICH ONE WOULD YOU PREFER FOR TEXT ANALYTICS ? ................................................................. 23  \\nQ2. HOW DOES DATA CLEANING PLAY A VITAL ROLE IN THE ANALYSIS ? ........................................................................... 23  \\nQ3. DIFFERENTIATE BETWEEN UNIVARIATE , BIVARIATE AND MULTIVARIATE ANALYSIS . ....................................................... 23  \\nQ4. EXPLAIN STAR SCHEMA. ................................................................................................................................. 23  \\nQ5. WHAT IS CLUSTER SAMPLING? ........................................................................................................................ 23  ', metadata={'source': 'ML questions dump.pdf', 'page': 0}),\n",
       " Document(page_content='Q6. WHAT IS SYSTEMATIC SAMPLING? ................................................................................................................... 24  \\nQ7. WHAT ARE EIGENVECTORS AND EIGENVALUES ? .................................................................................................. 24  \\nQ8. CAN YOU CITE SOME EXAMPLES WHERE A FALSE POSITIVE IS IMPORTANT THAN A FALSE NEGATIVE ? ................................ 24  \\nQ9. CAN YOU CITE SOME EXAMPLES WHERE A FALSE NEGATIVE IMPORTANT THAN A FALSE POSITIVE ? AND VICE VERSA ? .......... 24 \\nQ10. CAN YOU CITE SOME EXAMPLES WHERE BOTH FALSE POSITIVE AND FALSE NEGATIVES ARE EQUALLY IMPORTANT ? ............. 25  \\nQ11. CAN YOU EXPLAIN THE DIFFERENCE BETWEEN A VALIDATION SET AND A TEST SET? .................................................... 25  \\nQ12. EXPLAIN CROSS -VALIDATION . .......................................................................................................................... 25  \\nMACHINE LEARNING .......................................................................................................................................... 27  \\nQ1. WHAT IS MACHINE LEARNING? ....................................................................................................................... 27  \\nQ2. WHAT IS SUPERVISED LEARNING? .................................................................................................................... 27  \\nQ3. WHAT IS UNSUPERVISED LEARNING ? ................................................................................................................ 27  \\nQ4. WHAT ARE THE VARIOUS ALGORITHMS ? ............................................................................................................ 27  \\nQ5. WHAT IS ‘NAIVE’ IN A NAIVE BAYES? ................................................................................................................ 28  \\nQ6. WHAT IS PCA? WHEN DO YOU USE IT ? ............................................................................................................. 29  \\nQ7. EXPLAIN SVM ALGORITHM IN DETAIL . ............................................................................................................... 30  \\nQ8. WHAT ARE THE SUPPORT VECTORS IN SVM? ...................................................................................................... 31  \\nQ9. WHAT ARE THE DIFFERENT KERNELS IN SVM? .................................................................................................... 32  \\nQ10. WHAT ARE THE MOST KNOWN ENSEMBLE ALGORITHMS ? ...................................................................................... 32  \\nQ11. EXPLAIN DECISION TREE ALGORITHM IN DETAIL . .................................................................................................. 32  \\nQ12. WHAT ARE ENTROPY AND INFORMATION GAIN IN DECISION TREE ALGORITHM ? ........................................................ 33  \\nGini Impurity and Information Gain - CART ....................................................................................................... 34  \\nEntropy and Information Gain – ID3 .................................................................................................................. 37  \\nQ13. WHAT IS PRUNING IN DECISION TREE? .............................................................................................................. 41  \\nQ14. WHAT IS LOGISTIC REGRESSION ? STATE AN EXAMPLE WHEN YOU HAVE USED LOGISTIC REGRESSION RECENTLY . ................ 41  \\nQ15. WHAT IS LINEAR REGRESSION? ........................................................................................................................ 42  \\nQ16. WHAT ARE THE DRAWBACKS OF THE LINEAR MODEL? ......................................................................................... 43  \\nQ17. WHAT IS THE DIFFERENCE BETWEEN REGRESSION AND CLASSIFICATION ML TECHNIQUES ? ........................................... 43  \\nQ18. WHAT ARE RECOMMENDER SYSTEMS? ............................................................................................................. 43  \\nQ19. WHAT IS COLLABORATIVE FILTERING ? AND A CONTENT BASED ? ............................................................................. 44  \\nQ20. HOW CAN OUTLIER VALUES BE TREATED ? ........................................................................................................... 44  \\nQ21. WHAT ARE THE VARIOUS STEPS INVOLVED IN AN ANALYTICS PROJECT ? ..................................................................... 45  \\nQ22. DURING ANALYSIS , HOW DO YOU TREAT MISSING VALUES ? .................................................................................... 45  \\nQ23. HOW WILL YOU DEFINE THE NUMBER OF CLUSTERS IN A CLUSTERING ALGORITHM ? ..................................................... 45  \\nQ24. WHAT IS ENSEMBLE LEARNING? ...................................................................................................................... 48  \\nQ25. DESCRIBE IN BRIEF ANY TYPE OF ENSEMBLE LEARNING. ......................................................................................... 49  \\nBagging ............................................................................................................................................................. 49  \\nBoosting ............................................................................................................................................................. 49  \\nQ26. WHAT IS A RANDOM FOREST? HOW DOES IT WORK ? ........................................................................................... 50  \\nQ27. HOW DO YOU WORK TOWARDS A RANDOM FOREST? ......................................................................................... 51  \\nQ28. WHAT CROSS-VALIDATION TECHNIQUE WOULD YOU USE ON A TIME SERIES DATA SET ? ................................................ 52  \\nQ29. WHAT IS A BOX-COX TRANSFORMATION ? ......................................................................................................... 53  \\nQ30. HOW REGULARLY MUST AN ALGORITHM BE UPDATED? ....................................................................................... 53  \\nQ31. IF YOU ARE HAVING 4GB RAM IN YOUR MACHINE AND YOU WANT TO TRAIN YOUR MODEL ON 10GB DATA SET. HOW \\nWOULD YOU GO ABOUT THIS PROBLEM ? HAVE YOU EVER FACED THIS KIND OF PROBLEM IN YOUR MACHINE LEARNING /DATA SCIENCE \\nEXPERIENCE SO FAR ? .................................................................................................................................................... 53  \\nDEEP LEARNING ................................................................................................................................................. 55  \\nQ1. WHAT DO YOU MEAN BY DEEP LEARNING? ........................................................................................................ 55  \\nQ2. WHAT IS THE DIFFERENCE BETWEEN MACHINE LEARNING AND DEEP LEARNING ? ........................................................ 55  \\nQ3. WHAT, IN YOUR OPINION , IS THE REASON FOR THE POPULARITY OF DEEP LEARNING IN RECENT TIMES ? .......................... 56  \\nQ4. WHAT IS REINFORCEMENT LEARNING ? .............................................................................................................. 56  \\nQ5. WHAT ARE ARTIFICIAL NEURAL NETWORKS? ...................................................................................................... 57  ', metadata={'source': 'ML questions dump.pdf', 'page': 1}),\n",
       " Document(page_content='Q6. DESCRIBE THE STRUCTURE OF ARTIFICIAL NEURAL NETWORKS? ............................................................................. 57  \\nQ7. HOW ARE WEIGHTS INITIALIZED IN A NETWORK? ............................................................................................... 57  \\nQ8. WHAT IS THE COST FUNCTION? ....................................................................................................................... 58  \\nQ9. WHAT ARE HYPERPARAMETERS ? ..................................................................................................................... 58  \\nQ10. WHAT WILL HAPPEN IF THE LEARNING RATE IS SET INACCURATELY (TOO LOW OR TOO HIGH)? ................................... 58  \\nQ11. WHAT IS THE DIFFERENCE BETWEEN EPOCH, BATCH, AND ITERATION IN DEEP LEARNING? ......................................... 58  \\nQ12. WHAT ARE THE DIFFERENT LAYERS ON CNN? .................................................................................................... 58  \\nConvolution Operation ...................................................................................................................................... 60  \\nPooling Operation ............................................................................................................................................. 62  \\nClassification ..................................................................................................................................................... 63  \\nTraining ............................................................................................................................................................. 64  \\nTesting ............................................................................................................................................................... 65  \\nQ13. WHAT IS POOLING ON CNN, AND HOW DOES IT WORK? .................................................................................... 65  \\nQ14. WHAT ARE RECURRENT NEURAL NETWORKS (RNNS)? ........................................................................................ 65  \\nParameter Sharing ............................................................................................................................................ 67  \\nDeep RNNs ......................................................................................................................................................... 68  \\nBidirectional RNNs ............................................................................................................................................. 68  \\nRecursive Neural Network ................................................................................................................................. 69  \\nEncoder Decoder Sequence to Sequence RNNs ................................................................................................. 70  \\nLSTMs ................................................................................................................................................................ 70  \\nQ15. HOW DOES AN LSTM NETWORK WORK? ......................................................................................................... 70  \\nRecurrent Neural Networks ............................................................................................................................... 71  \\nThe Problem of Long-Term Dependencies ......................................................................................................... 72  \\nLSTM Networks .................................................................................................................................................. 73  \\nThe Core Idea Behind LSTMs ............................................................................................................................. 74  \\nQ16. WHAT IS A MULTI-LAYER PERCEPTRON (MLP)? ................................................................................................. 75  \\nQ17. EXPLAIN GRADIENT DESCENT. ......................................................................................................................... 76  \\nQ18. WHAT IS EXPLODING GRADIENTS ? .................................................................................................................... 77  \\nSolutions ............................................................................................................................................................ 78  \\nQ19. WHAT IS VANISHING GRADIENTS ? .................................................................................................................... 78  \\nSolutions ............................................................................................................................................................ 79  \\nQ20. WHAT IS BACK PROPAGATION AND EXPLAIN IT WORKS. ....................................................................................... 79  \\nQ21. WHAT ARE THE VARIANTS OF BACK PROPAGATION ? ............................................................................................ 79  \\nQ22. WHAT ARE THE DIFFERENT DEEP LEARNING FRAMEWORKS ? .................................................................................. 81  \\nQ23. WHAT IS THE ROLE OF THE ACTIVATION FUNCTION? ............................................................................................ 81  \\nQ24. NAME A FEW MACHINE LEARNING LIBRARIES FOR VARIOUS PURPOSES . .................................................................... 81  \\nQ25. WHAT IS AN AUTO-ENCODER? ........................................................................................................................ 81  \\nQ26. WHAT IS A BOLTZMANN MACHINE? ................................................................................................................. 82  \\nQ27. WHAT IS DROPOUT AND BATCH NORMALIZATION ? ............................................................................................. 83  \\nQ28. WHY IS TENSORFLOW THE MOST PREFERRED LIBRARY IN DEEP LEARNING? ............................................................. 83  \\nQ29. WHAT DO YOU MEAN BY TENSOR IN TENSORFLOW? .......................................................................................... 83  \\nQ30. WHAT IS THE COMPUTATIONAL GRAPH? ........................................................................................................... 83  \\nQ31. HOW IS LOGISTIC REGRESSION DONE ? ............................................................................................................... 83  \\nMISCELLANEOUS ................................................................................................................................................ 84  \\nQ1. EXPLAIN THE STEPS IN MAKING A DECISION TREE . ................................................................................................. 84  \\nQ2. HOW DO YOU BUILD A RANDOM FOREST MODEL ? ................................................................................................ 84  \\nQ3. DIFFERENTIATE BETWEEN UNIVARIATE , BIVARIATE, AND MULTIVARIATE ANALYSIS . ...................................................... 85  \\nUnivariate .......................................................................................................................................................... 85  \\nBivariate ............................................................................................................................................................ 85  \\nMultivariate ....................................................................................................................................................... 85  \\nQ4. WHAT ARE THE FEATURE SELECTION METHODS USED TO SELECT THE RIGHT VARIABLES ? .............................................. 86  \\nFilter Methods ................................................................................................................................................... 86  ', metadata={'source': 'ML questions dump.pdf', 'page': 2}),\n",
       " Document(page_content='Wrapper Methods ............................................................................................................................................. 86  \\nQ5. IN YOUR CHOICE OF LANGUAGE , WRITE A PROGRAM THAT PRINTS THE NUMBERS RANGING FROM ONE TO 50. BUT FOR \\nMULTIPLES OF THREE , PRINT \"FIZZ\" INSTEAD OF THE NUMBER AND FOR THE MULTIPLES OF FIVE , PRINT \"BUZZ.\" FOR NUMBERS WHICH \\nARE MULTIPLES OF BOTH THREE AND FIVE , PRINT \"FIZZBUZZ.\" .............................................................................................. 86  \\nQ6. YOU ARE GIVEN A DATA SET CONSISTING OF VARIABLES WITH MORE THAN 30 PERCENT MISSING VALUES . HOW WILL YOU \\nDEAL WITH THEM ? ....................................................................................................................................................... 87  \\nQ7. FOR THE GIVEN POINTS , HOW WILL YOU CALCULATE THE EUCLIDEAN DISTANCE IN PYTHON? ........................................ 87  \\nQ8. WHAT ARE DIMENSIONALITY REDUCTION AND ITS BENEFITS ? ................................................................................. 87  \\nQ9. HOW WILL YOU CALCULATE EIGENVALUES AND EIGENVECTORS OF THE FOLLOWING 3X3 MATRIX? ................................. 88  \\nQ10. HOW SHOULD YOU MAINTAIN A DEPLOYED MODEL ? ............................................................................................ 88  \\nQ11. HOW CAN A TIME -SERIES DATA BE DECLARED AS STATIONERY ? ............................................................................... 88  \\nQ12. \\'PEOPLE WHO BOUGHT THIS ALSO BOUGHT ...\\' RECOMMENDATIONS SEEN ON AMAZON ARE A RESULT OF WHICH ALGORITHM ?\\n 89  \\nQ13. WHAT IS A GENERATIVE ADVERSARIAL NETWORK? .............................................................................................. 89  \\nQ14. YOU ARE GIVEN A DATASET ON CANCER DETECTION . YOU HAVE BUILT A CLASSIFICATION MODEL AND ACHIEVED AN ACCURACY \\nOF 96 PERCENT. WHY SHOULDN \\'T YOU BE HAPPY WITH YOUR MODEL PERFORMANCE ? WHAT CAN YOU DO ABOUT IT ? ................... 90  \\nQ15. BELOW ARE THE EIGHT ACTUAL VALUES OF THE TARGET VARIABLE IN THE TRAIN FILE . WHAT IS THE ENTROPY OF THE TARGET \\nVARIABLE? [0, 0, 0, 1, 1, 1, 1, 1] .................................................................................................................................. 90  \\nQ16. WE WANT TO PREDICT THE PROBABILITY OF DEATH FROM HEART DISEASE BASED ON THREE RISK FACTORS : AGE, GENDER, AND \\nBLOOD CHOLESTEROL LEVEL . WHAT IS THE MOST APPROPRIATE ALGORITHM FOR THIS CASE ? CHOOSE THE CORRECT OPTION : ........... 90 \\nQ17. AFTER STUDYING THE BEHAVIOR OF A POPULATION , YOU HAVE IDENTIFIED FOUR SPECIFIC INDIVIDUAL TYPES THAT ARE \\nVALUABLE TO YOUR STUDY . YOU WOULD LIKE TO FIND ALL USERS WHO ARE MOST SIMILAR TO EACH INDIVIDUAL TYPE . WHICH \\nALGORITHM IS MOST APPROPRIATE FOR THIS STUDY ? .......................................................................................................... 90  \\nQ18. YOU HAVE RUN THE ASSOCIATION RULES ALGORITHM ON YOUR DATASET , AND THE TWO RULES {BANANA, APPLE} => {GRAPE} \\nAND {APPLE, ORANGE} => {GRAPE} HAVE BEEN FOUND TO BE RELEVANT . WHAT ELSE MUST BE TRUE ? CHOOSE THE RIGHT ANSWER : .. 90 \\nQ19. YOUR ORGANIZATION HAS A WEBSITE WHERE VISITORS RANDOMLY RECEIVE ONE OF TWO COUPONS . IT IS ALSO POSSIBLE THAT \\nVISITORS TO THE WEBSITE WILL NOT RECEIVE A COUPON . YOU HAVE BEEN ASKED TO DETERMINE IF OFFERING A COUPON TO WEBSITE \\nVISITORS HAS ANY IMPACT ON THEIR PURCHASE DECISIONS . WHICH ANALYSIS METHOD SHOULD YOU USE ? .................................... 91  \\nQ20. WHAT ARE THE FEATURE VECTORS ? .................................................................................................................. 91  \\nQ21. WHAT IS ROOT CAUSE ANALYSIS ? ..................................................................................................................... 91  \\nQ22. DO GRADIENT DESCENT METHODS ALWAYS CONVERGE TO SIMILAR POINTS ? ............................................................. 91  \\nQ23. WHAT ARE THE MOST POPULAR CLOUD SERVICES USED IN DATA SCIENCE? .............................................................. 91  \\nQ24. WHAT IS A CANARY DEPLOYMENT ? .................................................................................................................. 92  \\nQ25. WHAT IS A BLUE GREEN DEPLOYMENT ? ............................................................................................................ 93  \\n \\n \\n  ', metadata={'source': 'ML questions dump.pdf', 'page': 3}),\n",
       " Document(page_content='Data Science interview questions \\nStatistics \\n \\nQ1. What is the Central Limit Theorem and why is it important?  \\n \\nhttps://spin.atomicobject.com/2015/02/12/central-limit-theorem-intro/  \\n \\nSuppose that we are interested in estimating the average height among all people. Collecting data for \\nevery person in the world is impractical, bordering on impossible. While we can’t obtain a height \\nmeasurement from everyone in the population, we can still sample some people. The question now \\nbecomes, what can we say about the average height of the entire population given a single sample. \\nThe Central Limit Theorem addresses this question exactly. Formally, it states that if we sample from a \\npopulation using a sufficiently large sample size, the mean of the samples (also known as the sample \\npopulation) will be normally distributed (assuming true random sampling), the mean tending to the mean \\nof the population and variance equal to the variance of the population divided by the size of the sampling. \\nWhat’s especially important is that this will be true regardless of the distribution of the original \\npopulation. \\n \\nEX:  \\n \\nAs we can see, the distribution is pretty ugly. It certainly isn’t normal, uniform, or any other commonly \\nknown distribution. In order to sample from the above distribution, we need to define a sample size, \\nreferred to as N. This is the number of observations that we will sample at a time. Suppose that we choose \\nN to be 3. This means that we will sample in groups of 3. So for the above population, we might sample \\ngroups such as [5, 20, 41], [60, 17, 82], [8, 13, 61], and so on. \\nSuppose that we gather 1,000 samples of 3 from the above population. For each sample, we can compute \\nits average. If we do that, we will have 1,000 averages. This set of 1,000 averages is called a sampling \\ndistribution, and according to Central Limit Theorem, the sampling distribution will approach a normal \\ndistribution as the sample size N used to produce it increases. Here is what our sample distribution looks \\nlike for N = 3. \\n', metadata={'source': 'ML questions dump.pdf', 'page': 4}),\n",
       " Document(page_content=' \\n \\nAs we can see, it certainly looks uni-modal, though not necessarily normal. If we repeat the same process \\nwith a larger sample size, we should see the sampling distribution start to become more normal. Let’s \\nrepeat the same process again with N = 10. Here is the sampling distribution for that sample size. \\n \\n \\nQ2. What is sampling? How many sampling methods do you know?  \\n \\nhttps://searchbusinessanalytics.techtarget.com/definition/data-sampling   \\nhttps://nikolanews.com/difference-between-stratified-sampling-cluster-sampling-and-quota-sampling/   \\n \\nData sampling is a statistical analysis technique used to select, manipulate and analyze a representative \\nsubset of data points to identify patterns and trends in the larger data set being examined. It enables data \\nscientists, predictive modelers and other data analysts to work with a small, manageable amount of data \\nabout a statistical population to build and run analytical models more quickly, while still producing \\naccurate findings.  \\n', metadata={'source': 'ML questions dump.pdf', 'page': 5}),\n",
       " Document(page_content=\"Sampling can be particularly useful with data sets that are too large to efficiently analyze in full – for \\nexample, in big data analytics applications or surveys. Identifying and analyzing a representative sample \\nis more efficient and cost-effective than surveying the entirety of the data or population. \\nAn important consideration, though, is the size of the required data sample and the possibility of \\nintroducing a sampling error. In some cases, a small sample can reveal the most important information \\nabout a data set. In others, using a larger sample can increase the likelihood of accurately representing \\nthe data as a whole, even though the increased size of the sample may impede ease of manipulation and \\ninterpretation. \\nThere are many different methods for drawing samples from data; the ideal one depends on the data set \\nand situation. Sampling can be based on probability, an approach that uses random numbers that \\ncorrespond to points in the data set to ensure that there is no correlation between points chosen for the \\nsample. Further variations in probability sampling include: \\n \\n• Simple random sampling: Software is used to randomly select subjects from the whole population. \\n• Stratified sampling: Subsets of the data sets or population are created based on a common factor, \\nand samples are randomly collected from each subgroup. A sample is drawn from each strata \\n(using a random sampling method like simple random sampling or systematic sampling). \\no EX: In the image below, let's say you need a sample size of 6. Two members from each \\ngroup (yellow, red, and blue) are selected randomly. Make sure to sample proportionally: \\nIn this simple example, 1/3 of each group (2/6 yellow, 2/6 red and 2/6 blue) has been \\nsampled. If you have one group that's a different size, make sure to adjust your \\nproportions. For example, if you had 9 yellow, 3 red and 3 blue, a 5-item sample would \\nconsist of 3/9 yellow (i.e. one third), 1/3 red and 1/3 blue. \\n• Cluster sampling: The larger data set is divided into subsets (clusters) based on a defined factor, \\nthen a random sampling of clusters is analyzed. The sampling unit is the whole cluster; Instead of \\nsampling individuals from within each group, a researcher will study whole clusters. \\no EX: In the image below, the strata are natural groupings by head color (yellow, red, blue). \\nA sample size of 6 is needed, so two of the complete strata are selected randomly (in this \\nexample, groups 2 and 4 are chosen). \\n• Multistage sampling: A more complicated form of cluster sampling, this method also involves \\ndividing the larger population into a number of clusters. Second-stage clusters are then broken \\nout based on a secondary factor, and those clusters are then sampled and analyzed. This staging \\ncould continue as multiple subsets are identified, clustered and analyzed. \\n• Systematic sampling: A sample is created by setting an interval at which to extract data from the \\nlarger population – for example, selecting every 10th row in a spreadsheet of 200 items to create \\na sample size of 20 rows to analyze. \\n\", metadata={'source': 'ML questions dump.pdf', 'page': 6}),\n",
       " Document(page_content=' \\nSampling can also be based on non-probability, an approach in which a data sample is determined and \\nextracted based on the judgment of the analyst. As inclusion is determined by the analyst, it can be more \\ndifficult to extrapolate whether the sample accurately represents the larger population than when \\nprobability sampling is used. \\n \\nNon-probability data sampling methods include: \\n \\n• Convenience sampling: Data is collected from an easily accessible and available group. \\n• Consecutive sampling: Data is collected from every subject that meets the criteria until the \\npredetermined sample size is met. \\n• Purposive or judgmental sampling: The researcher selects the data to sample based on predefined \\ncriteria. \\n• Quota sampling: The researcher ensures equal representation within the sample for all subgroups \\nin the data set or population (random sampling is not used). \\n \\nOnce generated, a sample can be used for predictive analytics. For example, a retail business might use \\ndata sampling to uncover patterns about customer behavior and predictive modeling to create more \\neffective sales strategies. \\n \\nQ3. What is the difference between type I vs type II error? \\n \\nhttps://www.datasciencecentral.com/profiles/blogs/understanding-type-i-and-type-ii-errors  \\n \\nIs Ha true? No, H 0 is True (H a is Negative: TN); Yes, H 0 is False (H a is Positive: TP).  \\nA type I error occurs when the null hypothesis is true but is rejected. A type II error occurs when the null \\nhypothesis is false but erroneously fails to be rejected. \\n \\n No reject H 0 Reject H 0 \\nH0 is True TN FP (I error) \\nH0 is False FN (II error) TP \\n \\n \\nQ4. What is linear regression? What do the terms p-value, coefficient, and r-\\nsquared value mean? What is the significance of each of these components? \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 7}),\n",
       " Document(page_content='https://www.springboard.com/blog/linear-regression-in-python-a-tutorial/   \\nhttps://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-\\nvalues-and-coefficients   \\n \\nImagine you want to predict the price of a house. That will depend on some factors, called independent \\nvariables, such as location, size, year of construction…  if we assume there is a linear relationship between \\nthese variables and the price (our dependent variable), then our price is predicted by the following \\nfunction: \\n\\xa0 =  2+ 3 0 \\n \\nThe p-value in the table is the minimum I (the significance level) at which the coefficient is relevant. The \\nlower the p-value, the more important is the variable in predicting the price. Usually we set a 5% level, so \\nthat we have a 95% confidentiality that our variable is relevant. \\nThe p-value is used as an alternative to rejection points to provide the smallest level of significance at \\nwhich the null hypothesis would be rejected. A smaller p-value means that there is stronger evidence in \\nfavor of the alternative hypothesis. \\n \\nThe coefficient value signifies how much the mean of the dependent variable changes given a one-unit \\nshift in the independent variable while holding other variables in the model constant. This property of \\nholding the other variables constant is crucial because it allows you to assess the effect of each variable \\nin isolation from the others. \\n \\nR squared (R2) is a statistical measure that represents the proportion of the variance for a dependent \\nvariable that\\'s explained by an independent variable or variables in a regression model. \\n \\nQ5. What are the assumptions required for linear regression? \\n \\nThere are four major assumptions:  \\n \\n• There is a linear relationship between the dependent variables and the regressors, meaning the \\nmodel you are creating actually fits the data,  \\n• The errors or residuals ( H\\x18− H X\\x18) of the data are normally distributed and independent from \\neach other,  \\n• There is minimal multicollinearity between explanatory variables, and  \\n• Homoscedasticity. This means the variance around the regression line is the same for all values \\nof the predictor variable. \\n \\nQ6. What is a statistical interaction? \\nhttp://icbseverywhere.com/blog/mini-lessons-tutorials-and-support-pages/statistical-interactions/   \\n \\nBasically, an interaction is when the effect of one factor (input variable) on the dependent variable (output \\nvariable) differs among levels of another factor. When two or more independent variables are involved in \\na research design, there is more to consider than simply the \"main effect\" of each of the independent \\nvariables (also termed \"factors\"). That is, the effect of one independent variable on the dependent \\nvariable of interest may not be the same at all levels of the other independent variable. Another way to \\nput this is that the effect of one independent variable may depend on the level of the other independent \\nvariable. In order to find an interaction, you must have a factorial design, in which the two (or more) ', metadata={'source': 'ML questions dump.pdf', 'page': 8}),\n",
       " Document(page_content='independent variables are \"crossed\" with one another so that there are observations at every \\ncombination of levels of the two independent variables.  EX: stress level and practice to memorize words: \\ntogether they may have a lower performance. \\n \\nQ7. What is selection bias? \\n \\nhttps://www.elderresearch.com/blog/selection-bias-in-analytics   \\n \\nSelection (or ‘sampling’) bias occurs when the sample data that is gathered and prepared for modeling \\nhas characteristics that are not representative of the true, future population of cases the model will see. \\nThat is, active selection bias occurs when a subset of the data is systematically (i.e., non-randomly) \\nexcluded from analysis. \\n \\n \\nQ8. What is an example of a data set with a non-Gaussian distribution? \\n \\nhttps://www.quora.com/Most-machine-learning-datasets-are-in-Gaussian-distribution-Where-can-we-find-\\nthe-dataset-which-follows-Bernoulli-Poisson-gamma-beta-etc-distribution \\n \\nThe Gaussian distribution is part of the Exponential family of distributions, but there are a lot more of \\nthem, with the same sort of ease of use, in many cases, and if the person doing the machine learning has \\na solid grounding in statistics, they can be utilized where appropriate.  \\n \\nBinomial: multiple toss of a coin Bin(n,p): the binomial distribution consists of the probabilities of each of \\nthe possible numbers of successes on n trials for independent events that each have a probability of p of \\noccurring. \\nBernoulli: Bin(1,p) = Be(p) \\nPoisson: Pois( K) \\n  ', metadata={'source': 'ML questions dump.pdf', 'page': 9}),\n",
       " Document(page_content='Data Science \\n \\nQ1. What is Data Science? List the differences between supervised and \\nunsupervised learning. \\n \\nData Science is a blend of various tools, algorithms, and machine learning principles with the goal to \\ndiscover hidden patterns from the raw data. How is this different from what statisticians have been doing \\nfor years? The answer lies in the difference between explaining and predicting: statisticians work a \\nposteriori, explaining the results and designing a plan; data scientists use historical data to make \\npredictions. \\n \\nThe differences between supervised and unsupervised learning are: \\n \\nSupervised Unsupervised \\nInput data is labelled Input data is unlabeled \\nSplit in training/validation/test No split \\nUsed for prediction Used for analysis \\nClassification and Regression Clustering, dimension reduction,  \\nand density estimation \\n \\nQ2. What is Selection Bias? \\n \\nSelection bias is a kind of error that occurs when the researcher decides what has to be studied. It is \\nassociated with research where the selection of participants is not random. Therefore, some conclusions \\nof the study may not be accurate. \\n \\nThe types of selection bias include: \\n• Sampling bias: It is a systematic error due to a non-random sample of a population causing some \\nmembers of the population to be less likely to be included than others resulting in a biased \\nsample. \\n• Time interval: A trial may be terminated early at an extreme value (often for ethical reasons), but \\nthe extreme value is likely to be reached by the variable with the largest variance, even if all \\nvariables have a similar mean. \\n• Data: When specific subsets of data are chosen to support a conclusion or rejection of bad data \\non arbitrary grounds, instead of according to previously stated or generally agreed criteria. \\n• Attrition: Attrition bias is a kind of selection bias caused by attrition (loss of participants) \\ndiscounting trial subjects/tests that did not run to completion. \\n \\nQ3. What is bias-variance trade-off? \\n \\nBias: Bias is an error introduced in the model due to the oversimplification of the algorithm used (does \\nnot fit the data properly). It can lead to under-fitting. \\nLow bias machine learning algorithms — Decision Trees, k-NN and SVM  \\nHigh bias machine learning algorithms — Linear Regression, Logistic Regression \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 10}),\n",
       " Document(page_content='Variance: Variance is error introduced in the model due to a too complex algorithm, it performs very well \\nin the training set but poorly in the test set. It can lead to high sensitivity and overfitting. \\nPossible high variance – polynomial regression \\n  \\nNormally, as you increase the complexity of your model, you will see a reduction in error due to lower \\nbias in the model. However, this only happens until a particular point. As you continue to make your model \\nmore complex, you end up over-fitting your model and hence your model will start suffering from high \\nvariance. \\n \\nBias-Variance trade-off: The goal of any supervised machine learning algorithm is to have low bias and \\nlow variance to achieve good prediction performance. \\n \\n1. The k-nearest neighbor algorithm has low bias and high variance, but the trade-off can be changed \\nby increasing the value of k which increases the number of neighbors that contribute to the \\nprediction and in turn increases the bias of the model. \\n2. The support vector machine algorithm has low bias and high variance, but the trade-off can be \\nchanged by increasing the C parameter that influences the number of violations of the margin \\nallowed in the training data which increases the bias but decreases the variance. \\n3. The decision tree has low bias and high variance, you can decrease the depth of the tree or use \\nfewer attributes. \\n4. The linear regression has low variance and high bias, you can increase the number of features or \\nuse another regression that better fits the data. \\n \\nThere is no escaping the relationship between bias and variance in machine learning. Increasing the bias \\nwill decrease the variance. Increasing the variance will decrease bias. \\n \\nQ4. What is a confusion matrix? \\n \\nThe confusion matrix is a 2X2 table that contains 4 outputs provided by the binary classifier.  \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 11}),\n",
       " Document(page_content=' Predict + Predict - \\nActual + TP FN (II error) \\nActual - FP (I error) TN \\n \\nA data set used for performance evaluation is called a test data set. It should contain the correct labels \\nand predicted labels. The predicted labels will exactly the same if the performance of a binary classifier is \\nperfect. The predicted labels usually match with part of the observed labels in real-world scenarios. \\nA binary classifier predicts all data instances of a test data set as either positive or negative. This produces \\nfour outcomes: TP, FP, TN, FN. Basic measures derived from the confusion matrix: \\n \\n1. %AA?A -2C6 =\\x0c\\x0f\\x0b\\x0c\\x0e\\n\\x0f\\x0b\\x0e \\n \\n2. !44DA24H  =\\x10\\x0f\\x0b\\x10\\n\\x0f\\x0b\\x0e \\n \\n3. .6>B:C:E:CH  (-642<< ?A /AD6 @?B:C:E6 A2C6)=\\x10\\x0f\\n\\x10\\x0f\\x0b\\x0c\\x0e=\\x10\\x0f\\n\\x0f \\n \\n4. .@64:7:4:CH  (/AD6 >682C:E6  A2C6)=\\x10\\x0e\\n\\x10\\x0e\\x0b\\x0c\\x0f=\\x10\\x0e\\n\\x0e \\n \\n5. ,A64:B:?>  (,?B:C:E6 @A65:4C65  E2<D6)=\\x10\\x0f\\n\\x10\\x0f\\x0b\\x0c\\x0f \\n \\n6. &−.4?A6 ((2A=?>:4  =62> ?7 @A64:B:?>  2>5 A642<<)= \\x08 \\x10\\x0f\\n(\\x08 \\x10\\x0f \\x0b \\x0c\\x0f \\x0b \\x0c\\x0e) \\n \\nQ5. What is the difference between “long” and “wide” format data? \\n \\nIn the wide-format, a subject’s repeated responses will be in a single row, and each response is in a \\nseparate column. In the long-format, each row is a one-time point per subject. You can recognize data in \\nwide format by the fact that columns generally represent groups (variables). \\n \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 12}),\n",
       " Document(page_content='Q6. What do you understand by the term Normal Distribution? \\n \\nData is usually distributed in different ways with a bias to the left or to the right or it can all be jumbled \\nup. However, there are chances that data is distributed around a central value without any bias to the left \\nor right and reaches normal distribution in the form of a bell-shaped curve. \\n \\nThe random variables are distributed in the form of a symmetrical, bell-shaped curve. Properties of \\nNormal Distribution are as follows: \\n \\n1. Unimodal (Only one mode) \\n2. Symmetrical (left and right halves are mirror images) \\n3. Bell-shaped (maximum height (mode) at the mean) \\n4. Mean, Mode, and Median are all located in the center \\n5. Asymptotic \\n \\nQ7. What is correlation and covariance in statistics? \\n \\nCorrelation is considered or described as the best technique for measuring and also for estimating the \\nquantitative relationship between two variables. Correlation measures how strongly two variables are \\nrelated. Given two random variables, it is the covariance between both divided by the product of the two \\nstandard deviations of the single variables, hence always between -1 and 1. \\n \\nL=#?E(0,\\xa0)\\nM(0) M(\\xa0) ∈ [−1,1]  \\n \\nCovariance is a measure that indicates the extent to which two random variables change in cycle. It \\nexplains the systematic relation between a pair of random variables, wherein changes in one variable \\nreciprocal by a corresponding change in another variable. \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 13}),\n",
       " Document(page_content='#?E(0,\\xa0)=%[(0−%[0])(\\xa0−%[\\xa0])]=%[0\\xa0]−%[0]%[\\xa0] \\n \\n \\nQ8. What is the difference between Point Estimates and Confidence Interval? \\n \\nPoint Estimation gives us a particular value as an estimate of a population parameter. Method of Moments \\nand Maximum Likelihood estimator methods are used to derive Point Estimators for population \\nparameters. \\nA confidence interval gives us a range of values which is likely to contain the population parameter. The \\nconfidence interval is generally preferred, as it tells us how likely this interval is to contain the population \\nparameter. This likeliness or probability is called Confidence Level or Confidence coefficient and \\nrepresented by 1−I, where I is the level of significance. \\n \\nQ9. What is the goal of A/B Testing? \\n \\nIt is a hypothesis testing for a randomized experiment with two variables A and B. \\nThe goal of A/B Testing is to identify any changes to the web page to maximize or increase the outcome \\nof interest.  A/B testing is a fantastic method for figuring out the best online promotional and marketing \\nstrategies for your business. It can be used to test everything from website copy to sales emails to search \\nads. An example of this could be identifying the click-through rate for a banner ad.  \\n \\nQ10. What is p-value? \\n \\nWhen you perform a hypothesis test in statistics, a p-value can help you determine the strength of your \\nresults. p-value is the minimum significance level at which you can reject the null hypothesis. The lower \\nthe p-value, the more likely you reject the null hypothesis. \\n \\nQ11. In any 15-minute interval, there is a 20% probability that you will see at least \\none shooting star. What is the probability that you see at least one shooting star in \\nthe period of an hour? \\n \\n• ,A?323:<:CH  ?7 >?C B66:>8 2>H Bℎ??C:>8 BC2A :> 15 =:>DC6B :B =\\n 1 – ,(.66:>8 ?>6 Bℎ??C:>8 BC2A) =  1 – 0.2 =  0.8 \\n• ,A?323:<:CH  ?7 >?C B66:>8 2>H Bℎ??C:>8 BC2A :> Cℎ6 @6A:?5 ?7 ?>6 ℎ?DA= (0.8)\\n =\\n 0.4096 \\n', metadata={'source': 'ML questions dump.pdf', 'page': 14}),\n",
       " Document(page_content=' \\n• ,A?323:<:CH  ?7 B66:>8 2C <62BC ?>6 Bℎ??C:>8 BC2A :> Cℎ6 ?>6 ℎ?DA =\\n 1 – ,(+?C B66:>8 2>H BC2A) =  1 – 0.4096 =  0.5904 \\n \\nQ12. How can you generate a random number between 1 – 7 with only a die? \\n \\nAny die has six sides from 1-6. There is no way to get seven equal outcomes from a single rolling of a die. \\nIf we roll the die twice and consider the event of two rolls, we now have 36 different outcomes. To get \\nour 7 equal outcomes we have to reduce this 36 to a number divisible by 7. We can thus consider only 35 \\noutcomes and exclude the other one. A simple scenario can be to exclude the combination (6,6), i.e., to \\nroll the die again if 6 appears twice. All the remaining combinations from (1,1) till (6,5) can be divided into \\n7 parts of 5 each. This way all the seven sets of outcomes are equally likely. \\n \\nQ13. A certain couple tells you that they have two children, at least one of which \\nis a girl. What is the probability that they have two girls? \\n \\n,((2E:>8 CF? 8:A<B 8:E6> ?>6 8:A<)=1\\n2 \\n \\nQ14. A jar has 1000 coins, of which 999 are fair and 1 is double headed. Pick a coin \\nat random and toss it 10 times. Given that you see 10 heads, what is the probability \\nthat the next toss of that coin is also a head? \\n \\nThere are two ways of choosing the coin. One is to pick a fair coin and the other is to pick the one with \\ntwo heads. \\n \\n,A?323:<:CH  ?7 B6<64C:>8  72:A 4?:> =999\\n1000 =  0.999  \\n \\n,A?323:<:CH  ?7 B6<64C:>8  D>72:A 4?:> =1\\n1000=  0.001 \\n \\n.6<64C:>8  10 ℎ625B :> 2 A?F \\n=  .6<64C:>8  72:A 4?:> ∗ \\'6CC:>8 10 ℎ625B + .6<64C:>8  D>72:A 4?:> \\n=  ,(!)+ ,(\") \\n \\n,(!)=  0.999 ∗ U1\\n2V\\x07\\x06\\n=  0.999 ∗ U1\\n1024V =  0.000976  \\n \\n,(\")=  0.001 ∗ 1 = 0.001  \\n \\n,(!)\\n,(!)+,(\")= 0.000976\\n0.000976 + 0.001  =  0.4939  \\n \\n,(\")\\n,(!)+,(\")  = 0.001\\n0.001976  =  0.5061  \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 15}),\n",
       " Document(page_content=',A?323:<:CH  ?7 B6<64C:>8  2>?Cℎ6A ℎ625 = ,(!)\\n,(!)+,(\") ∗ 0.5 + ,(\")\\n,(!)+,(\") ∗ 1 = \\n=  0.4939 ∗ 0.5 + 0.5061 =  0.7531  \\n \\nQ15. What do you understand by statistical power of sensitivity and how do you \\ncalculate it? \\n \\nSensitivity is commonly used to validate the accuracy of a classifier (Logistic, SVM, Random Forest etc.). \\n \\n.6>B:C:E:CH  =/,\\n/,+&+  \\n \\nQ16. Why is Re-sampling done? \\n \\nhttps://machinelearningmastery.com/statistical-sampling-and-resampling/   \\n \\n• Sampling is an active process of gathering observations with the intent of estimating a population \\nvariable. \\n• Resampling is a methodology of economically using a data sample to improve the accuracy and \\nquantify the uncertainty of a population parameter. Resampling methods, in fact, make use of a \\nnested resampling method. \\n \\nOnce we have a data sample, it can be used to estimate the population parameter. The problem is that \\nwe only have a single estimate of the population parameter, with little idea of the variability or uncertainty \\nin the estimate. One way to address this is by estimating the population parameter multiple times from \\nour data sample. This is called resampling. Statistical resampling methods are procedures that describe \\nhow to economically use available data to estimate a population parameter. The result can be both a \\nmore accurate estimate of the parameter (such as taking the mean of the estimates) and a quantification \\nof the uncertainty of the estimate (such as adding a confidence interval). \\n \\nResampling methods are very easy to use, requiring little mathematical knowledge. A downside of the \\nmethods is that they can be computationally very expensive, requiring tens, hundreds, or even thousands \\nof resamples in order to develop a robust estimate of the population parameter. \\n \\nThe key idea is to resample form the original data — either directly or via a fitted model — to create \\nreplicate datasets, from which the variability of the quantiles of interest can be assessed without long-\\nwinded and error-prone analytical calculation. Because this approach involves repeating the original data \\nanalysis procedure with many replicate sets of data, these are sometimes called computer-intensive \\nmethods. Each new subsample from the original data sample is used to estimate the population \\nparameter. The sample of estimated population parameters can then be considered with statistical tools \\nin order to quantify the expected value and variance, providing measures of the uncertainty of the \\nestimate. Statistical sampling methods can be used in the selection of a subsample from the original \\nsample. \\n \\nA key difference is that process must be repeated multiple times. The problem with this is that there will \\nbe some relationship between the samples as observations that will be shared across multiple \\nsubsamples. This means that the subsamples and the estimated population parameters are not strictly ', metadata={'source': 'ML questions dump.pdf', 'page': 16}),\n",
       " Document(page_content='identical and independently distributed. This has implications for statistical tests performed on the sample \\nof estimated population parameters downstream, i.e. paired statistical tests may be required. \\n \\nTwo commonly used resampling methods that you may encounter are k-fold cross-validation and the \\nbootstrap. \\n \\n• Bootstrap. Samples are drawn from the dataset with replacement (allowing the same sample to \\nappear more than once in the sample), where those instances not drawn into the data sample \\nmay be used for the test set. \\n• k-fold Cross-Validation. A dataset is partitioned into k groups, where each group is given the \\nopportunity of being used as a held out test set leaving the remaining groups as the training set. \\nThe k-fold cross-validation method specifically lends itself to use in the evaluation of predictive \\nmodels that are repeatedly trained on one subset of the data and evaluated on a second held-out \\nsubset of the data. \\n \\nResampling is done in any of these cases: \\n• Estimating the accuracy of sample statistics by using subsets of accessible data or drawing \\nrandomly with replacement from a set of data points \\n• Substituting labels on data points when performing significance tests \\n• Validating models by using random subsets (bootstrapping, cross-validation) \\n \\nQ17. What are the differences between over-fitting and under-fitting? \\n \\nIn statistics and machine learning, one of the most common tasks is to fit a model to a set of training data, \\nso as to be able to make reliable predictions on general untrained data. \\n \\nIn overfitting, a statistical model describes random error or noise instead of the underlying relationship. \\nOverfitting occurs when a model is excessively complex, such as having too many parameters relative to \\nthe number of observations. A model that has been overfitted, has poor predictive performance, as it \\noverreacts to minor fluctuations in the training data. \\n \\nUnderfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying \\ntrend of the data. Underfitting would occur, for example, when fitting a linear model to non-linear data. \\nSuch a model too would have poor predictive performance. \\n \\nQ18. How to combat Overfitting and Underfitting? \\n \\nTo combat overfitting: \\n1. Add noise  \\n2. Feature selection \\n3. Increase training set \\n \\n4. L2 (ridge) or L1 (lasso) regularization; L1 drops weights, L2 no \\n5. Use cross-validation techniques, such as k folds cross-validation \\n6. Boosting and bagging \\n \\n7. Dropout technique  ', metadata={'source': 'ML questions dump.pdf', 'page': 17}),\n",
       " Document(page_content='8. Perform early stopping \\n \\n9. Remove inner layers \\n \\nTo combat underfitting: \\n1. Add features \\n2. Increase time of training \\n \\nQ19. What is regularization? Why is it useful? \\n \\nRegularization is the process of adding tuning parameter (penalty term) to a model to induce smoothness \\nin order to prevent overfitting. This is most often done by adding a constant multiple to an existing weight \\nvector. This constant is often the L1 (Lasso - | I|) or L2 (Ridge - I\\x08). The model predictions should then \\nminimize the loss function calculated on the regularized training set. \\n \\nQ20. What Is the Law of Large Numbers? \\n \\nIt is a theorem that describes the result of performing the same experiment a large number of times. This \\ntheorem forms the basis of frequency-style thinking. It says that the sample means, the sample variance \\nand the sample standard deviation converge to what they are trying to estimate. According to the law, \\nthe average of the results obtained from a large number of trials should be close to the expected value \\nand will tend to become closer to the expected value as more trials are performed. \\n \\nQ21. What Are Confounding Variables? \\n \\nIn statistics, a confounder is a variable that influences both the dependent variable and independent \\nvariable. \\n \\nIf you are researching whether a lack of exercise leads to weight gain: \\nlack of exercise = independent variable \\nweight gain = dependent variable \\nA confounding variable here would be any other variable that affects both of these variables, such as the \\nage of the subject. \\n \\nQ22. What Are the Types of Biases That Can Occur During Sampling?  \\n \\na. Selection bias \\nb. Under coverage bias \\nc. Survivorship bias \\n \\nQ23. What is Survivorship Bias? \\n \\nIt is the logical error of focusing aspects that support surviving some process and casually overlooking \\nthose that did not work because of their lack of prominence. This can lead to wrong conclusions in \\nnumerous different means. For example, during a recession you look just at the survived businesses, noting ', metadata={'source': 'ML questions dump.pdf', 'page': 18}),\n",
       " Document(page_content='that they are performing poorly. However, they perform better than the rest, which is failed, thus being \\nremoved from the time series. \\n \\nQ24. What is Selection Bias? What is under coverage bias? \\n \\nhttps://stattrek.com/survey-research/survey-bias.aspx   \\n \\nSelection bias occurs when the sample obtained is not representative of the population intended to be \\nanalyzed. For instance, you select only Asians to perform a study on the world population height. \\nUnder coverage bias occurs when some members of the population are inadequately represented in the \\nsample. A classic example of under coverage is the Literary Digest voter survey, which predicted that Alfred \\nLandon would beat Franklin Roosevelt in the 1936 presidential election. The survey sample suffered from \\nunder coverage of low-income voters, who tended to be Democrats. \\nHow did this happen? The survey relied on a convenience sample, drawn from telephone directories and \\ncar registration lists. In 1936, people who owned cars and telephones tended to be more affluent. Under \\ncoverage is often a problem with convenience samples. \\n \\nQ25. Explain how a ROC curve works? \\n \\nThe ROC curve is a graphical representation of the contrast between true positive rates and false positive \\nrates at various thresholds. It is often used as a proxy for the trade-off between the sensitivity (true \\npositive rate) and false positive rate. \\n \\n• /,- =\\x10\\x0f\\n\\x0f=\\x10\\x0f\\n\\x10\\x0f\\x0b\\x0c\\x0e \\n \\n• /+- =\\x10\\x0e\\n\\x10\\x0e\\x0b\\x0c\\x0f=\\x10\\x0e\\n\\x0e \\n \\n• &,- =\\x0c\\x0f\\n\\x10\\x0e\\x0b\\x0c\\x0f \\n \\n• &+-=\\x0c\\x0e\\n\\x0c\\x0e\\x0b\\x10 \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 19}),\n",
       " Document(page_content='Q26. What is TF/IDF vectorization? \\n \\nTF-IDF is short for term frequency-inverse document frequency, is a numerical statistic that is intended to \\nreflect how important a word is to a document in a collection or corpus. It is often used as a weighting \\nfactor in information retrieval and text mining. \\n \\n• /& =# ‘\"\\x1b\\x1d\\x15’ \\x18\\x1a \\x15\\x1b\\x14\\n\\x1f\\x1b\\x1f # \"\\x1b\\x1d\\x15\\x1e \\x18\\x1a \\x15\\x1b\\x14 \\n \\n• )$& =  <?8]# \\x15\\x1b\\x14\\x1e \"\\x18\\x1f\\x17 ‘\"\\x1b\\x1d\\x15’ \\x18\\x1a \\x18\\x1f\\n\\x1f\\x1b\\x1f \\x15\\x1b\\x14\\x1e \\x18\\x1a \\x14\\x1b\\x19\\x19\\x16\\x14\\x1f\\x18\\x1b\\x1a^ \\n \\nThe TF-IDF value increases proportionally to the number of times a word appears in the document but is \\noffset by the frequency of the word in the corpus, which helps to adjust for the fact that some words \\nappear more frequently in general. \\n \\nQ27. Why we generally use Soft-max (or sigmoid) non-linearity function as last \\noperation in-network? Why RELU in an inner layer? \\n \\nIt is because it takes in a vector of real numbers and returns a probability distribution. Its definition is as \\nfollows. Let x be a vector of real numbers (positive, negative, whatever, there are no constraints). \\nThen the i-eth component of soft-max(x) is: \\n \\nIt should be clear that the output is a probability distribution: each element is non-negative and the sum \\nover all components is 1. \\n \\nRELU because it avoids the vanishing gradient descent issue. \\n \\n  \\n', metadata={'source': 'ML questions dump.pdf', 'page': 20}),\n",
       " Document(page_content='Data Analysis  \\n \\nQ1. Python or R – Which one would you prefer for text analytics?  \\n \\nWe will prefer Python because of the following reasons: \\n• Python would be the best option because it has Pandas library that provides easy to use data \\nstructures and high-performance data analysis tools. \\n• R is more suitable for machine learning than just text analysis. \\n• Python performs faster for all types of text analytics. \\n \\nQ2. How does data cleaning play a vital role in the analysis? \\n \\nData cleaning can help in analysis because: \\n• Cleaning data from multiple sources helps transform it into a format that data analysts or data \\nscientists can work with. \\n• Data Cleaning helps increase the accuracy of the model in machine learning. \\n• It is a cumbersome process because as the number of data sources increases, the time taken to \\nclean the data increases exponentially due to the number of sources and the volume of data \\ngenerated by these sources. \\n• It might take up to 80% of the time for just cleaning data making it a critical part of the analysis \\ntask. \\n \\nQ3. Differentiate between univariate, bivariate and multivariate analysis. \\n \\nUnivariate analyses are descriptive statistical analysis techniques which can be differentiated based on \\none variable involved at a given point of time.  For example, the pie charts of sales based on territory \\ninvolve only one variable and can the analysis can be referred to as univariate analysis. \\n \\nThe bivariate analysis attempts to understand the difference between two variables at a time as in a \\nscatterplot. For example, analyzing the volume of sale and spending can be considered as an example of \\nbivariate analysis. \\n \\nMultivariate analysis deals with the study of more than two variables to understand the effect of variables \\non the responses. \\n \\nQ4. Explain Star Schema. \\n \\nIt is a traditional database schema with a central table. Satellite tables map IDs to physical names or \\ndescriptions and can be connected to the central fact table using the ID fields; these tables are known as \\nlookup tables and are principally useful in real-time applications, as they save a lot of memory. Sometimes \\nstar schemas involve several layers of summarization to recover information faster. \\n \\nQ5. What is Cluster Sampling? \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 21}),\n",
       " Document(page_content='Cluster sampling is a technique used when it becomes difficult to study the target population spread \\nacross a wide area and simple random sampling cannot be applied. Cluster Sample is a probability sample \\nwhere each sampling unit is a collection or cluster of elements. \\nFor example, a researcher wants to survey the academic performance of high school students in Japan. He \\ncan divide the entire population of Japan into different clusters (cities). Then the researcher selects a \\nnumber of clusters depending on his research through simple or systematic random sampling. \\n \\nQ6. What is Systematic Sampling? \\n \\nSystematic sampling is a statistical technique where elements are selected from an ordered sampling \\nframe. In systematic sampling, the list is progressed in a circular manner so once you reach the end of the \\nlist, it is progressed from the top again. The best example of systematic sampling is equal probability \\nmethod. \\n \\nQ7. What are Eigenvectors and Eigenvalues? \\n \\nEigenvectors are used for understanding linear transformations. In data analysis, we usually calculate the \\neigenvectors for a correlation or covariance matrix. Eigenvectors are the directions along which a \\nparticular linear transformation acts by flipping, compressing or stretching. \\nEigenvalue can be referred to as the strength of the transformation in the direction of eigenvector or the \\nfactor by which the compression occurs. \\n \\nQ8. Can you cite some examples where a false positive is important than a false \\nnegative? \\n \\nLet us first understand what false positives and false negatives are \\n \\n• False Positives are the cases where you wrongly classified a non-event as an event a.k.a Type I \\nerror. \\n• False Negatives are the cases where you wrongly classify events as non-events, a.k.a Type II error. \\n \\nExample 1: In the medical field, assume you have to give chemotherapy to patients. Assume a patient \\ncomes to that hospital and he is tested positive for cancer, based on the lab prediction but he actually \\ndoesn’t have cancer. This is a case of false positive. Here it is of utmost danger to start chemotherapy on \\nthis patient when he actually does not have cancer. In the absence of cancerous cell, chemotherapy will \\ndo certain damage to his normal healthy cells and might lead to severe diseases, even cancer. \\n \\nExample 2: Let’s say an e-commerce company decided to give $1000 Gift voucher to the customers whom \\nthey assume to purchase at least $10,000 worth of items. They send free voucher mail directly to 100 \\ncustomers without any minimum purchase condition because they assume to make at least 20% profit on \\nsold items above $10,000. Now the issue is if we send the $1000 gift vouchers to customers who have not \\nactually purchased anything but are marked as having made $10,000 worth of purchase. \\n \\nQ9. Can you cite some examples where a false negative important than a false \\npositive? And vice versa? \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 22}),\n",
       " Document(page_content='Example 1 FN: What if Jury or judge decides to make a criminal go free? \\n \\nExample 2 FN: Fraud detection. \\n \\nExample 3 FP: customer voucher use promo evaluation: if many used it and actually if was not true, \\npromo sucks. \\n \\nQ10. Can you cite some examples where both false positive and false negatives \\nare equally important? \\n \\nIn the Banking industry giving loans is the primary source of making money but at the same time if your \\nrepayment rate is not good you will not make any profit, rather you will risk huge losses. \\nBanks don’t want to lose good customers and at the same point in time, they don’t want to acquire bad \\ncustomers. In this scenario, both the false positives and false negatives become very important to measure. \\n \\nQ11. Can you explain the difference between a Validation Set and a Test Set? \\n \\nA Training Set: \\n• to fit the parameters i.e. weights  \\n \\nA Validation set: \\n• part of the training set \\n• for parameter selection \\n• to avoid overfitting \\n \\nA Test set: \\n• for testing or evaluating the performance of a trained machine learning model, i.e. evaluating the \\npredictive power and generalization. \\n \\nQ12. Explain cross-validation. \\n \\nhttps://machinelearningmastery.com/k-fold-cross-validation/ \\n \\nCross-validation is a resampling procedure used to evaluate machine learning models on a limited data \\nsample. The procedure has a single parameter called k that refers to the number of groups that a given \\ndata sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a \\nspecific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 \\nbecoming 10-fold cross-validation. Mainly used in backgrounds where the objective is forecast, and one \\nwants to estimate how accurately a model will accomplish in practice.  \\n \\nCross-validation is primarily used in applied machine learning to estimate the skill of a machine learning \\nmodel on unseen data. That is, to use a limited sample in order to estimate how the model is expected to \\nperform in general when used to make predictions on data not used during the training of the model. \\n \\nIt is a popular method because it is simple to understand and because it generally results in a less biased \\nor less optimistic estimate of the model skill than other methods, such as a simple train/test split. ', metadata={'source': 'ML questions dump.pdf', 'page': 23}),\n",
       " Document(page_content=' \\nThe general procedure is as follows: \\n \\n1. Shuffle the dataset randomly. \\n2. Split the dataset into k groups \\n3. For each unique group: \\na. Take the group as a hold out or test data set \\nb. Take the remaining groups as a training data set \\nc. Fit a model on the training set and evaluate it on the test set \\nd. Retain the evaluation score and discard the model \\n4. Summarize the skill of the model using the sample of model evaluation scores  \\n \\nThere is an alternative in Scikit-Learn called Stratified k fold, in which the split is shuffled to make it sure \\nyou have a representative sample of each class and a k fold in which you may not have the assurance of \\nit (not good with a very unbalanced dataset). \\n  \\n', metadata={'source': 'ML questions dump.pdf', 'page': 24}),\n",
       " Document(page_content='Machine Learning \\n \\nQ1. What is Machine Learning? \\n \\nMachine learning is the study of computer algorithms that improve automatically through experience. It \\nis seen as a subset of artificial intelligence. Machine Learning explores the study and construction of \\nalgorithms that can learn from and make predictions on data. You select a model to train and then \\nmanually perform feature extraction. Used to devise complex models and algorithms that lend themselves \\nto a prediction which in commercial use is known as predictive analytics. \\n \\nQ2. What is Supervised Learning? \\n \\nSupervised learning is the machine learning task of inferring a function from labeled training data. The \\ntraining data consist of a set of training examples. \\n \\nAlgorithms: Support Vector Machines, Regression, Naive Bayes, Decision Trees, K-nearest Neighbor \\nAlgorithm and Neural Networks \\n \\nE.g. If you built a fruit classifier, the labels will be “this is an orange, this is an apple and this is a banana”, \\nbased on showing the classifier examples of apples, oranges and bananas. \\n \\nQ3. What is Unsupervised learning? \\n \\nUnsupervised learning is a type of machine learning algorithm used to draw inferences from datasets \\nconsisting of input data without labelled responses. \\n \\nAlgorithms: Clustering, Anomaly Detection, Neural Networks and Latent Variable Models \\n \\nE.g. In the same example, a fruit clustering will categorize as “fruits with soft skin and lots of dimples”, \\n“fruits with shiny hard skin” and “elongated yellow fruits”. \\n \\nQ4. What are the various algorithms? \\n \\nThere are various algorithms. Here is a list. ', metadata={'source': 'ML questions dump.pdf', 'page': 25}),\n",
       " Document(page_content=' \\n \\n \\n \\n \\nQ5. What is ‘Naive’ in a Naive Bayes? \\n \\nhttps://en.wikipedia.org/wiki/Naive_Bayes_classifier \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 26}),\n",
       " Document(page_content='Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with \\nthe “naive” assumption of conditional independence between every pair of features given the value of \\nthe class variable. Bayes’ theorem states the following relationship, given class variable y and dependent \\nfeature vector G\\x07through G\\x1a: \\nUsing the naive conditional independence assumption that each G\\x18 is independent: \\nfor all :, this relationship is simplified to: \\nSince ,(G\\x07,…,G\\x1a) is constant given the input, we can use the following classification rule: \\n \\n \\nand we can use Maximum A Posteriori (MAP) estimation to estimate ,(H) and ,(H|G\\x18); the former is then \\nthe relative frequency of class H in the training set. \\n \\nThe different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution \\nof ,(H|G\\x18): can be Bernoulli, Binomial, Gaussian, and so on. \\n \\nQ6. What is PCA? When do you use it? \\n \\nhttps://en.wikipedia.org/wiki/Principal_component_analysis \\nhttps://blog.umetrics.com/what-is-principal-component-analysis-pca-and-how-it-is-used   \\nhttps://blog.umetrics.com/why-preprocesing-data-creates-better-data-analytics-models   \\n \\nPrincipal component analysis (PCA) is a statistical method used in Machine Learning. It consists in \\nprojecting data in a higher dimensional space into a lower dimensional space by maximizing \\nthe variance of each dimension.  \\nThe process works as following. We define a matrix A with > rows (the single observations of a dataset – \\nin a tabular format, each single row) and @ columns, our features. For this matrix we construct a variable \\nspace with as many dimensions as there are features. Each feature represents one coordinate axis. For \\n', metadata={'source': 'ML questions dump.pdf', 'page': 27}),\n",
       " Document(page_content='each feature, the length has been standardized according to a scaling criterion, normally by scaling to unit \\nvariance. It is determinant to scale the features to a common scale, otherwise the features with a greater \\nmagnitude will weigh more in determining the principal components. Once plotted all the observations \\nand computed the mean of each variable, that mean will be represented by a point in the center of our \\nplot (the center of gravity). Then, we subtract each observation with the mean, shifting the coordinate \\nsystem with the center in the origin. The best fitting line resulting is the line that best accounts for the \\nshape of the point swarm. It represents the maximum variance direction in the data. Each observation \\nmay be projected onto this line in order to get a coordinate value along the PC-line. This value is known \\nas a score. The next best-fitting line can be similarly chosen from directions perpendicular to the first. \\nRepeating this process yields an orthogonal basis in which different individual dimensions of the data are \\nuncorrelated. These basis vectors are called principal components . \\n \\nPCA is mostly used as a tool in exploratory data analysis and for making predictive models. It is often used \\nto visualize genetic distance and relatedness between populations.  \\n \\nQ7. Explain SVM algorithm in detail. \\n \\nhttps://en.wikipedia.org/wiki/Support_vector_machine  \\n \\nClassifying data is a common task in machine learning. Suppose some given data points each belong to \\none of two classes, and the goal is to decide which class a new data point will be in. In the case of support-\\nvector machines, a data point is viewed as a p-dimensional vector (a list of @ numbers), and we want to \\nknow whether we can separate such points with a ( @−1)-dimensional hyperplane. This is called a linear \\nclassifier. There are many hyperplanes that might classify the data. One reasonable choice as the best \\nhyperplane is the one that represents the largest separation, or margin, between the two classes. So, we \\nchoose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If \\nsuch a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines \\nis known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability. The best \\nhyper plane that divides the data is (\\t. \\nWe have n data (G\\x07,H\\x07),…,(G\\x1a,H\\x1a) and p different features G\\x18= (G\\x18\\x07,…,G\\x18\\x1c) and H\\x18 is either 1 or -1. \\nThe equation of the hyperplane (\\t is as the set of points x satisfying:  \\n', metadata={'source': 'ML questions dump.pdf', 'page': 28}),\n",
       " Document(page_content=' \\nF ∙G−3= 0 \\n \\nwhere F is the (not necessarily normalized) normal vector to the hyperplane. The parameter\\n\\x13\\n‖\"‖ determines the offset of the hyperplane from the origin along the normal vector w. \\n \\nSo, for each  :, either G\\x18 is in the hyperplane of 1 or -1. Basically, G\\x18 satisfies: \\nF ∙G\\x18−3≥ 1      ?A     F ∙G\\x18−3≤ −1   \\n \\n• SVMs are helpful in text and hypertext categorization, as their application can significantly reduce \\nthe need for labeled training instances in both the standard inductive and transductive settings. \\nSome methods for shallow semantic parsing are based on support vector machines. \\n• Classification of images can also be performed using SVMs. Experimental results show that SVMs \\nachieve significantly higher search accuracy than traditional query refinement schemes after just \\nthree to four rounds of relevance feedback. \\n• Classification of satellite data like SAR data using supervised SVM. \\n• Hand-written characters can be recognized using SVM. \\n \\nQ8. What are the support vectors in \\nSVM? \\n \\nIn the diagram, we see that the sketched lines mark the \\ndistance from the classifier (the hyper plane) to the closest \\ndata points called the support vectors (darkened data \\npoints). The distance between the two thin lines is called the \\nmargin. \\n \\nTo extend SVM to cases in which the data are not linearly \\nseparable, we introduce the hinge loss  function, \\n \\nmax (0,1− H\\x18(F∙G\\x18−3)) \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 29}),\n",
       " Document(page_content=\"This function is zero if x lies on the correct side of the margin. For data on the wrong side of the margin, \\nthe function's value is proportional to the distance from the margin. \\n \\nQ9. What are the different kernels in SVM? \\n \\nThere are four types of kernels in SVM. \\n1. LinearKernel \\n2. Polynomial kernel \\n3. Radial basis kernel \\n4. Sigmoid kernel \\n \\nQ10. What are the most known ensemble algorithms? \\n \\nhttps://towardsdatascience.com/the-ultimate-guide-to-adaboost-random-forests-and-xgboost-7f9327061c4f \\n  \\nThe most popular trees are: AdaBoost, Random Forest, and eXtreme Gradient Boosting (XGBoost). \\n \\nAdaBoost is  best used  in a dataset with low noise, when computational complexity or timeliness of results \\nis not a main concern and when there are not enough resources for broader hyperparameter tuning due \\nto lack of time and knowledge of the user. \\n \\nRandom forests should not be used when dealing with time series data or any other data where look-\\nahead bias should be avoided, and the order and continuity of the samples need to be ensured. This \\nalgorithm can handle noise relatively well, but more knowledge from the user is required to adequately \\ntune the algorithm compared to AdaBoost. \\n \\nThe main advantages of XGBoost is its lightning speed compared to other algorithms, such as AdaBoost, \\nand its regularization parameter that successfully reduces variance. But even aside from the regularization \\nparameter, this algorithm leverages a learning rate (shrinkage) and subsamples from the features like \\nrandom forests, which increases its ability to generalize even further. However, XGBoost is more difficult \\nto understand, visualize and to tune compared to AdaBoost and random forests. There is a multitude of \\nhyperparameters that can be tuned to increase performance. \\n \\nQ11. Explain Decision Tree algorithm in detail. \\n \\nhttps://en.wikipedia.org/wiki/Decision_tree_learning   \\nhttps://www.kdnuggets.com/2019/02/decision-trees-introduction.html/2   \\nhttps://medium.com/@naeemsunesara/giniscore-entropy-and-information-gain-in-decision-trees-\\ncbc08589852d   \\n \\nA decision tree is a supervised machine learning algorithm mainly used for Regression and Classification. \\nIt breaks down a data set into smaller and smaller subsets while at the same time an associated decision \\ntree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision \\ntree can handle both categorical and numerical data. The term Classification and Regression Tree (CART) \\nanalysis is an umbrella term used to refer to both of the above procedures. \\n \\nSome techniques, often called ensemble  methods, construct more than one decision tree: \", metadata={'source': 'ML questions dump.pdf', 'page': 30}),\n",
       " Document(page_content='• Boosted trees  Incrementally building an ensemble by training each new instance to emphasize \\nthe training instances previously mis-modeled. A typical example is AdaBoost . These can be used \\nfor regression-type and classification-type problems.  \\n• Bootstrap aggregated  (or bagged) decision trees, an early ensemble method, builds multiple \\ndecision trees by repeatedly resampling training data with replacement, and voting the trees for \\na consensus prediction.  \\no A random forest  classifier is a specific type of bootstrap aggregating . \\n• Rotation forest  – in which every decision tree is trained by first applying principal component \\nanalysis (PCA) on a random subset of the input features.  \\n \\nA special case of a decision tree is a decision list, which is a one-sided decision tree, so that every internal \\nnode has exactly 1 leaf node and exactly 1 internal node as a child (except for the bottommost node, \\nwhose only child is a single leaf node). While less expressive, decision lists are arguably easier to \\nunderstand than general decision trees due to their added sparsity, permit non-greedy learning methods \\nand monotonic constraints to be imposed. \\n \\nNotable decision tree algorithms include: \\n \\n• ID3 (Iterative Dichotomiser 3) \\n• C4.5 (successor of ID3) \\n• CART (Classification and Regression Tree) \\n• Chi-square automatic interaction detection (CHAID). Performs multi-level splits when computing \\nclassification trees. \\n• MARS: extends decision trees to handle numerical data better. \\n• Conditional Inference Trees. Statistics-based approach that uses non-parametric tests as splitting \\ncriteria, corrected for multiple testing to avoid overfitting. This approach results in unbiased \\npredictor selection and does not require pruning. \\n \\nQ12. What are Entropy and Information gain in Decision tree algorithm? \\n \\nhttps://www.saedsayad.com/decision_tree.htm   \\nhttps://medium.com/@naeemsunesara/giniscore-entropy-and-information-gain-in-decision-trees-\\ncbc08589852d   \\n \\nThere are a lot of algorithms which are employed to build a decision tree, ID3 (Iterative Dichotomiser 3), \\nC4.5, C5.0, CART (Classification and Regression Trees) to name a few but at their core all of them tell us \\nwhat questions to ask and when. \\n \\nThe below table has color and diameter of a fruit and the label tells the name of the fruit. How do we build \\na decision tree to classify the fruits? \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 31}),\n",
       " Document(page_content=' \\n \\nHere is how we will build the tree. We will start with a node which will ask a true or false question to split \\nthe data into two. The two resulting nodes will each ask a true or false question again to split the data \\nfurther and so on. \\nThere are 2 main things to consider with the above approach: \\n• Which is the best question to ask at each node \\n• When do we stop splitting the data further? \\n \\nLet’s start building the tree with the first or the topmost node. There is a list of possible questions which \\ncan be asked. The first node can ask the following questions: \\n• Is the color green? \\n• Is the color yellow? \\n• Is the color red? \\n• Is the diameter ≥ 3? \\n• Is the diameter ≥ 1? \\n \\nOf these possible set of questions, which one is the best to ask so that our data is split into two sets after \\nthe first node? Remember we are trying to split or classify our data into separate classes. Our question \\nshould be such that our data is partitioned into as unmixed or pure classes as possible. An impure set or \\nclass here refers to one which has many different types of objects for example if we ask the question for \\nthe above data, “Is the color green?” our data will be split into two sets one of which will be pure the other \\nwill have a mixed set of labels. If we assign a label to a mixed set, we have higher chances of being \\nincorrect. But how do we measure this impurity? \\n \\n \\n \\nGini Impurity and Information Gain - CART \\n \\nCART (Classification and Regression Trees) → uses Gini Index (Classification)  as metric. \\n', metadata={'source': 'ML questions dump.pdf', 'page': 32}),\n",
       " Document(page_content='The Gini Impurity (GI) metric measures the homogeneity of a set of items. The lowest possible value of GI \\nis 0.0. The maximum value of GI depends on the particular problem being investigated but gets close to \\n1.0. \\nSuppose for example you have 12 items — apples, grapes, lemons. If there are 0 apples, 0 grapes, 12 lemons, \\nthen you have minimal impurity (this is good for decision trees) and GI = 0.0. But if you have 4 apples, 4 \\ngrapes, 4 lemons, you have maximum impurity and it turns out that GI = 0.667. \\nI’ll show example calculations.  \\nMaximum GI: Apples, Grapes, Lemons \\n \\n \\n \\nWhen the number of items is evenly distributed, as in the example above, you have maximum GI but the \\nexact value depends on how many items there are. A bit less than maximum GI: \\n \\n \\nIn the example above, the items are not quite evenly distributed, and the GI is slightly less (which is better \\nwhen used for decision trees). Minimum GI: \\n \\n \\n \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 33}),\n",
       " Document(page_content=' \\n \\n \\n \\n \\n \\nThe Gini index  is not at all the same as a different metric called the  Gini coefficient . The Gini impurity \\nmetric can be used when creating a decision tree but there are alternatives, including Entropy \\nInformation gain . The advantage of GI is its simplicity. \\n \\nInformation Gain \\nInformation gain is another metric which tells us how much a question unmixes the labels at a \\nnode. “Mathematically it is just a difference between impurity values before splitting the data at a node \\nand the weighted average of the impurity after the split” . For instance, if we go back to our data of \\napples, lemons and grapes and ask the question “Is the color Green?” \\n \\n \\nThe information gain by asking this question is 0.144. Similarly, we can ask another question from the set \\nof possible questions split the data and compute information gain. This is also called (Recursive Binary \\nSplitting).  \\n \\n \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 34}),\n",
       " Document(page_content='The question where we have the highest information gain “Is diameter ≥ 3?”  is the best question to ask. \\nNote that the information gain is same for the question “Is the color red?” we just picked the first one at \\nrandom. \\nRepeating the same method at the child node we can complete the tree. Note that no further questions \\ncan be asked which would increase the information gain. \\n \\n \\n \\nAlso note that the rightmost leaf which says 50% Apple & 50% lemon means that this class cannot be \\ndivided further, and this branch can tell an apple or a lemon with 50% probability. For the grape and apple \\nbranches we stop asking further questions since the Gini Impurity is 0 for those. \\n \\nEntropy and Information Gain – ID3 \\n \\nID3 (Iterative Dichotomiser 3) → uses Entropy function and Information gain  as metrics. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 35}),\n",
       " Document(page_content=' \\nIf the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has \\nentropy of one. \\n \\n \\nTo build a decision tree, we need to calculate two types of entropy using frequency tables as follows: \\n \\na) Entropy using the frequency table of one attribute: \\n \\nb) Entropy using the frequency table of two attributes: \\n \\n \\n \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 36}),\n",
       " Document(page_content=' \\n \\nInformation Gain \\nThe information gain is based on the decrease in entropy after a dataset is split on an attribute. \\nConstructing a decision tree is all about finding attribute that returns the highest information gain (i.e., \\nthe most homogeneous branches).  \\n \\nStep 1: Calculate entropy of the target. \\n \\n \\nStep 2: The dataset is then split on the different attributes. The entropy for each branch is calculated. \\nThen it is added proportionally, to get total entropy for the split. The resulting entropy is subtracted from \\nthe entropy before the split. The result is the Information Gain or decrease in entropy. \\n \\n \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 37}),\n",
       " Document(page_content=' \\n \\nStep 3: Choose attribute with the largest information gain as the decision node, divide the dataset by its \\nbranches and repeat the same process on every branch. \\n \\n \\n \\n \\n \\n \\nStep 4a: A branch with entropy of 0 is a leaf node. \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 38}),\n",
       " Document(page_content=' \\n \\nStep 4b: A branch with entropy more than 0 needs further splitting. \\n \\n \\n \\nStep 5: The ID3 algorithm is run recursively on the non-leaf branches, until all data is classified. \\n \\nQ13. What is pruning in Decision Tree? \\n \\nPruning is a technique in machine learning and search algorithms that reduces the size of decision trees \\nby removing sections of the tree that provide little power to classify instances. So, when we remove sub-\\nnodes of a decision node, this process is called pruning or opposite process of splitting. \\n \\nQ14. What is logistic regression? State an example when you have used logistic \\nregression recently. \\n \\nLogistic Regression often referred to as the logit model is a technique to predict the binary outcome from \\na linear combination of predictor variables. Since we are interested in a probability outcome, a line does \\nnot fit the model. Logistic Regression is a classification algorithm that works by trying to learn a function \\n', metadata={'source': 'ML questions dump.pdf', 'page': 39}),\n",
       " Document(page_content='that approximates ,(\\xa0|0). It makes the central assumption that ,(0|\\xa0) can be approximated as a \\nsigmoid function applied to a linear combination of input features. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFor example, if you want to predict whether a particular political leader will win the election or not. In this \\ncase, the outcome of prediction is binary i.e. 0 or 1 (Win/Lose). The predictor variables here would be the \\namount of money spent for election campaigning of a particular candidate, the amount of time spent in \\ncampaigning, etc. \\n \\nQ15. What is Linear Regression? \\n \\nLinear regression is a statistical technique where the score of a variable Y is predicted from the score of a \\nsecond variable X. X is referred to as the predictor variable and Y as the criterion variable. \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 40}),\n",
       " Document(page_content=' \\n \\nQ16. What Are the Drawbacks of the Linear Model? \\n \\nSome drawbacks of the linear model are: \\n• The assumption of linearity of the model \\n• It can’t be used for count outcomes or binary outcomes. \\n• There are overfitting or underfitting problems that it can’t solve. \\n \\nQ17. What is the difference between Regression and classification ML \\ntechniques? \\n \\nBoth Regression and classification machine learning techniques come under Supervised machine learning \\nalgorithms. In Supervised machine learning algorithm, we have to train the model using labelled data set, \\nwhile training we have to explicitly provide the correct labels and algorithm tries to learn the pattern from \\ninput to output. If our labels are discrete values then it will a classification problem, but if our labels are \\ncontinuous values then it will be a regression problem. \\n \\nQ18. What are Recommender Systems? \\n \\nhttps://en.wikipedia.org/wiki/Recommender_system   \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 41}),\n",
       " Document(page_content='Recommender Systems are a subclass of information filtering systems that are meant to predict the \\npreferences or ratings that a user would give to a product. Recommender systems are widely used in \\nmovies, news, research articles, products, social tags, music, etc. \\nExamples include movie recommenders in IMDB, Netflix & BookMyShow, product recommenders in e- \\ncommerce sites like Amazon, eBay & Flipkart, YouTube video recommendations and game \\nrecommendations in Xbox. \\n \\nQ19. What is Collaborative filtering? And a content based? \\n \\nThe process of filtering used by most of the recommender systems to find patterns or information by \\ncollaborating viewpoints, various data sources and multiple agents. Collaborative filtering is a technique \\nthat can filter out items that a user might like on the basis of reactions by similar users. It works by \\nsearching a large group of people and finding a smaller set of users with tastes similar to a particular user. \\nIt looks at the items they like (usually based on rating) and combines them to create a ranked list of \\nsuggestions. Similar users are those with similar rating and on the based on that they get \\nrecommendations. In content based, we look only at the item level, recommending on similar items sold. \\n \\nAn example of collaborative filtering can be to predict the rating of a particular user based on his/her \\nratings for other movies and others’ ratings for all movies. This concept is widely used in recommending \\nmovies in IMDB, Netflix & BookMyShow, product recommenders in e-commerce sites like Amazon, eBay & \\nFlipkart, YouTube video recommendations and game recommendations in Xbox. \\n \\nQ20. How can outlier values be treated? \\n \\nOutlier values can be identified by using univariate or any other graphical analysis method. If the number \\nof outlier values is few then they can be assessed individually but for a large number of outliers, the values \\ncan be substituted with either the 99th or the 1st percentile values. \\nAll extreme values are not outlier values. The most common ways to treat outlier values: \\n1. Change it with a mean or median \\n2. Standardize the feature, changing the distribution but smoothing the outliers \\n3. Log transform the feature (with many outliers) \\n4. Drop the value \\n', metadata={'source': 'ML questions dump.pdf', 'page': 42}),\n",
       " Document(page_content='5. First/third quartile value if more than 2 M \\n \\nQ21. What are the various steps involved in an analytics project? \\n \\nThe following are the various steps involved in an analytics project: \\n1. Understand the Business problem \\n2. Explore the data and become familiar with it \\n3. Prepare the data for modeling by detecting outliers, treating missing values, transforming \\nvariables, etc. \\n4. After data preparation, start running the model, analyze the result and tweak the approach. This \\nis an iterative step until the best possible outcome is achieved. \\n5. Validate the model using a new data set. \\n6. Start implementing the model and track the result to analyze the performance of the model over \\nthe period of time. \\n \\nQ22. During analysis, how do you treat missing values? \\n \\nThe extent of the missing values is identified after identifying the variables with missing values. If any \\npatterns are identified the analyst has to concentrate on them as it could lead to interesting and \\nmeaningful business insights. \\nIf there are no patterns identified, then the missing values can be substituted with mean or median values \\n(imputation) or they can simply be ignored. Assigning a default value which can be mean, minimum or \\nmaximum value. Getting into the data is important. \\nIf it is a categorical variable, the default value is assigned. The missing value is assigned a default value. If \\nyou have a distribution of data coming, for normal distribution give the mean value. \\nIf 80% of the values for a variable are missing, then you can answer that you would be dropping the \\nvariable instead of treating the missing values. \\n \\nQ23. How will you define the number of clusters in a clustering algorithm? \\n \\nhttps://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/  \\n \\nThough the Clustering Algorithm is not specified, this question is mostly in reference to K-Means clustering \\nwhere “K” defines the number of clusters. The objective of clustering is to group similar entities in a way \\nthat the entities within a group are similar to each other, but the groups are different from each other. ', metadata={'source': 'ML questions dump.pdf', 'page': 43}),\n",
       " Document(page_content='For example, the following image shows three different groups. \\nWithin Sum of squares is generally used to explain the homogeneity within a cluster. If you plot WSS (as \\nthe sum of the squared distance between each member of the cluster and its centroid) for a range of \\nnumber of clusters, you will get the plot shown below. \\n• The Graph is generally known as Elbow Curve. \\n• Red circled a point in above graph i.e. Number of Cluster = 3 is the point after which you don’t see \\nany decrement in WSS. \\n• This point is known as the bending point and taken as K in K – Means. \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 44}),\n",
       " Document(page_content='This is the widely used approach but few data scientists also use Hierarchical clustering first to create \\ndendrograms and identify the distinct groups from there. \\n \\nThe algorithm starts by finding the two points that are closest to each other on the basis of Euclidean \\ndistance. If we look back at Graph1, we can see that points 2 and 3 are closest to each other while points \\n7 and 8 are closes to each other. Therefore a cluster will be formed between these two points first. In \\nGraph2, you can see that the dendograms have been created joining points 2 with 3, and 8 with 7. The \\nvertical height of the dendogram shows the Euclidean distances between points. From Graph2, it can be \\nseen that Euclidean distance between points 8 and 7 is greater than the distance between point 2 and 3. \\nThe next step is to join the cluster formed by joining two points to the next nearest cluster or point which \\nin turn results in another cluster. If you look at Graph1, point 4 is closest to cluster of point 2 and 3, \\ntherefore in Graph2 dendrogram is generated by joining point 4 with dendrogram of point 2 and 3. This \\nprocess continues until all the points are joined together to form one big cluster. \\nOnce one big cluster is formed, the longest vertical distance without any horizontal line passing through \\nit is selected and a horizontal line is drawn through it. The number of vertical lines this newly created \\nhorizontal line passes is equal to number of clusters. Take a look at the following plot: \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 45}),\n",
       " Document(page_content='We can see that the largest vertical distance without any horizontal line passing through it is represented \\nby blue line. So we draw a new horizontal red line that passes through the blue line. Since it crosses the \\nblue line at two points, therefore the number of clusters will be 2. Basically the horizontal line is a \\nthreshold, which defines the minimum distance required to be a separate cluster. If we draw a line further \\ndown, the threshold required to be a new cluster will be decreased and more clusters will be formed as \\nsee in the image below: \\n \\nIn the above plot, the horizontal line passes through four vertical lines resulting in four clusters: cluster \\nof points 6,7,8 and 10, cluster of points 3,2,4 and points 9 and 5 will be treated as single point clusters. \\n \\nQ24. What is Ensemble Learning? \\n \\nIn statistics and machine learning , ensemble methods use multiple learning algorithms to obtain \\nbetter predictive performance  than could be obtained from any of the constituent learning algorithms \\nalone. Ensembles are a divide-and-conquer approach used to improve performance. The main principle \\nbehind ensemble methods is that a group of “weak learners” can come together to form a “strong \\n', metadata={'source': 'ML questions dump.pdf', 'page': 46}),\n",
       " Document(page_content='learner”. Each classifier, individually, is a “weak learner,” while all the classifiers taken together are a \\n“strong learner”. \\n \\n \\nQ25. Describe in brief any type of Ensemble Learning. \\n \\nhttps://medium.com/@ruhi3929/bagging-and-boosting-method-c036236376eb  \\n \\nEnsemble learning has many types but two more popular ensemble learning techniques are mentioned \\nbelow. \\n \\nBagging  \\n \\nBagging tries to implement similar learners on small sample populations and then takes a mean of all the \\npredictions. In generalized bagging, you can use different learners on different population. As you expect \\nthis helps us to reduce the variance error.   \\n \\nPros \\nØ Bagging method helps when we face variance or overfitting in the model. It provides an \\nenvironment to deal with variance by using N learners of same size on same algorithm. \\nØ During the sampling of train data, there are many observations which overlaps. So, the \\ncombination of these learners helps in overcoming the high variance. \\nØ Bagging uses Bootstrap sampling method (Bootstrapping is any test or metric that uses \\nrandom sampling with replacement and  falls under the broader class of resampling methods.) \\nCons \\nØ Bagging is not helpful in case of bias or underfitting in the data. \\nØ Bagging ignores the value with the highest and the lowest result which may have a wide difference \\nand provides an average result. \\n \\nBoosting \\n \\nBoosting is an iterative technique which adjusts the weight of an observation based on the last \\nclassification. If an observation was classified incorrectly, it tries to increase the weight of this observation \\nand vice versa. Boosting in general decreases the bias error and builds strong predictive models. However, \\nthey may over fit on the training data. \\n \\nPros \\nØ Boosting technique takes care of the weightage of the higher accuracy sample and lower accuracy \\nsample and then gives the combined results. \\nØ Net error is evaluated in each learning steps. It works good with interactions. \\nØ Boosting technique helps when we are dealing with bias or underfitting in the data set. \\nØ Multiple boosting techniques are available. For example: AdaBoost, LPBoost, XGBoost, \\nGradientBoost, BrownBoost \\nCons \\nØ Boosting technique often ignores overfitting or variance issues in the data set. ', metadata={'source': 'ML questions dump.pdf', 'page': 47}),\n",
       " Document(page_content='Ø It increases the complexity of the classification. \\nØ Time and computation can be a bit expensive. \\n \\n \\nThere are multiple areas where Bagging and Boosting technique is used to boost the accuracy. \\n• Banking: Loan defaulter prediction, fraud transaction \\n• Credit risks \\n• Kaggle competitions \\n• Fraud detection \\n• Recommender system for Netflix \\n• Malware \\n• Wildlife conservations and so on. \\n \\nQ26. What is a Random Forest? How does it work? \\n \\nRandom forest is a versatile machine learning method capable of performing: \\n• regression  \\n• classification \\n• dimensionality reduction \\n• treat missing values \\n• outlier values \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 48}),\n",
       " Document(page_content='It is a type of ensemble learning method, where a group of weak models combine to form a powerful \\nmodel. The random forest starts with a standard machine learning technique called a “decision tree” \\nwhich, in ensemble terms, corresponds to our weak learner. In a decision tree, an input is entered at the \\ntop and as it traverses down the tree the data gets bucketed into smaller and smaller sets. \\nIn Random Forest, we grow multiple trees as opposed to a single tree. To classify a new object based on \\nattributes, each tree gives a classification. The forest chooses the classification having the most votes \\n(Over all the trees in the forest) and in case of regression, it takes the average of outputs by different \\ntrees. \\n \\nQ27. How Do You Work Towards a Random Forest? \\n \\nhttps://blog.citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics   \\n \\nThe underlying principle of this technique is that several weak learners combined to provide a keen \\nlearner. Here is how such a system is trained for some number of trees T: \\n \\n1. Sample N cases at random with replacement to create a subset of the data. The subset should be \\nabout 66% of the total set. \\n2. At each node: \\na. For some number  m (see below) , m predictor variables are selected at random from all \\nthe predictor variables. \\nb. The predictor variable that provides the best split, according to some objective function, \\nis used to do a binary split on that node. \\nc. At the next node, choose another m variables at random from all predictor variables and \\ndo the same. \\n \\nDepending upon the value of m, there are three slightly different systems: \\n', metadata={'source': 'ML questions dump.pdf', 'page': 49}),\n",
       " Document(page_content=' \\n• Random splitter selection: == 1 \\n• Breiman’s bagger: = = total number of predictor variables (@) \\n• Random forest: =≪ number of predictor variables.  \\no Brieman suggests three possible values for =: \\x07\\n\\x08W@,W@, 2W@ \\n \\n \\nWhen a new input is entered into the system, it is run down all of the trees. The result may either be an \\naverage or weighted average of all of the terminal nodes that are reached, or, in the case of categorical \\nvariables, a voting majority. \\n \\nNote that: \\nØ With a large number of predictors (@≫ 0), the eligible predictor set (=) will be quite different \\nfrom node to node. \\nØ The greater the inter-tree correlation, the greater the random forest error rate, so one pressure \\non the model is to have the trees as uncorrelated as possible. \\nØ As m goes down, both inter-tree correlation and the strength of individual trees go down. So some \\noptimal value of m must be discovered. \\nØ Strengths: Random forest runtimes are quite fast, and they are able to deal with unbalanced and \\nmissing data.  \\nØ Weaknesses: Random Forest used for regression cannot predict beyond the range in the training \\ndata, and that they may over-fit data sets that are particularly noisy. Of course, the best test of \\nany algorithm is how well it works upon your own data set. \\n \\nQ28. What cross-validation technique would you use on a time series data set? \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 50}),\n",
       " Document(page_content='Instead of using k-fold cross-validation, you should be aware of the fact that a time series is not randomly \\ndistributed data — It is inherently ordered by chronological order. \\nIn case of time series data, you should use techniques like forward=chaining — Where you will be model \\non past data then look at forward-facing data. \\n \\nfold 1: training[1], test[2] \\nfold 2: training[1 2], test[3] \\nfold 3: training[1 2 3], test[4] \\nfold 4: training[1 2 3 4], test[5] \\n \\nQ29. What is a Box-Cox Transformation? \\n \\nThe dependent variable for a regression analysis might not satisfy one or more assumptions of an ordinary \\nleast squares regression. The residuals could either curve as the prediction increases or follow the skewed \\ndistribution. In such scenarios, it is necessary to transform the response variable so that the data meets \\nthe required assumptions. A Box-Cox transformation is a statistical technique to transform non-normal \\ndependent variables into a normal shape. If the given data is not normal then most of the statistical \\ntechniques assume normality. Applying a Box-Cox transformation means that you can run a broader \\nnumber of tests. \\nA Box-Cox transformation is a way to transform non-normal dependent variables into a normal shape. \\nNormality is an important assumption for many statistical techniques, if your data isn’t normal, applying \\na Box-Cox means that you are able to run a broader number of tests. The Box-Cox transformation is named \\nafter statisticians George Box and Sir David Roxbee Cox who collaborated on a 1964 paper and developed \\nthe technique. \\n \\nQ30. How Regularly Must an Algorithm be Updated? \\n \\nYou will want to update an algorithm when: \\n• You want the model to evolve as data streams through infrastructure \\n• The underlying data source is changing \\n• There is a case of non-stationarity (mean, variance change over the time) \\n• The algorithm underperforms/results lack accuracy \\n \\nQ31. If you are having 4GB RAM in your machine and you want to train your model \\non 10GB data set. How would you go about this problem? Have you ever faced this \\nkind of problem in your machine learning/data science experience so far? \\n \\nFirst of all, you have to ask which ML model you want to train. \\nFor Neural networks: Batch size with Numpy array will work. Steps: \\n1. Load the whole data in the Numpy array. Numpy array has a property to create a mapping of the \\ncomplete data set, it doesn’t load complete data set in memory. \\n2. You can pass an index to Numpy array to get required data. \\n3. Use this data to pass to the Neural network. \\n4. Have a small batch size.  \\nFor SVM: Partial fit will work. Steps: \\n1. Divide one big data set in small size data sets. ', metadata={'source': 'ML questions dump.pdf', 'page': 51}),\n",
       " Document(page_content='2. Use a partial fit method of SVM, it requires a subset of the complete data set. \\n3. Repeat step 2 for other subsets. \\nHowever, you could actually face such an issue in reality. So, you could check out the best laptop for \\nMachine Learning to prevent that. Having said that, let’s move on to some questions on deep learning. \\n  ', metadata={'source': 'ML questions dump.pdf', 'page': 52}),\n",
       " Document(page_content='Deep Learning \\n \\nQ1. What do you mean by Deep Learning? \\n \\nDeep Learning is nothing but a paradigm of machine learning which has shown incredible promise in \\nrecent years. This is because of the fact that Deep Learning shows a great analogy with the functioning of \\nthe neurons in the human brain. \\n \\n \\nQ2. What is the difference between machine learning and deep learning? \\n \\nhttps://parsers.me/deep-learning-machine-learning-whats-the-difference/  \\n \\nMachine learning is a field of computer science that gives computers the ability to learn without being \\nexplicitly programmed. Machine learning can be categorized in the following four categories. \\n1. Supervised machine learning, \\n2. Semi-supervised machine learning, \\n3. Unsupervised machine learning,  \\n4. Reinforcement learning. \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 53}),\n",
       " Document(page_content='Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and \\nfunction of the brain called artificial neural networks. \\n• The main difference between deep learning and machine learning is due to the way data is \\npresented in the system. Machine learning algorithms almost always require structured data, \\nwhile deep learning networks rely on layers of ANN (artificial neural networks). \\n• Machine learning algorithms are designed to “learn” to act by understanding labeled data and \\nthen use it to produce new results with more datasets. However, when the result is incorrect, \\nthere is a need to “teach them”. Because machine learning algorithms require bulleted data, they \\nare not suitable for solving complex queries that involve a huge amount of data. \\n• Deep learning networks do not require human intervention, as multilevel layers in neural \\nnetworks place data in a hierarchy of different concepts, which ultimately learn from their own \\nmistakes. However, even they can be wrong if the data quality is not good enough. \\n• Data decides everything. It is the quality of the data that ultimately determines the quality of the \\nresult. \\n• Both of these subsets of AI are somehow connected to data, which makes it possible to represent \\na certain form of “intelligence.” However, you should be aware that deep learning requires much \\nmore data than a traditional machine learning algorithm. The reason for this is that deep learning \\nnetworks can identify different elements in neural network layers only when more than a million \\ndata points interact. Machine learning algorithms, on the other hand, are capable of learning by \\npre-programmed criteria. \\nQ3. What, in your opinion, is the reason for the popularity of Deep Learning in \\nrecent times? \\n \\nNow although Deep Learning has been around for many years, the major breakthroughs from these \\ntechniques came just in recent years. This is because of two main reasons: \\n• The increase in the amount of data generated through various sources \\n• The growth in hardware resources required to run these models \\nGPUs are multiple times faster and they help us build bigger and deeper deep learning models in \\ncomparatively less time than we required previously. \\n \\nQ4. What is reinforcement learning? \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 54}),\n",
       " Document(page_content='Reinforcement Learning allows to take actions to max cumulative reward. It learns by trial and error \\nthrough reward/penalty system. Environment rewards agent so by time agent makes better decisions.  \\nEx: robot=agent, maze=environment. Used for complex tasks (self-driving cars, game AI). \\n \\nRL is a series of time steps in a Markov Decision Process: \\n1. Environment: space in which RL operates \\n2. State: data related to past action RL took \\n3. Action: action taken \\n4. Reward: number taken by agent after last action \\n5. Observation: data related to environment: can be visible or partially shadowed \\n \\nQ5. What are Artificial Neural Networks? \\n \\nArtificial Neural networks are a specific set of algorithms that have revolutionized machine learning. They \\nare inspired by biological neural networks. Neural Networks can adapt to changing the input, so the \\nnetwork generates the best possible result without needing to redesign the output criteria. \\n \\nQ6. Describe the structure of Artificial Neural Networks? \\n \\nArtificial Neural Networks works on the same principle as a biological Neural Network. It consists of inputs \\nwhich get processed with weighted sums and Bias, with the help of Activation Functions. \\n \\n \\nQ7. How Are Weights Initialized in a Network? \\n \\nThere are two methods here: we can either initialize the weights to zero or assign them randomly. \\n \\nInitializing all weights to 0: This makes your model similar to a linear model. All the neurons and every \\nlayer perform the same operation, giving the same output and making the deep net useless. \\n \\nInitializing all weights randomly: Here, the weights are assigned randomly by initializing them very close \\nto 0. It gives better accuracy to the model since every neuron performs different computations. This is the \\nmost commonly used method. \\n', metadata={'source': 'ML questions dump.pdf', 'page': 55}),\n",
       " Document(page_content=' \\nQ8. What Is the Cost Function? \\n \\nAlso referred to as “loss” or “error,” cost function is a measure to evaluate how good your model’s \\nperformance is. It’s used to compute the error of the output layer during backpropagation. We push that \\nerror backwards through the neural network and use that during the different training functions.  \\nThe most known one is the mean sum of squared errors. \\n \\n \\n \\nH X\\x18=N( ∑(F\\x18G\\x18)+3 )  \\n \\n \\nQ9. What Are Hyperparameters? \\n \\nWith neural networks, you’re usually working with hyperparameters once the data is formatted correctly. \\nA hyperparameter is a parameter whose value is set before the learning process begins. It determines \\nhow a network is trained and the structure of the network (such as the number of hidden units, the \\nlearning rate, epochs, batches, etc.). \\n \\nQ10. What Will Happen If the Learning Rate Is Set inaccurately (Too Low or Too \\nHigh)? \\n \\nWhen your learning rate is too low, training of the model will progress very slowly as we are making \\nminimal updates to the weights. It will take many updates before reaching the minimum point. \\nIf the learning rate is set too high, this causes undesirable divergent behavior to the loss function due to \\ndrastic updates in weights. It may fail to converge (model can give a good output) or even diverge (data is \\ntoo chaotic for the network to train). \\n \\nQ11. What Is The Difference Between Epoch, Batch, and Iteration in Deep \\nLearning? \\n \\n• Epoch – Represents one iteration over the entire dataset (everything put into the training model). \\n• Batch – Refers to when we cannot pass the entire dataset into the neural network at once, so we \\ndivide the dataset into several batches. \\n• Iteration – if we have 10,000 images as data and a batch size of 200. then an epoch should run 50 \\niterations (10,000 divided by 50). \\n \\nQ12. What Are the Different Layers on CNN? \\n \\nhttps://towardsdatascience.com/basics-of-the-classic-cnn-a3dce1225add \\n', metadata={'source': 'ML questions dump.pdf', 'page': 56}),\n",
       " Document(page_content=' \\nThe Convolutional neural networks are regularized versions of multilayer perceptron (MLP). They were \\ndeveloped based on the working of the neurons of the animal visual cortex. \\n \\n \\nLet’s say we have a color image in JPG form and its size is 480 x 480. The representative array will be 480 \\nx 480 x 3. Each of these numbers is given a value from 0 to 255 which describes the pixel intensity at that \\npoint. RGB intensity values of the image are visualized by the computer for processing. \\n \\nThe objective of using the CNN: \\n \\nThe idea is that you give the computer this array of numbers and it will output numbers that describe the \\nprobability of the image being a certain class (.80 for a cat, .15 for a dog, .05 for a bird, etc.). It works \\nsimilar to how our brain works. When we look at a picture of a dog, we can classify it as such if the picture \\nhas identifiable features such as paws or 4 legs. In a similar way, the computer is able to perform image \\nclassification by looking for low-level features such as edges and curves and then building up to more \\nabstract concepts through a series of convolutional layers. The computer uses low-level features obtained \\nat the initial levels to generate high-level features such as paws or eyes to identify the object. \\n \\nThere are four layers in CNN: \\n \\n1. Convolutional Layer –  the layer that performs a convolutional operation, creating several smaller \\npicture windows to go over the data. \\n2. Activation Layer (ReLU Layer)  – it brings non-linearity to the network and converts all the negative \\npixels to zero. The output is a rectified feature map. It follows each convolutional layer. \\n3. Pooling Layer –  pooling is a down-sampling operation that reduces the dimensionality of the \\nfeature map. Stride = how much you slide, and you get the max of the > G > matrix \\n4. Fully Connected Layer –  this layer recognizes and classifies the objects in the image. \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 57}),\n",
       " Document(page_content='Convolution Operation \\n \\nFirst Layer: \\n \\n1. Input to a convolutional layer  \\nThe image is resized to an optimal size and is fed as input to the convolutional layer. \\nLet us consider the input as 32x32x3 array of pixel values. \\n \\n \\n2. There exists a filter or neuron or kernel which lays over some of the pixels of the input image \\ndepending on the dimensions of the Kernel size. \\nLet the dimensions of the kernel of the filter be 5x5x3.  \\n \\n \\n \\n3. The Kernel actually slides over the input image; thus, it is multiplying the values in the filter \\nwith the original pixel values of the image (aka computing element-wise multiplications). \\nThe multiplications are summed up generating a single number for that particular receptive field \\nand hence for sliding the kernel a total of 784 numbers are mapped to 28x28 array known as the \\nfeature map. \\n \\n**Now if we consider two kernels of the same dimension then the obtained first layer feature map \\nwill be (28x28x2). \\n \\nHigh-level Perspective \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 58}),\n",
       " Document(page_content='• Let us take a kernel of size (7x7x3) for understanding. Each of the kernels is considered to be a \\nfeature identifier, hence say that our filter will be a curve detector. \\n \\n \\n \\n• The original image and the visualization of the kernel on the image. \\n \\n \\n \\n \\nThe sum of the multiplication value that is generated is =  4∗(50∗30)+(20∗30) =  6600 (large \\nnumber). \\n \\n• Now when the kernel moves to the other part of the image. \\n \\n \\n \\nThe sum of the multiplication value that is generated is = 0 (small number). \\n \\nThe use of the small and the large value \\n \\n1. The value is much lower! This is because there wasn’t anything in the image section that responded \\nto the curve detector filter. Remember, the output of this convolution layer is an activation map. \\nSo, in the simple case of a one filter convolution (and if that filter is a curve detector), the activation \\nmap will show the areas in which there at most likely to be curved in the picture. \\n', metadata={'source': 'ML questions dump.pdf', 'page': 59}),\n",
       " Document(page_content=' \\n2. In the previous example, the top-left value of our 26 x 26 x 1 activation map (26 because of the 7x7 \\nfilter instead of 5x5) will be 6600. This high value means that it is likely that there is some sort of \\ncurve in the input volume that caused the filter to activate. The top right value in our activation \\nmap will be 0 because there wasn’t anything in the input volume that caused the filter to activate. \\nThis is just for one filter. \\n \\n3. This is just a filter that is going to detect lines that curve outward and to the right. We can have \\nother filters for lines that curve to the left or for straight edges. The more filters, the greater the \\ndepth of the activation map, and the more information we have about the input volume. \\n \\nIn the picture, we can see some examples of actual visualizations of the filters of the first conv. \\nlayer of a trained network. Nonetheless, the main argument remains the same. The filters on the \\nfirst layer convolve around the input image and “activate” (or compute high values) when the \\nspecific feature it is looking for is in the input volume. \\n \\n \\nSequential convolutional layers after the first one \\n \\n1. When we go through another conv. layer, the output of the first conv. layer becomes the input of \\nthe 2nd conv. layer. \\n \\n2. However, when we’re talking about the 2nd conv. layer, the input is the activation map(s) that \\nresult from the first layer. So, each layer of the input is basically describing the locations in the \\noriginal image for where certain low-level features appear. \\n \\n3. Now when you apply a set of filters on top of that (pass it through the 2nd conv. layer), the output \\nwill be activations that represent higher-level features. Types of these features could be \\nsemicircles (a combination of a curve and straight edge) or squares (a combination of several \\nstraight edges). As you go through the network and go through more convolutional layers, you \\nget activation maps that represent more and more complex features. \\n \\n4. By the end of the network, you may have some filters that activate when there is handwriting in \\nthe image, filters that activate when they see pink objects, etc. \\n \\nPooling Operation \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 60}),\n",
       " Document(page_content='It consists in getting the largest number out of a matrix to get the most important number and reduce \\nthe dimention. \\n \\nMax Pooling example \\n \\n \\n \\n2x2 filters with stride = 2 (maximum value) is considered \\n \\n \\n3x3 filters with stride = 1 (maximum value) is considered \\n \\n \\n \\n \\n3x3 filters with stride = 2 (maximum value) is considered \\n \\nClassification \\n \\n1. Flatten: The pooled matrix is converted to a vector . \\n', metadata={'source': 'ML questions dump.pdf', 'page': 61}),\n",
       " Document(page_content='2. Fully Connected layer: The way this fully connected layer works is that it looks at the output of \\nthe previous layer (which as we remember should represent the activation maps of high-level \\nfeatures) and the number of classes p (10 for digit classification). For example, if the program is \\npredicting that some image is a dog, it will have high values in the activation maps that represent \\nhigh-level features like a paw or 4 legs, etc. Basically, an FC layer looks at what high level features \\nmost strongly correlate to a particular class and has particular weights so that when you compute \\nthe products between the weights and the previous layer, you get the correct probabilities for the \\ndifferent classes.  \\n3. Soft-max approach: The output of a fully connected layer is as follows [0 .1 .1 .75 0 0 0 0 0 .05], \\nthen this represents a 10% probability that the image is a 1, a 10% probability that the image is a \\n2, a 75% probability that the image is a 3, and a 5% probability that the image is a 9 (SoftMax \\napproach) for digit classification.  \\n \\nTraining \\n \\n§We know kernels also known as feature identifiers, used for identification of specific features. But how \\nthe kernels are initialized with the specific weights or how do the filters know what values to have. \\n \\nHence comes the important step of training. The training process is also known as backpropagation, which \\nis further separated into 4 distinct sections or processes. \\n• Forward Pass \\n• Loss Function \\n• Backward Pass \\n• Weight Update \\n \\nThe Forward Pass \\n \\nFor the first epoch or iteration of the training the initial kernels of the first convolutional layer are \\ninitialized with random values. Thus, after the first iteration output will be something like \\n[.1.1.1.1.1.1.1.1.1.1], which does not give preference to any class as the kernels don’t have specific \\nweights. \\n \\nThe Loss Function \\n \\nThe training involves images along with labels, hence the label for the digit 3 will be [0 0 0 1 0 0 0 0 0 0], \\nwhereas the output after a first epoch is very different, hence we will calculate loss (MSE — Mean Squared \\nError) \\n \\n \\nThe objective is to minimize the loss, which is an optimization problem in calculus. It involves trying to \\nadjust the weights to reduce the loss. \\n \\nThe Backward Pass \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 62}),\n",
       " Document(page_content='It involves determining which weights contributed most to the loss and finding ways to adjust them so \\nthat the loss decreases. It is computed using  $\\r\\n$\\x11 (or ∇\"*), where L is the loss and the W is the weights of \\nthe corresponding kernel. \\n \\nThe weights update \\n \\nThis is where the weights of the kernel are updated using the following equation. \\n \\n \\nHere the Learning Rate is chosen by the programmer. Larger value of the learning rate indicates much \\nlarger steps towards optimization of steps and larger time to convolve to an optimized weight. \\n \\nTesting \\n \\nFinally, to see whether or not our CNN works, we have a different set of images and labels (can’t double \\ndip between training and test!) and pass the images through the CNN. We compare the outputs to the \\nground truth and see if our network works! \\n \\nQ13. What Is Pooling on CNN, and How Does It Work? \\n \\nPooling is used to reduce the spatial dimensions of a CNN. It performs down-sampling operations to \\nreduce the dimensionality and creates a pooled feature map by sliding a filter matrix over the input matrix. \\n \\nQ14. What are Recurrent Neural Networks (RNNs)? \\n \\nhttps://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce   \\n \\nRNNs are a type of artificial neural networks designed to recognize the pattern from the sequence of data \\nsuch as Time series, stock market and government agencies etc.  \\n \\nRecurrent Neural Networks (RNNs) add an interesting twist to basic neural networks. A vanilla neural \\nnetwork takes in a fixed size vector as input which limits its usage in situations that involve a ‘series’ type \\ninput with no predetermined size. \\n \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 63}),\n",
       " Document(page_content=\" \\n \\nRNNs are designed to take a series of input with no predetermined limit on size. One could ask what’s \\nthe big deal, I can call a regular NN repeatedly too? \\n \\n \\n \\n \\nSure can, but the ‘series’ part of the input means something. A single input item from the series is related \\nto others and likely has an influence on its neighbors. Otherwise it's just “many” inputs, not a “series” \\ninput (duh!). \\n \\nRecurrent Neural Network remembers the past and its decisions are influenced by what it has learnt from \\nthe past. Note: Basic feed forward networks “remember” things too, but they remember things they \\nlearnt during training. For example, an image classifier learns what a “1” looks like during training and \\nthen uses that knowledge to classify things in production. \\nWhile RNNs learn similarly while training, in addition, they remember things learnt from prior input(s) \\nwhile generating output(s). RNNs can take one or more input vectors and produce one or more output \\nvectors and the output(s) are influenced not just by weights applied on inputs like a regular NN, but also \\nby a “hidden” state vector representing the context based on prior input(s)/output(s). So, the same input \\ncould produce a different output depending on previous inputs in the series. \\n \\n\", metadata={'source': 'ML questions dump.pdf', 'page': 64}),\n",
       " Document(page_content=' \\n \\nIn summary, in a vanilla neural network, a fixed size input vector is transformed into a fixed size output \\nvector. Such a network becomes “recurrent” when you repeatedly apply the transformations to a series \\nof given input and produce a series of output vectors. There is no pre-set limitation to the size of the \\nvector. And, in addition to generating the output which is a function of the input and hidden state, we \\nupdate the hidden state itself based on the input and use it in processing the next input. \\n \\nParameter Sharing \\n \\nYou might have noticed another key difference between Figure 1 and Figure 3. In the earlier, multiple \\ndifferent weights are applied to the different parts of an input item generating a hidden layer neuron, \\nwhich in turn is transformed using further weights to produce an output. There seems to be a lot of \\nweights in play here. Whereas in Figure 3, we seem to be applying the same weights over and over again \\nto different items in the input series. \\n \\nI am sure you are quick to point out that we are kind of comparing apples and oranges here. The first \\nfigure deals with “a” single input whereas the second figure represents multiple inputs from a series. But \\nnevertheless, intuitively speaking, as the number of inputs increase, shouldn’t the number of weights in \\nplay increase as well? Are we losing some versatility and depth in Figure 3? \\n \\nPerhaps we are. We are sharing parameters across inputs in Figure 3. If we don’t share parameters across \\ninputs, then it becomes like a vanilla neural network where each input node requires weights of their own. \\nThis introduces the constraint that the length of the input has to be fixed and that makes it impossible to \\nleverage a series type input where the lengths differ and is not always known. \\n \\nBut what we seemingly lose in value here, we gain back by introducing the “hidden state” that links one \\ninput to the next. The hidden state captures the relationship that neighbors might have with each other \\nin a serial input and it keeps changing in every step, and thus effectively every input undergoes a different \\ntransition! \\n \\nImage classifying CNNs have become so successful because the 2D convolutions are an effective form of \\nparameter sharing where each convolutional filter basically extracts the presence or absence of a feature \\nin an image which is a function of not just one pixel but also of its surrounding neighbor pixels. \\n', metadata={'source': 'ML questions dump.pdf', 'page': 65}),\n",
       " Document(page_content=' \\nIn other words, the success of CNNs and RNNs can be attributed to the concept of “parameter sharing” \\nwhich is fundamentally an effective way of leveraging the relationship between one input item and its  \\nsurrounding neighbors in a more intrinsic fashion compared to a vanilla neural network. \\n \\nDeep RNNs \\n \\nWhile it’s good that the introduction of hidden state enabled us to effectively identify the relationship \\nbetween the inputs, is there a way we can make an RNN “deep” and gain the multi-level abstractions and \\nrepresentations we gain through “depth” in a typical neural network? \\n \\n \\n \\n \\nHere are four possible ways to add depth.  \\n \\n1) We can add hidden states, one on top of another, feeding the output of one to the next.  \\n2) We can also add additional nonlinear hidden layers between input to hidden state.  \\n3) We can increase depth in the hidden to hidden transition.  \\n4) We can increase depth in the hidden to output transition.  \\n \\nBidirectional RNNs \\n \\nSometimes it’s not just about learning from the past to predict the future, but we also need to look into \\nthe future to fix the past. In speech recognition and handwriting recognition tasks, where there could be \\nconsiderable ambiguity given just one part of the input, we often need to know what’s coming next to \\nbetter understand the context and detect the present. \\n \\n \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 66}),\n",
       " Document(page_content=' \\n \\nThis does introduce the obvious challenge of how much into the future we need to look into, because if \\nwe have to wait to see all inputs then the entire operation will become costly. And in cases like speech \\nrecognition, waiting till an entire sentence is spoken might make for a less compelling use case. Whereas \\nfor NLP tasks, where the inputs tend to be available, we can likely consider entire sentences all at once. \\nAlso, depending on the application, if the sensitivity to immediate and closer neighbors is higher than \\ninputs that come further away, a variant that looks only into a limited future/past can be modeled. \\n \\nRecursive Neural Network \\n \\nA recurrent neural network parses the inputs in a sequential fashion. A recursive neural network is similar \\nto the extent that the transitions are repeatedly applied to inputs, but not necessarily in a sequential \\nfashion. Recursive Neural Networks are a more general form of Recurrent Neural Networks. It can operate \\non any hierarchical tree structure. Parsing through input nodes, combining child nodes into parent nodes \\nand combining them with other child/parent nodes to create a tree like structure. Recurrent Neural \\nNetworks do the same, but the structure there is strictly linear. i.e. weights are applied on the first input \\nnode, then the second, third and so on.  \\n \\n \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 67}),\n",
       " Document(page_content='But this raises questions pertaining to the structure. How do we decide that? If the structure is fixed like \\nin Recurrent Neural Networks then the process of training, backprop, makes sense in that they are similar \\nto a regular neural network. But if the structure isn’t fixed, is that learnt as well? \\n \\nEncoder Decoder Sequence to Sequence RNNs \\n \\nEncoder Decoder or Sequence to Sequence RNNs are used a lot in translation services. The basic idea is \\nthat there are two RNNs, one an encoder that keeps updating its hidden state and produces a final single \\n“Context” output. This is then fed to the decoder, which translates this context to a sequence of outputs. \\nAnother key difference in this arrangement is that the length of the input sequence and the length of the \\noutput sequence need not necessarily be the same. \\n \\n \\n \\n \\nLSTMs \\n \\nLSTM is not a different variant of RNN architecture, but rather it introduces changes to how we compute \\noutputs and hidden state using the inputs. \\nIn a vanilla RNN, the input and the hidden state are simply passed through a single tanh layer. LSTM (Long-\\nShort-Term Memory) networks improve on this simple transformation and introduces additional gates \\nand a cell state, such that it fundamentally addresses the problem of keeping or resetting context, across \\nsentences and regardless of the distance between such context resets. There are variants of LSTMs \\nincluding GRUs that utilize the gates in different manners to address the problem of long-term \\ndependencies. \\n \\nQ15. How Does an LSTM Network Work? \\n \\nhttp://karpathy.github.io/2015/05/21/rnn-effectiveness   \\nhttps://colah.github.io/posts/2015-08-Understanding- LSTMs/   \\n', metadata={'source': 'ML questions dump.pdf', 'page': 68}),\n",
       " Document(page_content=' \\n \\nLong-Short-Term Memory (LSTM) is a special kind of recurrent neural network capable of learning long- \\nterm dependencies, remembering information for long periods as its default behavior. There are three \\nsteps in an LSTM network: \\n \\n• Step 1: The network decides what to forget and what to remember. \\n• Step 2: It selectively updates cell state values. \\n• Step 3: The network decides what part of the current state makes it to the output. \\n \\nRecurrent Neural Networks \\n \\nHumans don’t start their thinking from scratch every second. As you read this essay, you understand each \\nword based on your understanding of previous words. You don’t throw everything away and start thinking \\nfrom scratch again. Your thoughts have persistence. \\nTraditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine \\nyou want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional \\nneural network could use its reasoning about previous events in the film to inform later ones. \\nRecurrent neural networks address this issue. They are networks with loops in them, allowing information \\nto persist. \\n \\n \\n \\nRecurrent Neural Networks have loops. \\n \\nIn the above diagram, a chunk of neural network, A, looks at some input G\\x1f and outputs a value ℎ\\x1f. A loop \\nallows information to be passed from one step of the network to the next. \\nThese loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, \\nit turns out that they aren’t all that different than a normal neural network. A recurrent neural network \\ncan be thought of as multiple copies of the same network, each passing a message to a successor. Consider \\nwhat happens if we unroll the loop: \\n \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 69}),\n",
       " Document(page_content=' \\nAn unrolled recurrent neural network. \\n \\nThis chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. \\nThey’re the natural architecture of neural network to use for such data. \\nAnd they certainly are used! In the last few years, there have been incredible success applying RNNs to a \\nvariety of problems: speech recognition, language modeling, translation, image captioning…  \\n \\nEssential to these successes is the use of “LSTMs,” a very special kind of recurrent neural network which \\nworks, for many tasks, much better than the standard version. Almost all exciting results based on \\nrecurrent neural networks are achieved with them.  \\n \\nThe Problem of Long-Term Dependencies \\n \\nOne of the appeals of RNNs is the idea that they might be able to connect previous information to the \\npresent task, such as using previous video frames might inform the understanding of the present frame. \\nIf RNNs could do this, they’d be extremely useful. But can they? It depends. \\nSometimes, we only need to look at recent information to perform the present task. For example, consider \\na language model trying to predict the next word based on the previous ones. If we are trying to predict \\nthe last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next \\nword is going to be sky. In such cases, where the gap between the relevant information and the place that \\nit’s needed is small, RNNs can learn to use the past information. \\n \\n \\n \\nBut there are also cases where we need more context. Consider trying to predict the last word in the text \\n“I grew up in France… I speak fluent French.” Recent information suggests that the next word is probably \\nthe name of a language, but if we want to narrow down which language, we need the context of France, \\nfrom further back. It’s entirely possible for the gap between the relevant information and the point where \\nit is needed to become very large. \\n \\nUnfortunately, as that gap grows, RNNs become unable to learn to connect the information. \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 70}),\n",
       " Document(page_content=' \\n \\nIn theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could \\ncarefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem \\nto be able to learn them. Thankfully, LSTMs don’t have this problem! \\n \\nLSTM Networks \\n \\nLong Short-Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of \\nlearning long-term dependencies. They work tremendously well on a large variety of problems and are \\nnow widely used. \\nLSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for \\nlong periods of time is practically their default behavior, not something they struggle to learn! \\nAll recurrent neural networks have the form of a chain of repeating modules of neural network. In \\nstandard RNNs, this repeating module will have a very simple structure, such as a single tanh layer. \\n \\n \\n \\nThe repeating module in a standard RNN contains a single layer. \\n \\nLSTMs also have this chain like structure, but the repeating module has a different structure. Instead of \\nhaving a single neural network layer, there are four, interacting in a very special way. \\n', metadata={'source': 'ML questions dump.pdf', 'page': 71}),\n",
       " Document(page_content=' \\n \\nThe repeating module in an LSTM contains four interacting layers. \\n \\n \\n \\nIn the above diagram, each line carries an entire vector, from the output of one node to the inputs of \\nothers. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are \\nlearned neural network layers. Lines merging denote concatenation, while a line forking denotes its \\ncontent being copied and the copies going to different locations.  \\n \\nThe Core Idea Behind LSTMs \\n \\nThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram. \\nThe cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor \\nlinear interactions. It’s very easy for information to just flow along it unchanged. \\n \\n \\n \\nThe LSTM does have the ability to remove or add information to the cell state, carefully regulated by \\nstructures called gates. \\nGates are a way to optionally let information through. They are composed out of a sigmoid neural net \\nlayer and a pointwise multiplication operation. \\n', metadata={'source': 'ML questions dump.pdf', 'page': 72}),\n",
       " Document(page_content=' \\n \\nThe sigmoid layer outputs numbers between zero and one, describing how much of each component \\nshould be let through. A value of zero means “let nothing through,” while a value of one means “let \\neverything through!” \\nAn LSTM has three of these gates, to protect and control the cell state. \\n \\nQ16. What Is a Multi-layer Perceptron (MLP)? \\n \\nhttps://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53  \\n \\nAs in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer. It has the same \\nstructure as a single layer perceptron with one or more hidden layers.  \\n \\nPerceptron is a single layer neural network  and a multi-layer perceptron is called Neural Networks. \\nA (single layer) perceptron is a single layer neural network that works as a linear binary classifier. Being a \\nsingle layer neural network, it can be trained without the use of more advanced algorithms like back \\npropagation and instead can be trained by \"stepping towards\" your error in steps specified by a learning \\nrate. When someone says perceptron, I usually think of the single layer version. \\n \\n \\n \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 73}),\n",
       " Document(page_content=' \\n \\nA single layer perceptron can classify only linear separable classes with binary output {0,1} or {-1,1}, but \\nMLP can classify nonlinear classes. The activation functions are used to map the input between the \\nrequired values like {0, 1} or {-1, 1} . \\n \\n \\nExcept for the input layer, each node in the other layers uses a nonlinear activation function. This means \\nthe input layers, the data coming in, and the activation function is based upon all nodes and weights being \\nadded together, producing the output. MLP uses a supervised learning method called “backpropagation.” \\nIn backpropagation, the neural network calculates the error with the help of cost function. It propagates  \\nthis error backward from where it came (adjusts the weights to train the model more accurately). \\n \\nUsually, RELU is in hidden layers (it does not classify), and Soft-max or tanh is in output layers.  \\n \\nQ17. Explain Gradient Descent. \\n \\nLet’s first explain what a gradient is. A gradient is a mathematical function. When calculated on a point of \\na function, it gives the hyperplane (or slope) of the directions in which the function increases more. The \\ngradient vector can be interpreted as the \"direction and rate of fastest increase\". If the gradient of a \\n', metadata={'source': 'ML questions dump.pdf', 'page': 74}),\n",
       " Document(page_content='function is non-zero at a point p, the direction of the gradient is the direction in which the function \\nincreases most quickly from p, and the magnitude  of the gradient is the rate of increase in that direction. \\nFurther, the gradient is the zero vector at a point if and only if it is a stationary point  (where the derivative \\nvanishes).  \\nIn DS, it simply measures the change in all weights with regard to the change in error, as we are partially \\nderivating by w the loss function.  \\n \\nGradient descent  is a first-order iterative optimization algorithm for finding the minimum of a function. \\n \\nThe goal of the gradient descent is to minimize a given function which, in our case, is the loss function of \\nthe neural network. To achieve this goal, it performs two steps iteratively. \\n \\n1. Compute the slope (gradient) that is the first-order derivative of the function at the current point \\n2. Move-in the opposite direction of the slope increase from the current point by the computed \\namount \\n \\nSo, the idea is to pass the training set through the hidden layers of the neural network and then update \\nthe parameters of the layers by computing the gradients using the training samples from the training \\ndataset. \\n \\nThink of it like this. Suppose a man is at top of the valley and he wants to get to the bottom of the valley. \\nSo, he goes down the slope. He decides his next position based on his current position and stops when he \\ngets to the bottom of the valley which was his goal. \\n \\nQ18. What is exploding gradients? \\n \\nhttps://machinelearningmastery.com/exploding-gradients-in-neural-networks/   \\n \\nWhile training an RNN, if you see exponentially growing (very large) error gradients which accumulate and \\nresult in very large updates to neural network model weights during training, they’re known as exploding \\ngradients. At an extreme, the values of weights can become so large as to overflow and result in NaN \\nvalues. The explosion occurs through exponential growth by repeatedly multiplying gradients through the \\nnetwork layers that have values larger than 1.0. \\n \\nThis has the effect of your model is unstable and unable to learn from your training data. \\n', metadata={'source': 'ML questions dump.pdf', 'page': 75}),\n",
       " Document(page_content=' \\nThere are some subtle signs that you may be suffering from exploding gradients during the training of \\nyour network, such as: \\n \\n• The model is unable to get traction on your training data (e.g. poor loss). \\n• The model is unstable, resulting in large changes in loss from update to update. \\n• The model loss goes to NaN during training. \\n• The model weights quickly become very large during training. \\n• The error gradient values are consistently above 1.0 for each node and layer during training. \\n \\nSolutions \\n \\n1. Re-Design the Network Model:  \\na. In deep neural networks, exploding gradients may be addressed by redesigning the \\nnetwork to have fewer layers.  There may also be some benefit in using a smaller batch \\nsize while training the network.  \\nb. In RNNs, updating across fewer prior time steps during training, called truncated \\nBackpropagation through time , may reduce the exploding gradient problem.  \\n \\n2. Use Long Short-Term Memory Networks: In RNNs, exploding gradients can be reduced by using \\nthe Long Short-Term Memory (LSTM)  memory units and perhaps related gated-type neuron \\nstructures. Adopting LSTM memory units is a new best practice for recurrent neural networks for \\nsequence prediction.  \\n \\n3. Use Gradient Clipping: Exploding gradients can still occur in very deep Multilayer Perceptron \\nnetworks with a large batch size and LSTMs with very long input sequence lengths. If exploding \\ngradients are still occurring, you can check for and limit the size of gradients during the training \\nof your network. This is called gradient clipping. Specifically, the values of the error gradient are \\nchecked against a threshold value and clipped or set to that threshold value if the error gradient \\nexceeds the threshold.  \\n \\n4. Use Weight Regularization:  another approach, if exploding gradients are still occurring, is to check \\nthe size of network weights and apply a penalty to the networks loss function  for large weight \\nvalues. This is called weight regularization and often an L1 (absolute weights) or an L2 (squared \\nweights) penalty can be used.  \\n \\nQ19. What is vanishing gradients? \\n \\nWhile training an RNN, your slope can become either too small; this makes the training difficult. When \\nthe slope is too small, the problem is known as a Vanishing Gradient. It leads to long training times, poor \\nperformance, and low accuracy.  \\n \\n• Hyperbolic tangent and Sigmoid/Soft-max suffer vanishing gradient. \\n• RNNs suffer vanishing gradient, LSTM no (so it is perfect to predict stock prices). In fact, the \\npropagation of error through previous layers makes the gradient get smaller so the weights are \\nnot updated. \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 76}),\n",
       " Document(page_content='Solutions \\n \\n1. Choose RELU \\n2. Use LSTM (for RNNs) \\n3. Use ResNet (Residual Network)  à after some layers, add x again: &(G)→ ⋯ →&(G)+G \\n4. Multi-level hierarchy:  pre-train one layer at the time through unsupervised learning, then fine-\\ntune via backpropagation \\n5. Gradient checking:  debugging strategy used to numerically track and assess gradients during \\ntraining. \\n \\nQ20. What is Back Propagation and Explain it Works. \\n \\nBackpropagation is a training algorithm used for neural network. In this method, we update the weights \\nof each layer from the last layer recursively, with the formula: \\n \\nF\\x1c\\x1d\\x16!\\x18\\x1b \\x1e  \\x19\\x12#\\x16\\x1d=F\\x19\\x12#\\x16\\x1d−J∇\"*(F) \\n \\nIt has the following steps: \\n \\n• Forward Propagation of Training Data (initializing weights with random or pre-assigned values) \\n• Gradients are computed using output weights and target \\n• Back Propagate for computing gradients of error from output activation \\n• Update the Weights \\n \\nQ21. What are the variants of Back Propagation? \\n \\nhttps://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a \\n \\n• Stochastic Gradient Descent:  In Batch Gradient Descent we were considering all the examples for \\nevery step of Gradient Descent. But what if our dataset is very huge. Deep learning models crave for \\ndata. The more the data the more chances of a model to be good. Suppose our dataset has 5 million \\nexamples, then just to take one step the model will have to calculate the gradients of all the 5 million \\nexamples. This does not seem an efficient way.  To tackle this problem, we have Stochastic Gradient \\nDescent. In Stochastic Gradient Descent (SGD), we consider just one example at a time to take a single \\nstep. We do the following steps in one epoch  for SGD: \\n1. Take an example \\n2. Feed it to Neural Network \\n3. Calculate its gradient \\n4. Use the gradient we calculated in step 3 to update the weights \\n5. Repeat steps 1–4 for all the examples in training dataset \\n \\nSince we are considering just one example at a time the cost will fluctuate over the training examples \\nand it will not necessarily decrease. But in the long run, you will see the cost decreasing with \\nfluctuations. Also, because the cost is so fluctuating, it will never reach the minimum, but it will keep \\ndancing around it. SGD can be used for larger datasets. It converges faster when the dataset is large \\nas it causes updates to the parameters more frequently. ', metadata={'source': 'ML questions dump.pdf', 'page': 77}),\n",
       " Document(page_content=' \\n \\n• Batch Gradient Descent:  all the training data is taken into consideration to take a single step. We take \\nthe average of the gradients of all the training examples and then use that mean gradient to update \\nour parameters. So that’s just one step of gradient descent in one epoch. Batch Gradient Descent is \\ngreat for convex or relatively smooth error manifolds. In this case, we move somewhat directly \\ntowards an optimum solution.  The graph of cost vs epochs is also quite smooth because we are \\naveraging over all the gradients of training data for a single step. The cost keeps on decreasing over \\nthe epochs. \\n \\n \\n \\n• Mini-batch Gradient Descent:  It’s one of the most popular optimization algorithms. It’s a variant of \\nStochastic Gradient Descent and here instead of single training example, mini batch of samples is \\nused. Batch Gradient Descent can be used for smoother curves. SGD can be used when the dataset is \\nlarge. Batch Gradient Descent converges directly to minima. SGD converges faster for larger datasets. \\nBut, since in SGD we use only one example at a time, we cannot implement the vectorized \\nimplementation on it. This can slow down the computations. To tackle this problem, a mixture of \\nBatch Gradient Descent and SGD is used. Neither we use all the dataset all at once nor we use the \\nsingle example at a time. We use a batch of a fixed number of training examples which is less than the \\nactual dataset and call it a mini-batch. Doing this helps us achieve the advantages of both the former \\nvariants we saw. So, after creating the mini-batches of fixed size, we do the following steps in one \\nepoch: \\n1. Pick a mini-batch \\n2. Feed it to Neural Network \\n3. Calculate the mean gradient of the mini-batch \\n4. Use the mean gradient we calculated in step 3 to update the weights \\n5. Repeat steps 1–4 for the mini-batches we created \\n', metadata={'source': 'ML questions dump.pdf', 'page': 78}),\n",
       " Document(page_content=\"Just like SGD, the average cost over the epochs in mini-batch gradient descent fluctuates because we are \\naveraging a small number of examples at a time. So, when we are using the mini-batch gradient descent \\nwe are updating our parameters frequently as well as we can use vectorized implementation for faster \\ncomputations. \\n \\nQ22. What are the different Deep Learning Frameworks? \\n \\n• PyTorch: PyTorch is an open source machine learning library based on the Torch library, used for \\napplications such as computer vision and natural language processing, primarily developed by \\nFacebook's AI Research lab. It is free and open-source software released under the Modified BSD \\nlicense. \\n• TensorFlow: TensorFlow is a free and open-source software library for dataflow and differentiable \\nprogramming across a range of tasks. It is a symbolic math library and is also used for machine learning \\napplications such as neural networks. Licensed by Apache License 2.0. Developed by Google Brain \\nTeam. \\n• Microsoft Cognitive Toolkit: Microsoft Cognitive Toolkit describes neural networks as a series of \\ncomputational steps via a directed graph. \\n• Keras: Keras is an open-source neural-network library written in Python. It is capable of running on \\ntop of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML. Designed to enable fast \\nexperimentation with deep neural networks, it focuses on being user-friendly, modular, and \\nextensible. Licensed by MIT. \\n \\nQ23. What is the role of the Activation Function? \\n \\nThe Activation function is used to introduce non-linearity into the neural network helping it to learn more \\ncomplex function. Without which the neural network would be only able to learn linear function which is \\na linear combination of its input data. An activation function is a function in an artificial neuron that \\ndelivers an output based on inputs. \\n \\nQ24. Name a few Machine Learning libraries for various purposes. \\n \\nPurpose Libraries \\nScientific Computation Numpy \\nTabular Data Pandas, GeoPandas \\nData Modelling & Preprocessing  Scikit Learn \\nTime-Series Analysis Statsmodels \\nText processing NTLK, Regular Expressions \\nDeep Learning TensorFlow, Pytorch \\nVisualization Bokeh, Seaborn \\nPlotting Matplot \\n \\nQ25. What is an Auto-Encoder? \\n \\nhttps://www.quora.com/What-is-an-autoencoder-What-are-its-applications   \\n \", metadata={'source': 'ML questions dump.pdf', 'page': 79}),\n",
       " Document(page_content='Auto-encoders are simple learning networks that aim to transform inputs into outputs with the minimum \\npossible error. This means that we want the output to be as close to input as possible. We add a couple \\nof layers between the input and the output, and the sizes of these layers are smaller than the input layer. \\nThe auto-encoder receives unlabeled input which is then encoded to reconstruct the input.  \\n \\nAn autoencoder  is a type of artificial neural network used to learn efficient data coding in an unsupervised \\nmanner. The aim of an autoencoder  is to learn a representation (encoding) for a set of data, typically for \\ndimensionality reduction, by training the network to ignore signal “noise”.  Along with the reduction side, \\na reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a \\nrepresentation as close as possible to its original input, hence its name. Several variants exist to the basic \\nmodel, with the aim of forcing the learned representations of the input to assume useful properties. \\nAutoencoders are effectively used for solving many applied problems, from face recognition to acquiring \\nthe semantic meaning of words.  \\n \\n \\n \\n \\n \\nQ26. What is a Boltzmann Machine? \\n \\nBoltzmann machines have a simple learning algorithm that allows them to discover interesting features \\nthat represent complex regularities in the training data. The Boltzmann machine is basically used to \\noptimize the weights and the quantity for the given problem. The learning algorithm is very slow in \\nnetworks with many layers of feature detectors. “Restricted Boltzmann Machines” algorithm has a single \\nlayer of feature detectors which makes it faster than the rest.  \\n \\n \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 80}),\n",
       " Document(page_content='Q27. What Is Dropout and Batch Normalization? \\n \\nDropout is a technique of dropping out hidden and visible nodes of a network randomly to prevent \\noverfitting of data (typically dropping 20 per cent of the nodes). It doubles the number of iterations \\nneeded to converge the network. It used to avoid overfitting, as it increases the capacity of generalization. \\n \\nBatch normalization is the technique to improve the performance and stability of neural networks by \\nnormalizing the inputs in every layer so that they have mean output activation of zero and standard \\ndeviation of one. \\n \\nQ28. Why Is TensorFlow the Most Preferred Library in Deep Learning? \\n \\nTensorFlow provides both C++ and Python APIs, making it easier to work on and has a faster compilation \\ntime compared to other Deep Learning libraries like Keras and PyTorch. TensorFlow supports both CPU \\nand GPU computing devices. \\n \\nQ29. What Do You Mean by Tensor in TensorFlow? \\n \\nA tensor is a mathematical object represented as arrays of higher dimensions. Think of a n-D matrix. These \\narrays of data with different dimensions and ranks fed as input to the neural network are called “Tensors.” \\n \\nQ30. What is the Computational Graph? \\n \\nEverything in a TensorFlow is based on creating a computational graph. It has a network of nodes where \\neach node operates. Nodes represent mathematical operations, and edges represent tensors. Since data \\nflows in the form of a graph, it is also called a “DataFlow Graph.” \\n \\nQ31. How is logistic regression done? \\n \\nLogistic regression measures the relationship between the dependent variable (our label of what we want \\nto predict) and one or more independent variables (our features) by estimating probability using its \\nunderlying logistic function (sigmoid). \\n  ', metadata={'source': 'ML questions dump.pdf', 'page': 81}),\n",
       " Document(page_content=\"Miscellaneous \\n \\nQ1. Explain the steps in making a decision tree. \\n \\n1. Take the entire data set as input \\n2. Calculate entropy of the target variable, as well as the predictor attributes \\n3. Calculate your information gain of all attributes (we gain information on sorting different objects from \\neach other) \\n4. Choose the attribute with the highest information gain as the root node \\n5. Repeat the same procedure on every branch until the decision node of each branch is finalized \\n \\nFor example, let's say you want to build a decision tree to decide whether you should accept or decline a \\njob offer. The decision tree for this case is as shown: \\n \\n \\n  \\nIt is clear from the decision tree that an offer is accepted if: \\n \\n• Salary is greater than $50,000 \\n• The commute is less than an hour \\n• Coffee is offered \\n \\nQ2. How do you build a random forest model? \\n \\nA random forest is built up of a number of decision trees. If you split the data into different packages and \\nmake a decision tree in each of the different groups of data, the random forest brings all those trees \\ntogether. \\n \\n\", metadata={'source': 'ML questions dump.pdf', 'page': 82}),\n",
       " Document(page_content='Steps to build a random forest model: \\n \\n1. Randomly select ; features from a total of = features where ; <<  = \\n2. Among the ; features, calculate the node D using the best split point \\n3. Split the node into daughter nodes using the best split \\n4. Repeat steps two and three until leaf nodes are finalized \\n5. Build forest by repeating steps one to four for > times to create > number of trees \\n \\nQ3. Differentiate between univariate, bivariate, and multivariate analysis. \\n \\nUnivariate \\n \\nUnivariate data contains only one variable. The purpose of the univariate analysis is to describe the data \\nand find patterns that exist within it. \\n \\nExample: height of students \\nHeight (in cm) \\n164 \\n167.3 \\n170 \\n174.2 \\n178 \\n180 \\n                     \\nThe patterns can be studied by drawing conclusions using mean, median, mode, dispersion or range, \\nminimum, maximum, etc. \\n \\nBivariate \\n \\nBivariate data involves two different variables. The analysis of this type of data deals with causes and \\nrelationships and the analysis is done to determine the relationship between the two variables. \\n \\nExample: temperature and ice cream sales in the summer season \\nTemperature (in Celsius) Sales (in K $) \\n20 2.0 \\n25 2.1 \\n26 2.3 \\n28 2.7 \\n30 3.1 \\n \\nHere, the relationship is visible from the table that temperature and sales are directly proportional to each \\nother. The hotter the temperature, the better the sales.  \\n \\nMultivariate \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 83}),\n",
       " Document(page_content='Multivariate data involves three or more variables, it is categorized under multivariate. It is similar to a \\nbivariate but contains more than one dependent variable. \\n \\nExample: data for house price prediction \\n \\nThe patterns can be studied by drawing conclusions using mean, median, and mode, dispersion or range, \\nminimum, maximum, etc. You can start describing the data and using it to guess what the price of the \\nhouse will be. \\n \\nQ4. What are the feature selection methods used to select the right variables? \\n \\nThere are two main methods for feature selection. \\n \\nFilter Methods \\n \\nThis involves: \\n• Linear discrimination analysis  \\n• ANOVA \\n• Chi-Square \\n \\nThe best analogy for selecting features is \"bad data in, bad answer out.\" When we\\'re limiting or selecting \\nthe features, it\\'s all about cleaning up the data coming in.  \\n \\nWrapper Methods \\n \\nThis involves: \\n• Forward Selection: We test one feature at a time and keep adding them until we get a good \\nfit \\n• Backward Selection: We test all the features and start removing them to see what works \\nbetter \\n• Recursive Feature Elimination: Recursively looks through all the different features and how \\nthey pair together \\n \\nWrapper methods are very labor-intensive, and high-end computers are needed if a lot of data analysis is \\nperformed with the wrapper method. \\n \\nQ5. In your choice of language, write a program that prints the numbers ranging \\nfrom one to 50. But for multiples of three, print \"Fizz\" instead of the number and \\nfor the multiples of five, print \"Buzz.\" For numbers which are multiples of both three \\nand five, print \"FizzBuzz.\" \\n \\nThe code is shown below: \\n \\n \\n \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 84}),\n",
       " Document(page_content=\"for x in range(51): \\n \\n if x % 3 == 0 and x % 5 == 0: \\n  print(‘fizzbuzz’) \\n \\n elif x % 3 == 0: \\n  print(‘fizz’) \\n  \\n elif x % 5 == 0: \\n  print(‘buzz’) \\n \\n else: \\nprint(‘fizzbuzz’) \\n \\nQ6. You are given a data set consisting of variables with more than 30 percent \\nmissing values. How will you deal with them? \\n \\nIf the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; \\nwe use the rest of the data to predict the values. \\n \\nFor smaller data sets, we can impute missing values with the mean, median, or average of the rest of the \\ndata using pandas data frame in python. There are different ways to do so, such as: \\n \\n df.mean(), df.fillna(mean) \\n \\nOther option of imputation is using KNN for numeric or classification values (as KNN just uses k closest \\nvalues to impute the missing value). \\n \\nQ7. For the given points, how will you calculate the Euclidean distance in \\nPython? \\n \\nplot1 = [1,3] \\nplot2 = [2,5] \\n \\nThe Euclidean distance can be calculated as follows:  \\n \\neuclidean_distance = sqrt((plot1[0]-plot2[0])**2 + (plot1[1]-\\nplot2[1])**2) \\n \\nQ8. What are dimensionality reduction and its benefits? \\n \\nDimensionality reduction refers to the process of converting a data set with vast dimensions into data \\nwith fewer dimensions (fields) to convey similar information concisely. \\n \\nThis reduction helps in compressing data and reducing storage space. It also reduces computation time as \\nfewer dimensions lead to less computing. It removes redundant features; for example, there's no point in \\nstoring a value in two different units (meters and inches). \\n \", metadata={'source': 'ML questions dump.pdf', 'page': 85}),\n",
       " Document(page_content=\"Q9. How will you calculate eigenvalues and eigenvectors of the following 3x3 \\nmatrix? \\n \\n Determinant of !−K1 and solve to find K. \\n \\nQ10. How should you maintain a deployed model? \\n \\nThe steps to maintain a deployed model are (CREM):  \\n \\n1. Monitor: constant monitoring of all models is needed to determine their performance accuracy. \\nWhen you change something, you want to figure out how your changes are going to affect things. \\nThis needs to be monitored to ensure it's doing what it's supposed to do. \\n2. Evaluate:  evaluation metrics of the current model are calculated to determine if a new algorithm \\nis needed.  \\n3. Compare:  the new models are compared to each other to determine which model performs the \\nbest.  \\n4. Rebuild: the best performing model is re-built on the current state of data. \\n \\nQ11. How can a time-series data be declared as stationery? \\n \\n \\n\", metadata={'source': 'ML questions dump.pdf', 'page': 86}),\n",
       " Document(page_content=\"Q12. 'People who bought this also bought...' recommendations seen on Amazon \\nare a result of which algorithm? \\n \\nThe recommendation engine is accomplished with collaborative filtering. Collaborative filtering explains \\nthe behavior of other users and their purchase history in terms of ratings, selection, etc. \\nThe engine makes predictions on what might interest a person based on the preferences of other users.  \\n \\nIn this algorithm, item features are unknown. \\n \\nFor example, a sales page shows that a certain number of people buy a new phone and also buy tempered \\nglass at the same time. Next time, when a person buys a phone, he or she may see a recommendation to \\nbuy tempered glass as well. \\n \\nQ13. What is a Generative Adversarial Network? \\n \\nSuppose there is a wine shop purchasing wine from dealers, which they resell later. But some dealers sell \\nfake wine. In this case, the shop owner should be able to distinguish between fake and authentic wine. The \\nforger will try different techniques to sell fake wine and make sure specific techniques go past the shop \\nowner’s check. The shop owner would probably get some feedback from wine experts that some of the \\nwine is not original. The owner would have to improve how he determines whether a wine is fake or \\nauthentic.  \\nThe forger’s goal is to create wines that are indistinguishable from the authentic ones while the shop owner \\nintends to tell if the wine is real or not accurately. \\n \\n \\n \\n• There is a noise vector coming into the forger who is generating fake wine. \\n• Here the forger acts as a Generator. \\n• The shop owner acts as a Discriminator. \\n• The Discriminator gets two inputs; one is the fake wine, while the other is the real authentic wine. \\nThe shop owner has to figure out whether it is real or fake. \\n \\nSo, there are two primary components of Generative Adversarial Network (GAN) named: \\n \\n1. Generator \\n2. Discriminator \\n\", metadata={'source': 'ML questions dump.pdf', 'page': 87}),\n",
       " Document(page_content=\"The generator is a CNN that keeps keys producing images and is closer in appearance to the real images \\nwhile the discriminator tries to determine the difference between real and fake images. The ultimate aim \\nis to make the discriminator learn to identify real and fake images. \\n \\nQ14. You are given a dataset on cancer detection. You have built a classification \\nmodel and achieved an accuracy of 96 percent. Why shouldn't you be happy with \\nyour model performance? What can you do about it? \\n \\nCancer detection results in imbalanced data. In an imbalanced dataset, accuracy should not be based as a \\nmeasure of performance. It is important to focus on the remaining four percent, which represents the \\npatients who were wrongly diagnosed. Early diagnosis is crucial when it comes to cancer detection and \\ncan greatly improve a patient's prognosis. \\n \\nHence, to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True \\nNegative Rate), F measure to determine the class wise performance of the classifier. \\n \\nQ15. Below are the eight actual values of the target variable in the train file. What \\nis the entropy of the target variable? [0, 0, 0, 1, 1, 1, 1, 1]  \\n \\nThe target variable, in this case, is 1 (the last) \\n \\nThe formula for calculating the entropy is, putting @= 5 and >= 8, we get: \\n \\n%>CA?@H = −U5\\n8 <?8U5\\n8V+3\\n8 logU3\\n8VV \\n \\nQ16. We want to predict the probability of death from heart disease based on \\nthree risk factors: age, gender, and blood cholesterol level. What is the most \\nappropriate algorithm for this case? Choose the correct option: \\n \\nThe most appropriate algorithm for this case is logistic regression. \\n \\nQ17. After studying the behavior of a population, you have identified four specific \\nindividual types that are valuable to your study. You would like to find all users who \\nare most similar to each individual type. Which algorithm is most appropriate for \\nthis study? \\n \\nAs we are looking for grouping people together specifically by four different similarities, it indicates the \\nvalue of k. Therefore, K-means clustering is the most appropriate algorithm for this study. \\n \\nQ18. You have run the association rules algorithm on your dataset, and the two \\nrules {banana, apple} => {grape} and {apple, orange} => {grape} have been found to \\nbe relevant. What else must be true? Choose the right answer: \\n \", metadata={'source': 'ML questions dump.pdf', 'page': 88}),\n",
       " Document(page_content='The answer is A: {grape, apple} must be a frequent itemset. \\n \\nQ19. Your organization has a website where visitors randomly receive one of two \\ncoupons. It is also possible that visitors to the website will not receive a coupon. You \\nhave been asked to determine if offering a coupon to website visitors has any \\nimpact on their purchase decisions. Which analysis method should you use? \\n \\nOne-way ANOVA: in statistics, one-way analysis of variance is a technique that can be used to compare \\nmeans of two or more samples. This technique can be used only for numerical response data, the \"Y\", \\nusually one variable, and numerical or categorical input data, the \"X\", always one variable, hence \"one-\\nway\".  \\nThe ANOVA tests the null hypothesis, which states that samples in all groups are drawn from populations \\nwith the same mean values. To do this, two estimates are made of the population variance. The ANOVA \\nproduces an F-statistic, the ratio of the variance calculated among the means to the variance within the \\nsamples. If the group means are drawn from populations with the same mean values, the variance \\nbetween the group means should be lower than the variance of the samples, following the central limit \\ntheorem. A higher ratio therefore implies that the samples were drawn from populations with different \\nmean values. \\n \\nQ20. What are the feature vectors? \\n \\nA feature vector is an n-dimensional vector of numerical features that represent an object. In machine \\nlearning, feature vectors are used to represent numeric or symbolic characteristics (called features) of an \\nobject in a mathematical way that\\'s easy to analyze. \\n \\nQ21. What is root cause analysis? \\n \\nRoot cause analysis was initially developed to analyze industrial accidents but is now widely used in other \\nareas. It is a problem-solving technique used for isolating the root causes of faults or problems. A factor \\nis called a root cause if its deduction from the problem-fault-sequence averts the final undesirable event \\nfrom recurring. \\n \\nQ22. Do gradient descent methods always converge to similar points? \\n \\nThey do not, because in some cases, they reach a local minimum or a local optimum point. You would not \\nreach the global optimum point. This is governed by the data and the starting conditions. \\n \\nQ23. What are the most popular Cloud Services used in Data Science? \\n \\nhttps://www.zdnet.com/article/the-top-cloud-providers-of-2020-aws-microsoft-azure-google-cloud-\\nhybrid-saas/  \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 89}),\n",
       " Document(page_content=' \\n \\n \\nQ24. What is a Canary Deployment? \\n \\nhttps://www.split.io/glossary/canary-deployment/  \\n \\nA canary deployment, or canary release, allows you to rollout your features to only a subset of users as \\nan initial test to make sure nothing else in your system broke. \\n \\nThe initial steps for implementing canary deployment are: \\n1. create two clones of the production environment,  \\n2. have a load balancer that initially sends all traffic to one version,  \\n3. create new functionality in the other version.  \\n \\nWhen you deploy the new software version, you shift some percentage – say, 10% – of your user base to \\nthe new version while maintaining 90% of users on the old version. If that 10% reports no errors, you can \\nroll it out to gradually more users, until the new version is being used by everyone. If the 10% has \\nproblems, though, you can roll it right back, and 90% of your users will have never even seen the problem. \\n \\nCanary deployment benefits include zero downtime, easy rollout and quick rollback – plus the added \\nsafety from the gradual rollout process. It also has some drawbacks – the expense of maintaining multiple \\nserver instances, the difficult clone-or-don’t-clone database decision. \\n \\n', metadata={'source': 'ML questions dump.pdf', 'page': 90}),\n",
       " Document(page_content='Typically, software development teams implement blue/green deployment when they’re sure the new \\nversion will work properly and want a simple, fast strategy to deploy it. Conversely, canary deployment is \\nmost useful when the development team isn’t as sure about the new version and they don’t mind a slower \\nrollout if it means they’ll be able to catch the bugs. \\n \\nQ25. What is a Blue Green Deployment? \\n \\nhttps://docs.cloudfoundry.org/devguide/deploy-apps/blue-green.html   \\n \\nBlue-green deployment is a technique that reduces downtime and risk by running two identical \\nproduction environments called Blue and Green. \\n \\nAt any time, only one of the environments is live, with the live environment serving all production traffic. \\nFor this example, Blue is currently live, and Green is idle. \\n \\nAs you prepare a new version of your model, deployment and the final stage of testing takes place in the \\nenvironment that is not live: in this example, Green. Once you have deployed and fully tested the model \\nin Green, you switch the router, so all incoming requests now go to Green instead of Blue. Green is now \\nlive, and Blue is idle. \\n \\nThis technique can eliminate downtime due to app deployment and reduces risk: if something unexpected \\nhappens with your new version on Green, you can immediately roll back to the last version by switching \\nback to Blue. \\n \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 91}),\n",
       " Document(page_content=' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThis document is based on the original document by Steve Nouri ( LinkedIn). \\n \\nReviewed and corrected by Davide Callegaro ( LinkedIn). \\n \\nOriginal credits to kdnuggets, Simplilearn, Edureka, Guru99, Hackernoon, \\nDatacamp, Nitin Panwar, Michael Rundell.  \\n \\nBelow some questions the reader shall view the link of the original article. \\n \\n  \\n', metadata={'source': 'ML questions dump.pdf', 'page': 92}),\n",
       " Document(page_content='. \\nWhat is DevOps? Introduction to DevOps | Roles | Payscale  \\nBy this DevOps Interview Questions and answers, many students are got placed in many reputed \\ncompanies with high package salary. So utilize our DevOps Interview Questions and answers to grow in \\nyour career.  \\nQ1) what is DevOps?  \\nBy the name DevOps, it’s very clear that it’s a collaboration of Development as well as Operations. But one \\nshould know that DevOps is not a tool, or software or framework, DevOps is a Combination of Tools \\nwhich helps for t he automation of the whole infrastructure.  \\nDevOps is basically an implementation of Agile methodology on the Development side as well as \\nOperations side.  \\nQ2) why do we need DevOps?  \\nTo fulfil the need of delivering more and faster and better application to meet more and more demands \\nof users, we need DevOps. DevOps helps deployment to happen really fast compared to any other \\ntraditional tools.  \\nQ3) Mention the key aspects or principle behind DevOps?  \\nThe key aspects or principle behind DevOps is:  \\n• Infrastructur e as a Code  \\n• Continuous Integration  \\n• Continuous Deployment  \\n• Automation  \\n• Continuous Monitoring  \\n• Security  \\nQ4) List out some of the popular tools for DevOps?  \\n• Git \\n• Jenkins  ', metadata={'source': 'ML questions dump.pdf', 'page': 93}),\n",
       " Document(page_content='• Ansible  \\n• Puppet  \\n• Nagios  \\n• Docker  \\n• ELK (Elasticsearch, Logstash, Kibana)  \\nQ5) what is a version control system?  \\nVersion Control System (VCS) is a software that helps software developers to work together and maintain \\na complete history of their work.  \\nSome of the feature of VCS as follows:  \\n• Allow developers to wok simultaneously  \\n• Does not allow o verwriting on each other changes.  \\n• Maintain the history of every version.  \\nThere are two types of Version Control Systems:  \\n1. Central Version Control System, Ex: Git, Bitbucket  \\n2. Distributed/Decentralized Version Control System, Ex: SVN  \\nQ6) What is Git and explain the difference between Git and SVN?  \\nGit is a source code management (SCM) tool which handles small as well as large projects with efficiency. \\nIt is basically used to store our repositories in remote server such as GitHub.  \\n      GIT SVN \\nGit is a De centralized \\nVersion Control Tool  SVN is a Centralized \\nVersion Control Tool  \\nGit contains the local \\nrepo as well as the full \\nhistory of the whole \\nproject on all the SVN relies only on the \\ncentral server to store ', metadata={'source': 'ML questions dump.pdf', 'page': 94}),\n",
       " Document(page_content='developers hard drive, \\nso if there is a server \\noutage , you can easily \\ndo recovery from your \\nteam mates local git \\nrepo.  all the versions of the \\nproject file  \\nPush and pull \\noperations are fast  Push and pull \\noperations are slower \\ncompared to Git  \\nIt belongs to \\n3rd generation Version \\nControl Tool  It belongs to \\n2nd generation Version \\nControl tools  \\nClient nodes can share \\nthe entire repositories \\non their local system  Version history is \\nstored on server -side \\nrepository  \\nCommits can be done \\noffline too  Commits can be done \\nonly online  \\nWork are shared \\nautomatically by \\ncommit  Nothing is shared \\nautomatically  \\nQ7) what language is used in Git?  \\nGit is written in C language, and since its written in C language its very fast and reduces the overhead of \\nruntimes.  ', metadata={'source': 'ML questions dump.pdf', 'page': 95}),\n",
       " Document(page_content='Q8) what is SubGit?  \\nSubGit  is a tool for migrating SVN to Git. It creates a writable Git mirror of a local or remote Subversion \\nrepository and uses both Subversion and Git if you like.  \\nQ9) how can you clone a Git repository via Jenkins?  \\nFirst, we must enter the e -mail and user name  for your Jenkins system, then switch into your job directory \\nand execute the “git config” command.  \\nQ10)What are the Advantages of Ansible?  \\n• Agentless, it doesn’t require any extra package/daemons to be installed  \\n• Very low overhead  \\n• Good performance  \\n• Idempoten t \\n• Very Easy to learn  \\n• Declarative not procedural  \\nQ11) what’s the use of Ansible?  \\nAnsible is mainly used in IT infrastructure to manage or deploy applications to remote nodes. Let’s say we \\nwant to deploy one application in 100’s of nodes by just executing on e command, then Ansible is the one \\nactually coming into the picture but should have some knowledge on Ansible script to understand or \\nexecute the same.  \\nQ12) what’s the difference between Ansible Playbook and Roles?  \\n        Roles     Playbooks  \\nRoles are reusable \\nsubsets of a play.  Playbooks contain Plays.  \\nA set of tasks for \\naccomplishing certain \\nrole.  Mapps among hosts and \\nroles.  ', metadata={'source': 'ML questions dump.pdf', 'page': 96}),\n",
       " Document(page_content='Example: common, \\nwebservers.  Example: site.yml, \\nfooservers.yml, \\nwebservers.yml.  \\n Q13) How do I see a list of all the ansible_ variables?  \\nAnsible by default gathers “facts” about the machines, and these facts can be accessed in Playbooks and \\nin templates. To see a list of all the facts that are available about a machine, you can run th e “setup” \\nmodule as an ad -hoc action:  \\nAnsible -m setup hostname  \\nThis will print out a dictionary of all the facts that are available for that particular host.  \\nQ14) what is Docker?  \\nDocker is a containerization technology that packages your application and a ll its dependencies together \\nin the form of Containers to ensure that your application works seamlessly in any environment.  \\nQ15) what is Docker image?  \\nDocker image is the source of Docker container. Or in other words, Docker images are used to create \\nconta iners.  \\nQ16) what is Docker Container?  \\nDocker Container is the running instance of Docker Image.  \\nQ17) Can we consider DevOps as Agile methodology?  \\nOf Course, we can!! The only difference between agile methodology and DevOps is that, agile \\nmethodology is imp lemented only for development section and DevOps implements agility on both \\ndevelopment as well as operations section.  \\n Q18) what are the advantages of using Git?  \\n1. Data redundancy and replication  \\n2. High availability  \\n3. Only one. git directory per repository  \\n4. Supe rior disk utilization and network performance  \\n5. Collaboration friendly  ', metadata={'source': 'ML questions dump.pdf', 'page': 97}),\n",
       " Document(page_content='6. Git can use any sort of projects.  \\nQ19) what is kernel?  \\nA kernel is the lowest level of easily replaceable software that interfaces with the hardware in your \\ncomputer.  \\nQ20) what is differe nce between grep -i and grep -v? \\nI ignore alphabet difference V accept this value  \\nex) ls | grep -i docker  \\nDockerfile  \\ndocker.tar.gz  \\nls | grep -v docker  \\nDesktop  \\nDockerfile  \\nDocuments  \\nDownloads  \\nYou can’t see anything with name docker.tar.gz  \\nQ21) How can you define particular space to the file  \\nThis feature is generally used to give the swap space to the server. Lets say in below machine I have to \\ncreate swap space of 1GB then,  \\ndd if=/dev/zero of=/swapfile1 bs=1G count=1  \\n Q22) what is concept of sudo in linux?  \\nSudo(superuser do) is a utility for UNIX - and Linux -based systems that provides an     efficient way to give \\nspecific users permission to use specific system commands at the root (most powerful) level of the \\nsystem.  \\n Q23) what is a Jenkins Pipeline?  \\nJenkins Pipeline (or simply “Pipeline”) is a suite of plugins which supports implementing and integrating \\ncontinuous delivery pipelines into Jenkins.  ', metadata={'source': 'ML questions dump.pdf', 'page': 98}),\n",
       " Document(page_content=' Q24) How to stop and restart the Docker container?  \\nTo stop the container: docker stop container ID  \\nNow to  restart the Docker container: docker restart container ID  \\nQ25) What platforms does Docker run on?  \\nDocker runs on only Linux and Cloud platforms:  \\n• Ubuntu 12.04 LTS+  \\n• Fedora 20+  \\n• RHEL 6.5+  \\n• CentOS 6+  \\n• Gentoo  \\n• ArchLinux  \\n• openSUSE 12.3+  \\n• CRUX 3.0+  \\nCloud:  \\n• Amazon EC2 \\n• Google Compute Engine  \\n• Microsoft Azure  \\n• Rackspace  \\nNote that Docker does not run on Windows or Mac for production as there is no support, yes you can use \\nit for testing purpose even in windows  \\n Q26) what are the tools used for docker networking?  \\nFor docke r networking we generally use kubernets and docker swarm.  \\n Q27) what is docker compose?  \\nLets say you want to run multiple docker container, at that time you have to create the docker compose \\nfile and type the command docker -compose up. It will run all the containers mentioned in docker \\ncompose file.  ', metadata={'source': 'ML questions dump.pdf', 'page': 99}),\n",
       " Document(page_content='Q28) What is Scrum?  \\nScrum is basically used to divide your complex software and product development task into smaller \\nchunks, using iterations and incremental practises. Each iteration is of two weeks. Scrum con sists of three \\nroles: Product owner, scrum master and Team  \\nQ29) What does the commit object contain?  \\nCommit object contain the following components:  \\nIt contains a set of files, representing the state of a project at a given point of time reference to paren t \\ncommit objects  \\nAn SHAI name, a 40 -character string that uniquely identifies the commit object (also called as hash).  \\nQ30) Explain the difference between git pull and git fetch?  \\nGit pull command basically pulls any new changes or commits from a branch fro m your central repository \\nand updates your target branch in your local repository.  \\nGit fetch is also used for the same purpose, but its slightly different form Git pull. When you trigger a git \\nfetch, it pulls all new commits from the desired branch and sto res it in a new branch in your local \\nrepository. If we want to reflect these changes in your target branch, git fetch must be followed with a git \\nmerge. Our target branch will only be updated after merging the target branch and fetched branch. Just \\nto make  it easy for us, remember the equation below:  \\nGit pull = git fetch + git merge  \\nQ31)  How do we know in Git if a branch has already been merged into master?  \\ngit branch –merged  \\nThe above command lists the branches that have been merged into the current branch . \\ngit branch –no-merged  \\nthis command lists the branches that have not been merged.  \\nQ32) What is ‘Staging Area’ or ‘Index’ in GIT?  \\nBefore committing a file, it must be formatted and reviewed in an intermediate area known as ‘Staging \\nArea’ or ‘Indexing Area’.  \\n#git add <file_name>  ', metadata={'source': 'ML questions dump.pdf', 'page': 100}),\n",
       " Document(page_content='Q33)  What is Git Stash?  \\nLet’s say you’ve been working on part of your project, things are in a messy state and you want to switch \\nbranches for some time to work on something else. The problem is, you don’t want to do a  commit of \\nyour half -done work just, so you can get back to this point later. The answer to this issue is Git stash.  \\nGit Stashing takes your working directory that is, your modified tracked files and staged changes and \\nsaves it on a stack of unfinished cha nges that you can reapply at any time.  \\nQ34) What is Git stash drop?  \\nGit ‘stash drop’ command is basically used to remove the stashed item. It will basically remove the last \\nadded stash item by default, and it can also remove a specific item if you include it as an argument.  \\nI have provided an example below:  \\nIf you want to remove any particular stash item from the list of stashed items you can use the below \\ncommands:  \\ngit stash list: It will display the list of stashed items as follows:  \\nstash@{0}: WIP on mast er: 049d080 added the index file  \\nstash@{1}: WIP on master: c265351 Revert “added files”  \\nstash@{2}: WIP on master: 13d80a5 added number to log  \\nQ35) What is the function of ‘git config’?  \\nGit uses our username to associate commits with an identity. The git co nfig command can be used to \\nchange our Git configuration, including your username.  \\nSuppose you want to give a username and email id to associate commit with an identity so that you can \\nknow who has made a commit. For that I will use:  \\ngit config –global use r.name “Your Name”: This command will add your username.  \\ngit config –global user.email “Your E -mail Address”: This command will add your email id.  \\nQ36) How can you create a repository in Git?  \\nTo create a repository, you must create a directory for the proj ect if it does not exist, then run command \\n“git init”. By running this command .git directory will be created inside the project directory.  \\nQ37) Describe the branching strategies you have used?  \\nGenerally, they ask this question to understand your branching  knowledge  \\nFeature branching  ', metadata={'source': 'ML questions dump.pdf', 'page': 101}),\n",
       " Document(page_content='This model keeps all the changes for a feature inside of a branch. When the feature branch is fully tested \\nand validated by automated tests, the branch is then merged into master.  \\nTask branching  \\nIn this task branching model eac h task is implemented on its own branch with the task key included in the \\nbranch name. It is quite easy to see which code implements which task, just look for the task key in the \\nbranch name.  \\nRelease branching  \\nOnce the develop branch has acquired enough fe atures for a release, then we can clone that branch to \\nform a Release branch. Creating this release branch starts the next release cycle, so no new features can \\nbe added after this point, only bug fixes, documentation generation, and other release -oriented  tasks \\nshould go in this branch. Once it’s ready to ship, the release gets merged into master and then tagged \\nwith a version number. In addition, it should be merged back into develop branch, which may have \\nprogressed since the release was initiated earlie r. \\nQ38) What is Jenkins?  \\nJenkins is an open source continuous integration tool which is written in Java language. It keeps a track \\non version control system and to initiate and monitor a build system if any changes occur. It monitors the \\nwhole process and provides reports and notifications to alert the concern team.  \\nQ39) What is the difference between Maven, Ant and Jenkins?  \\nMaven and Ant are Build Technologies whereas Jenkins is a continuous integration(CI/CD) tool.  \\nQ40) Explain what is continuous integrat ion? \\nWhen multiple developers or teams are working on different segments of same web application, we need \\nto perform integration test by integrating all the modules. To do that an automated process for each \\npiece of code is performed on daily bases so that  all your code gets tested. And this whole process is \\ntermed as continuous integration.  \\nQ41) What is the relation between Hudson and Jenkins?  \\nHudson was the earlier name of current Jenkins. After some issue faced, the project name was changed \\nfrom Hudson t o Jenkins.  \\nQ42) What are the advantages of Jenkins?  \\nAdvantage of using Jenkins  ', metadata={'source': 'ML questions dump.pdf', 'page': 102}),\n",
       " Document(page_content='• Bug tracking is easy at early stage in development environment.  \\n• Provides a very large numbers of plugin support.  \\n• Iterative improvement to the code, code is basically divided into small sprints.  \\n• Build failures are cached at integration stage.  \\n• For each code commit changes an automatic build report notification get generated.  \\n• To notify developers about build report success or failure, it can be integrated with LDAP mail \\nserver.  \\n• Achieves continuous integration agile development and test -driven development environment.  \\n• With simple steps, maven release project can also be automated.  \\nQ43) Which SCM tools does Jenkins supports?  \\nSource code management tools supported by Jenkins are below:  \\n• AccuRev  \\n• CVS \\n• Subversion  \\n• Git \\n• Mercurial  \\n• Perforce  \\n• Clearcase  \\n• RTC \\nQ44) What is Ansible?  \\nAnsible is a software configuration management tool to deploy an application using ssh  without      any \\ndowntime. It is also used for management and configuration of software applications.   Ansible is \\ndeveloped in Python language.  \\nQ45) How can your setup Jenkins jobs?  \\nSteps to set up Jenkins job as follows:  \\nSelect new item from the menu.  \\nAfter that enter a name for the job (it can be anything) and select free -style job.  ', metadata={'source': 'ML questions dump.pdf', 'page': 103}),\n",
       " Document(page_content='Then click OK to create new job in Jenkins dashboard.  \\nThe next page enables you to configure your job, and it’s done.  \\nQ46) What is your daily activities in your curre nt role?  \\n• Working on JIRA Tickets  \\n• Builds and Deployments  \\n• Resolving issues when builds and deployments fails by coordinating and collaborating with the \\ndev team  \\n• Infrastructure maintenance  \\n• Monitoring health of applications  \\nQ47) What are the challenges you fac ed in recent times?  \\nI need to implement trending technologies like Docker to automate the configuration management \\nactivities in my project by showing POC.  \\nQ48) What are the build and deployment failures you got and how you resolved those?  \\nI use to get mos t of the time out of memory issue. So I fixed this issue by restarting the server which is \\nnot best practice. I did the permanent fix by increase the Perm Gen Space and Heap Space.  \\nQ49) I want a file that consists of last 10 lines of the some other file?  \\nTail -10 filename >filename  \\nQ50) How to check the exit status of the commands?  \\necho $?  \\nBecome  an DevOps  Expert  with Certification  in 25hours  \\nQ51) I want to get the information from file which consist s of the word “GangBoard”  \\ngrep “GangBoard” filename  \\nQ52) I want to search the files with the name of “GangBoard”  \\nfind / -type f -name “*GangBoard*”  \\nQ53) Write a shell script to print only prime numbers?  \\nprime.sh  \\necho \"1\"  ', metadata={'source': 'ML questions dump.pdf', 'page': 104}),\n",
       " Document(page_content='i=3 \\nj=300 \\nflag=0 \\ntem=2 \\necho \"1\"whil e [ $i -ne $j ] \\ndo \\n        temp=`echo $i`  \\n        while [ $temp -ne $tem ]  \\n        do \\n                temp=`expr $temp - 1` \\n                n=`expr $i % $temp`  \\n                if [ $n -eq 0 -a $flag -eq 0 ] \\n                then \\n                        flag=1 \\n                fi \\n        done \\n        if [ $flag -eq 0 ] \\n        then \\n                echo $i \\n        else \\n                flag=0 \\n        fi \\n        i=`expr $i + 1`  \\ndone \\nQ54) How to pass the parameters to the script and how  can I get those parameters?  \\nScriptname.sh parameter1 parameter2  \\nI will use $* to get the parameters.  \\nQ55) What is the default file permissions for the file and how can I modify it?  \\nDefault file permissions are : rw -r—r— \\nIf  I want to change the default file permissions I need to use umask command ex: umask 666  ', metadata={'source': 'ML questions dump.pdf', 'page': 105}),\n",
       " Document(page_content='Q56) How you will do the releases?  \\nThere are some steps to follow.  \\n• Create a check list  \\n• Create a release branch  \\n• Bump the version  \\n• Merge release branch to master & tag  it. \\n• Use a Pull request to merge the release merge  \\n• Deploy master to Prod Environment  \\n• Merge back into  develop  & delete  release  branch  \\n• Change log generation  \\n• Communicating with stack holders  \\n• Grooming the issue tracker  \\nQ57) How you automate the whole build and release process?  \\n• Check out a set of source code files.  \\n• Compile the code and report on progress along the way.  \\n• Run automated unit tests against successful compiles.  \\n• Create an installer.  \\n• Publish the installer to a download site, and notify teams that the installer is available.  \\n• Run the installer to create an installed executable.  \\n• Run automated tests against the executable.  \\n• Report the results of the tests.  \\n• Launch a subordinate project to update standard libraries.  \\n• Promote executables and othe r files to QA for further testing.  \\n• Deploy finished releases to production environments, such as Web servers or CD \\nmanufacturing.  \\nThe above process will be done by Jenkins by creating the jobs.  ', metadata={'source': 'ML questions dump.pdf', 'page': 106}),\n",
       " Document(page_content='Q58) I have 50 jobs in the Jenkins dash board , I want to build  at a time all the jobs  \\nIn Jenkins there is a plugin called build after other projects build. We can provide job names over there \\nand If one parent job run then it will automatically run the all other jobs. Or we can use Pipe line jobs.  \\nQ59) How can I inte grate all the tools with Jenkins?  \\nI have to navigate to the manage Jenkins and then global tool configurations there you have to provide \\nall the details such as Git URL , Java version, Maven version , Path etc.  \\nQ60) How to install Jenkins via Docker?  \\nThe s teps are:  \\n• Open up a terminal window.  \\n• Download the  jenkinsci/blueocean  image & run it as a container in Docker using the \\nfollowing  docker  run command:( \\nhttps://docs.docker.com/engine/reference/commandline/run/)  \\n• docker run \\\\  -u root \\\\  –rm \\\\  -d \\\\  -p 8080:8080 \\\\  -p 50000:50000 \\\\  -v jenkins -\\ndata:/var/jenkins_home \\\\  -v /var/run/docker.sock:/var/run/docker.sock \\\\  jenkinsci/blueocean  \\n• Proce ed to the  Post-installation setup wizard (https://jenkins.io/doc/book/installing/#setup -\\nwizard)  \\n• Accessing the Jenkins/Blue Ocean Docker container  docker exec -it jenkins -blueocean bash  \\n• Accessing the Jenkins console log through Docker logsdocker logs <docke r-container -\\nname>Accessing the Jenkins home directorydocker exec -it <docker -container -name> bash  \\nQ61) Did you ever participated in Prod Deployments? If yes what is the procedure?  \\nYes I have participated, we need to follow the following steps in my point o f view  \\n• Preparation & Planning : What kind of system/technology was supposed to run on what kind of \\nmachine  \\n• The specifications regarding the clustering of systems  \\n• How all these stand -alone boxes were going to talk to each other in a foolproof manner  \\n• Product ion setup should be documented to bits. It needs to be neat, foolproof, and \\nunderstandable.  ', metadata={'source': 'ML questions dump.pdf', 'page': 107}),\n",
       " Document(page_content='• It should have all a system configurations, IP addresses, system specifications, & installation \\ninstructions.  \\n• It needs to be updated as & when any change is made to  the production environment of the \\nsystem  \\nQ62) My application is not coming up for some reason? How can you bring it up?  \\nWe need to follow the steps  \\n• Network connection  \\n• The Web Server is not receiving users’s request  \\n• Checking the logs  \\n• Checking the process id’s whether services are running or not  \\n• The Application Server is not receiving user’s request(Check the Application Server Logs and \\nProcesses)  \\n• A network level ‘connection reset’ is happening somewhere.  \\nQ63) Did you automate anything in your proje ct? Please explain  \\nYes I have automated couple of things such as  \\n•  Password expiry automation  \\n•  Deleting the older log files  \\n•  Code quality threshold violations etc.  \\nQ64) What is IaC? How you will achieve this?  \\nInfrastructure as Code (IaC) is the management o f infrastructure (networks, virtual machines, load \\nbalancers, and connection topology) in a descriptive model, using the same versioning as DevOps team \\nuses for source code. This  will be achieved by using the tools such as Chef, Puppet and Ansible etc.  \\nQ65) What is multifactor authentication? What is the use of it?  \\nMultifactor authentication (MFA) is a security system that requires more than one method of \\nauthentication from independent categories of credentials to verify the user’s identity for a login or  other \\ntransaction.  \\n•  Security for every enterprise user — end & privileged users, internal and external  ', metadata={'source': 'ML questions dump.pdf', 'page': 108}),\n",
       " Document(page_content='•  Protect across enterprise resources — cloud & on -prem  apps, VPNs, endpoints, servers, \\nprivilege elevation and more  \\n•  Reduce cost & complexity with an integrated identity platform  \\nQ66) I want to copy the artifacts from one location to another location in cloud. How?  \\nCreate two S3 buckets, one to use as the sou rce, and the other to use as the destination and then create \\npolicies.  \\nQ67) How can I modify the commit message in git?  \\nI have to use following command and enter the required message.  \\nGit commit –amend  \\nQ68) How can you avoid the waiting time for the trigge red jobs in Jenkins.  \\nFirst I will check the Slave nodes capacity, If it is fully loaded then I will add the slave node by doing the \\nfollowing process.  \\nGo to the Jenkins dashboard -> Manage Jenkins ->Manage Nodes  \\nCreate the new node a  \\nBy giving the all requ ired fields and launch the slave machine as you want.  \\nQ69) What are the Pros and Cons of Ansible?  \\nPros:  \\n1. Open Source  \\n2. Agent less  \\n3. Improved efficiency , reduce cost  \\n4. Less Maintenance  \\n5. Easy to understand yaml files  \\nCons:  \\n1. Underdeveloped GUI with limited features  \\n2. Increased focus on orchestration over configuration management  \\n3. SSH communication slows down in scaled environments  ', metadata={'source': 'ML questions dump.pdf', 'page': 109}),\n",
       " Document(page_content='Q70) How you handle the merge conflicts in git?  \\nFollow the steps  \\n1. Create Pull request  \\n2. Modify according to the requirement by sitting with developers  \\n3. Commit the correct file to the branch  \\n4. Merge the current branch with master branch.  \\nQ71) I want to delete 10 days older log files. How can I?  \\nThere is a command in unix to achieve this task find <directory_path> -mtime +10 -name “*.log” -exec rm \\n-f {} \\\\; 2>/dev/null  \\nWhat is the difference among chef, puppet and ansible?  \\n Chef  Puppet  Ansible  \\nInteroperability  Works \\nOnly on \\nLinux/Unix  Works Only \\non \\nLinux/Unix  Supports \\nWindows \\nbut server \\nshould be \\nLinux/Unix  \\nConf. Language  It uses \\nRuby  Puppet DSL  YAML \\n(Python)  \\nAvailability  Primary \\nServer \\nand \\nBackup \\nServer  Multi \\nMaster \\nArchitecture  Single \\nActive \\nNode  ', metadata={'source': 'ML questions dump.pdf', 'page': 110}),\n",
       " Document(page_content='Q72) How you get the Inventory variables defined for the host?  \\nWe need to use the following command  \\nAnsible – m debug - a “var= hostvars[‘hostname’]” localhost(10.92.62.215)  \\nQ73) How you will take backup for Jenkins?  \\nCopy   JENKINS_HOME directory and “jobs” directory to replicate it in another server  \\nQ74) How to deploy docker container to aws?  \\nAmazon provides the service called Amaz on Elastic Container Service; By using this creating and \\nconfiguring the task definition and services we will launch the applications.  \\nQ75) I want to change the default port number of apache tomcat. How?  \\nGo to the tomcat folder and navigate to the conf fol der there you will find a server.xml file. You can \\nchange connector port tag as you want.  \\nQ76) In how many ways you can install the Jenkins?  \\nWe can install Jenkins in 3 Ways  \\n• By downloading Jenkins archive file  \\n• By running as a service Java –jar Jenkins.war  \\n• By deploying Jenkins.war to the webapps folder in tomcat.  \\nQ77) How you will run Jenkins job from command line?  \\nWe have a Jenkins CLI from there we need to use the curl command  \\ncurl -X POST -u YOUR_USER:YOUR_USE R_PASSWORD http://YOUR_JENKINS_URL/job/YOUR_JOB/build  \\nQ78) How you will do tagging in git?  \\nWe have following command to create tags in git  \\nGit tag v0.1  \\nQ79) How can you connect a container to a network when it starts?  \\nWe need to use a following command  \\ndocker run -itd –network=multi -host-network busybox  \\nQ80) How you will do code commit and code deploy in cloud?  \\n• Create a deployment environment  ', metadata={'source': 'ML questions dump.pdf', 'page': 111}),\n",
       " Document(page_content='• Get a copy of the sample code  \\n• Create your pipeline  \\n• Activate your pipeline  \\n• Commit a change and update the App.  \\nQ81) H ow to access variable names in Ansible?  \\nUsing hostvars method we can access and add the variables like below  \\n{{ hostvars[inventory_hostname][‘ansible_’ + which_interface][‘ipv4’][‘address’] }}  \\nQ82) What is Infrastructure as Code?  \\nWhere the Configuration of any servers or tool chain or application stack required for an association can \\nbe made into progressively elucidating dimension of code and that can be utilized for provisioning and \\noverseeing foundation components like Virtual Machine, So ftware, Network Elements, however it varies \\nfrom contents utilizing any language, where they are a progression of static advances coded, where \\nVersion control can be utilized so as to follow condition changes .  \\nPrecedent Tools are Ansible, Terraform.  \\nQ83) What are the zones the Version control can acquaint with get proficient DevOps practice?  \\nA clearly fundamental region of Version Control is Source code the executives, Where each engineer code \\nought to be pushed to a typical storehouse for keeping up assem ble and discharge in CI/CD pipelines.  \\nAnother territory can be Version control For Administrators when they use Infrastructure as A Code (IAC) \\napparatuses and rehearses for keeping up The Environment setup.  \\nAnother Area of Version Control framework Can be Artifactory Management Using Repositories like \\nNexus and DockerHub  \\nQ84) Why Opensource apparatuses support DevOps?  \\nOpensource devices dominatingly utilized by any association which is adjusting (or) embraced DevOps \\npipelines in light of the fact that devop s accompanied an attention on robotization in different parts of \\nassociation manufacture and discharge and change the executives and furthermore framework the board \\nzones.  \\nSo creating or utilizing a solitary apparatus is unthinkable and furthermore everyth ing is fundamentally an \\nexperimentation period of advancement and furthermore coordinated chops down the advantage of ', metadata={'source': 'ML questions dump.pdf', 'page': 112}),\n",
       " Document(page_content='building up a solitary device , so opensource devices were accessible available practically spares each \\nreason and furthermore gives assoc iation a choice to assess the device dependent on their need.  \\nQ85) What is the distinction among Ansible and chef(or) manikin?  \\nAnsible is Agentless design the board device, where manikin or gourmet expert needs operator should be \\nkept running on the specia list hub and culinary specialist or manikin depends on draw demonstrate, \\nwhere your cookbook or show for gourmet expert and manikin separately from the ace will be pulled by \\nthe operator and ansible uses ssh to convey and it gives information driven guidel ines to the hubs should \\nbe overseen , progressively like RPC execution, ansible utilizations YAML scripting, though manikin (or) \\nculinary specialist is worked by ruby uses their own DSL .  \\nQ86) What is Jinja2 templating in ansible playbooks and their utiliz ation?  \\nJinja2 templating is the Python standard for templating , consider it like a sed editorial manager for \\nAnsible , where it very well may be utilized is when there is a requirement for dynamic change of any \\nconfig record to any application like consid er mapping a MySQL application to the IP address of the \\nmachine, where it is running, it can’t be static , it needs modifying it progressively at runtime.  \\nArrangement  \\nThe vars inside the   supports are supplanted by ansible while running utilizing layout mo dule.  \\nQ87) What is the requirement for sorting out playbooks as the job, is it vital?  \\nArranging playbooks as jobs , gives greater clarity and reusability to any plays , while consider an errand \\nwhere MySQL establishment ought to be done after the evacuatio n of Oracle DB , and another \\nprerequisite is expected to introduce MySQL after java establishment, in the two cases we have to \\nintroduce MySQL , yet without jobs need to compose playbooks independently for both use cases , yet \\nutilizing jobs once the MySQL  establishment job is made can be used any number of times by \\nsummoning utilizing rationale in site.yaml .  \\nNo, it isn’t important to make jobs for each situation, however making jobs is the best practice in Ansible.  \\nQ88) What is the fundamental disservice of Docker holders?  \\nAs the lifetime of any compartments is while pursuing a holder is wrecked you can’t recover any \\ninformation inside a compartment, the information inside a compartment is lost perpetually, however \\ntenacious capacity for information inside  compartments should be possible utilizing volumes mount to an \\nouter source like host machine and any NFS drivers.  ', metadata={'source': 'ML questions dump.pdf', 'page': 113}),\n",
       " Document(page_content='Q89) What are the docker motor and docker form?  \\nDocker motor contacts the docker daemon inside the machine and makes the runtime condition an d \\nprocedure for any compartment, docker make connects a few holders to shape as a stack utilized in \\nmaking application stacks like LAMP, WAMP, XAMP  \\nQ90) What are the Different modes does a holder can be run?  \\nDocker holder can be kept running in two modes  \\nConnected: Where it will be kept running in the forefront of the framework you are running, gives a \\nterminal inside to compartment when – t choice is utilized with it, where each log will be diverted to \\nstdout screen.  \\nIsolates: This mode is typically kept r unning underway, where the holder is confined as a foundation \\nprocedure and each yield inside a compartment will be diverted log records \\ninside/var/lib/docker/logs/<container -id>/<container -id.json> and which can be seen by docker logs \\norder.  \\nQ91) What the  yield of docker assess order will be?  \\nDocker examines <container -id> will give yield in JSON position, which contains subtleties like the IP \\naddress of the compartment inside the docker virtual scaffold and volume mount data and each other \\ndata identified  with host (or) holder explicit like the basic document driver utilized, log driver utilized.  \\ndocker investigate [OPTIONS] NAME|ID [NAME|ID…] Choices  \\n• Name, shorthand Default Description  \\n• group, – f Format the yield utilizing the given Go layout  \\n• measure, – s Display all out document sizes if the sort is the compartment  \\n• type Return JSON for a predefined type  \\nQ92) What is the order can be utilized to check the asset usage by docker holders?  \\nDocker details order can be utilized to check the asset usa ge of any docker holder, it gives the yield \\npractically equivalent to Top direction in Linux, it shapes the base for compartment asset observing \\ninstruments like a counsel, which gets yield from docker details order.  \\ndocker details [OPTIONS] [CONTAINER…] C hoices  \\nName, shorthand Default Description  \\n• all, – a Show all holders (default demonstrates simply running)  ', metadata={'source': 'ML questions dump.pdf', 'page': 114}),\n",
       " Document(page_content=\"• group Pretty -print pictures utilizing a Go layout  \\n• no-stream Disable spilling details and just draw the main outcome  \\n• no-trunc Do not truncate yield  \\nQ93) How to execute some errand (or) play on localhost just while executing playbooks on \\nvarious has on an ansible?  \\nIn ansible, there is a module called delegate_to, in this module area give the specific host (or) has where \\nyour errands (or) assignment shoul d be run.  \\nundertakings:  \\nname: ” Elasticsearch Hitting”  \\nuri: url=’_search?q=status:new’ headers='{“Content -type”:”application/json”}’ method=GET \\nreturn_content=yes  \\nregister: yield  \\ndelegate_to: 127.0.0.1  \\nQ94) What is the distinction among set_fact and vars i n ansible?  \\nWhere a set_fact sets the incentive for a factor at one time and stays static, despite the fact that the \\nesteem is very powerful and vars continue changing according to the esteem continues changing for the \\nvariable.  \\nassignments:  \\nset_fact:  \\nfact_time: “Truth: ”  \\ntroubleshoot: var=fact_time  \\norder: rest 2  \\ntroubleshoot: var=fact_time  \\nassignments:  \\nname: queries in factors versus queries in realities  \\nhas: localhost  \\nvars: \\nvar_time: “Var: ”  \\nDespite the fact that the query for the date has been utilized in  both the cases, wherein the vars are \", metadata={'source': 'ML questions dump.pdf', 'page': 115}),\n",
       " Document(page_content=\"utilized it modifies dependent on an opportunity to time each time executed inside the playbook lifetime. \\nBe that as it may, Fact dependably continues as before once query is finished  \\nQ95) What is a query in ansible and what are query modules bolstered by ansible?  \\nQuery modules enable access to information in Ansible from outside sources. These modules are assessed \\non the Ansible control machine and can incorporate perusing the filesyst em yet in addition reaching \\noutside information stores and administrations.  \\nOrganization is {lookup{‘<plugin>’,'<source(or)connection_string>’}}  \\nA portion of the query modules upheld by ansible are  \\nDocument  \\npipe \\nredis  \\njinja layouts  \\netcd kv store  \\nQ96) How m ight you erase the docker pictures put away at your nearby machine and how \\nmight you do it for every one of the pictures without a moment’s delay?  \\nThe direction docker RMI <image -id> can be utilized to erase the docker picture from nearby machine, \\nthough a  few pictures may should be constrained in light of the fact that the picture might be utilized by \\nsome other holder (or) another picture , to erase pictures you can utilize the mix of directions by docker \\nRMI $(docker pictures – q), where docker pictures will give the docker picture names, to get just the ID of \\ndocker pictures just , we are utilizing – q switch with docker pictures order.  \\nQ97) What are the organizers in the Jenkins establishment and their employments?  \\nJENKINS_HOME – which will be/$JENKINS_USER/.jenkins it is the root envelope of any Jenkins \\nestablishment and it contains subfolders each for various purposes.  \\nemployments/ – Folder contains all the data pretty much every one of the occupations arranged in the \\nJenkins example.  \\nInsid e employments/, you will have the envelope made for each activity and inside those organizers, you \\nwill have fabricate organizers as indicated by each form numbers each form will have its log records, \\nwhich we see in Jenkins web support.  \\nModules/ – where a ll your modules will be recorded.  \", metadata={'source': 'ML questions dump.pdf', 'page': 116}),\n",
       " Document(page_content='Workspace/ – this will be available to hold all the workspace documents like your source code pulled from \\nSCM.  \\nQ98) What are the approaches to design Jenkins framework?  \\nJenkins can be designed in two different ways  \\nWeb: Wh ere there is a choice called design a framework, in their area, you can make all setup changes.  \\nManual on filesystem: Where each change should likewise be possible straightforwardly on the Jenkins \\nconfig.xml document under the Jenkins establishment catalog , after you make changes on the filesystem, \\nyou have to restart your Jenkins, either can do it specifically from terminal (or) you can utilize Reload \\nsetup from plate under oversee Jenkins menu or you can hit/restart endpoint straightforwardly.  \\nQ99) What i s the job Of HTTP REST API in DevOps?  \\nAs DevOps is absolutely centers around Automating your framework and gives changes over the pipeline \\nto various stages like an every CI/CD pipeline will have stages like form, test, mental soundness test, UAT, \\nDeployme nt to Prod condition similarly as with each phase there are diverse devices is utilized and \\ndistinctive innovation stack is displayed and there should be an approach to incorporate with various \\ninstrument for finishing an arrangement toolchain, there comes  a requirement for HTTP API , where each \\napparatus speaks with various devices utilizing API , and even client can likewise utilize SDK to interface \\nwith various devices like BOTOX for Python to contact AWS API’s for robotization dependent on occasions \\n, these days its not cluster handling any longer , it is generally occasion driven pipelines  \\nQ100) What are Micro services, and how they control proficient DevOps rehearses?  \\nWhere In conventional engineering , each application is stone monument application im plies that \\nanything is created by a gathering of designers, where it has been sent as a solitary application in \\nnumerous machines and presented to external world utilizing load balances, where the micro services \\nimplies separating your application into lit tle pieces, where each piece serves the distinctive capacities \\nexpected to finish a solitary exchange and by separating , designers can likewise be shaped to gatherings \\nand each bit of utilization may pursue diverse rules for proficient advancement stage, as a result of spry \\nimprovement ought to be staged up a bit and each administration utilizes REST API (or) Message lines to \\nconvey between another administration.  \\nSo manufacture and arrival of a non -strong form may not influence entire design, rather, some  usefulness \\nis lost, that gives the confirmation to productive and quicker CI/CD pipelines and DevOps Practices.  \\nGet DevOps  Online  Training  ', metadata={'source': 'ML questions dump.pdf', 'page': 117}),\n",
       " Document(page_content='Q101) What are the manners in which that a pipeline can be made in Jenkins?  \\nThere are two different ways of a pipeline can be made in Jenkins  \\nScripted Pipelines:  \\nProgressively like a programming approach  \\nExplanatory pipelines:  \\nDSL approach explicitly to make Jenkins pipelines.  \\nThe pipeline ought to be made in Jenkins document and the area can either be in SCM or nearby \\nframework.  \\nDefinitive and Scripted Pipelines are developed on a very basic level in an unexpected way. Definit ive \\nPipeline is a later element of Jenkins Pipeline which:  \\ngives more extravagant grammatical highlights over Scripted Pipeline language structure, and is intended \\nto make composing and perusing Pipeline code less demanding.  \\nQ102) What are the Labels in Je nkins and where it tends to be used?  \\nSimilarly as with CI/CD arrangement should be concentrated , where each application in the association \\ncan be worked by a solitary CI/CD server , so in association there might be various types of utilization like \\njava, c#,.NET and so forth, likewise with microservices approach your programming stack is inexactly \\ncoupled for the task , so you can have Labeled in every hub and select the choice Only assembled \\nemployments while name coordinating this hub, so when a manufact ure is planned with the mark of the \\nhub present in it, it hangs tight for next agent in that hub to be accessible, despite the fact that there are \\ndifferent agents in hubs.  \\nQ103) What is the utilization of Blueocean in Jenkins?  \\nBlue Ocean reconsiders the c lient experience of Jenkins. Planned from the beginning for Jenkins Pipeline, \\nyet at the same time good with free -form occupations, Blue Ocean diminishes mess and builds lucidity for \\neach individual from the group.  \\nIt gives complex UI to recognize each pha se of the pipeline and better pinpointing for issues and \\nextremely rich Pipeline editorial manager for apprentices.  \\nQ104) What is the callback modules in Ansible, give a few instances of some callback \\nmodules?  \\nCallback modules empower adding new practices to Ansible when reacting to occasions. Of course, \\ncallback modules control a large portion of the yield you see when running the direction line programs, ', metadata={'source': 'ML questions dump.pdf', 'page': 118}),\n",
       " Document(page_content='however can likewise be utilized to include an extra yield, coordinate with different apparatuses and \\nmarshall the occasions to a capacity backend. So at whatever point a play is executed and after it creates \\na few occasions, that occasions are imprinted onto Stdout screen, so callback module can be put into any \\ncapacity backend for log preparing.  \\nModel ca llback modules are ansible -logstash, where each playbook execution is brought by logstash in \\nthe JSON group and can be incorporated some other backend source like elasticsearch.  \\nQ105) What are the scripting dialects can be utilized in DevOps?  \\nAs with scrip ting dialects, the fundamental shell scripting is utilized to construct ventures in Jenkins \\npipelines and python contents can be utilized with some other devices like Ansible , terraform as a \\nwrapper content for some other complex choice unraveling underta kings in any mechanization as python \\nis more unrivaled in complex rationale deduction than shell contents and ruby contents can likewise be \\nutilized as fabricate ventures in Jenkins.  \\nQ106) What is Continuous Monitoring and why checking is basic in DevOps?  \\nDevOps draws out each association capacity of fabricate and discharge cycle to be a lot shorter with an \\nidea of CI/CD, where each change is reflected into generation conditions fastly, so it should be firmly \\nobserved to get client input. So the idea of con stant checking has been utilized to assess every \\napplication execution progressively (at any rate Near Real Time) , where every application is produced \\nwith application execution screen specialists perfect and the granular dimension of measurements are \\ntaken out like JVM details and even practical savvy measurements inside the application can likewise be \\nspilled out progressively to Agents , which thusly provides for any backend stockpiling and that can be \\nutilized by observing groups in dashboards and caut ions to get persistently screen the application.  \\nQ107) Give a few instances of persistent observing instruments?  \\nWhere numerous persistent observing instruments are accessible in the market, where utilized for an \\nalternate sort of use and sending model  \\nDocker compartments can be checked by consultant operator, which can be utilized by Elasticsearch to \\nstore measurements (or) you can utilize TICK stack (Telegraph, influxdb, Chronograph, Capacitor) for each \\nframework observing in NRT(Near Real Time) and You can utilize Logstash (or) Beats to gather Logs from \\nframework , which thusly can utilize Elasticsearch as Storage Backend can utilize Kibana (or) Grafana as \\nvisualizer.  \\nThe framework observing should be possible by Nagios and Icinga.  ', metadata={'source': 'ML questions dump.pdf', 'page': 119}),\n",
       " Document(page_content='Q108) What is docker swarm?  \\nGathering of Virtual machines with Docker Engine can be grouped and kept up as a solitary framework \\nand the assets likewise being shared by the compartments and docker swarm ace calendars the docker \\nholder in any of the machines under the bun ch as indicated by asset accessibility  \\nDocker swarm init can be utilized to start docker swarm bunch and docker swarm joins with the ace IP \\nfrom customer joins the hub into the swarm group.  \\nQ109) What are Microservices, and how they control productive DevO ps rehearses?  \\nWhere In conventional engineering , each application is stone monument application implies that \\nanything is created by a gathering of designers, where it has been conveyed as a solitary application in \\nnumerous machines and presented to extern al world utilizing load balancers, where the microservices \\nimplies separating your application into little pieces, where each piece serves the diverse capacities \\nexpected to finish a solitary exchange and by separating , engineers can likewise be shaped to  gatherings \\nand each bit of utilization may pursue distinctive rules for proficient advancement stage, on account of \\nlight-footed improvement ought to be staged up a bit and each administration utilizes REST API (or) \\nMessage lines to impart between another  administration.  \\nSo manufacture and arrival of a non -hearty variant may not influence entire design, rather, some \\nusefulness is lost, that gives the affirmation to proficient and quicker CI/CD pipelines and DevOps \\nPractices.  \\nQ110) What are the manners in w hich that a pipeline can be made in Jenkins?  \\nThere are two different ways of a pipeline can be made in Jenkins  \\nScripted Pipelines:  \\nProgressively like a programming approach  \\nExplanatory pipelines:  \\nDSL approach explicitly to make Jenkins pipelines.  \\nThe pipel ine ought to be made in Jenkins record and the area can either be in SCM or neighborhood \\nframework.  \\nDefinitive and Scripted Pipelines are developed in a general sense in an unexpected way. Explanatory \\nPipeline is a later element of Jenkins Pipeline which:  \\ngives more extravagant linguistic highlights over Scripted Pipeline sentence structure, and is intended to \\nmake composing and perusing Pipeline code simpler.  ', metadata={'source': 'ML questions dump.pdf', 'page': 120}),\n",
       " Document(page_content='Q111) What are the Labels in Jenkins and where it very well may be used?  \\nLikewise with CI/CD arrangement should be incorporated , where each application in the association can \\nbe worked by a solitary CI/CD server , so in association there might be various types of use like java, \\nc#,.NET and so forth, similarly as with microserv ices approach your programming stack is inexactly \\ncoupled for the undertaking , so you can have Labeled in every hub and select the alternative Only \\nassembled occupations while mark coordinating this hub, so when a fabricate is booked with the name of \\nthe hub present in it, it sits tight for next agent in that hub to be accessible, despite the fact that there are \\ndifferent agents in hubs.  \\nQ112) What is the utilization of Blueocean in Jenkins?  \\nBlue Ocean reexamines the client experience of Jenkins. Planned s tarting from the earliest stage for \\nJenkins Pipeline, yet at the same time good with free -form occupations, Blue Ocean lessens mess and \\nexpands clearness for each individual from the group.  \\nIt gives modern UI to recognize each phase of the pipeline and bet ter pinpointing for issues and rich \\nPipeline proofreader for fledglings.  \\nQ113) What is the callback modules in ansible, give a few instances of some callback modules?  \\nCallback modules empower adding new practices to Ansible when reacting to occasions. As a  matter of \\ncourse, callback modules control the greater part of the yield you see when running the direction line \\nprograms, yet can likewise be utilized to include an extra yield, coordinate with different instruments and \\nmarshall the occasions to a capaci ty backend. So at whatever point a play is executed and after it delivers \\na few occasions, that occasions are imprinted onto Stdout screen, so callback module can be put into any \\ncapacity backend for log handling.  \\nPrecedent callback modules are ansible -logstash, where each playbook execution is gotten by logstash in \\nthe JSON position and can be incorporated some other backend source like elasticsearch.  \\nQ114) What are the scripting dialects can be utilized in DevOps?  \\nAs with scripting dialects, the fundament al shell scripting is utilized to assemble ventures in Jenkins \\npipelines and python contents can be utilized with some other instruments like Ansible.  \\nQ115) For what reason is each instrument in DevOps is generally has some DSL (Domain \\nSpecific Language)?  \\nDevops is a culture created to address the necessities of lithe procedure, where the advancement rate is \\nquicker ,so sending should coordinate its speed and that needs activities group to arrange and work with ', metadata={'source': 'ML questions dump.pdf', 'page': 121}),\n",
       " Document(page_content='dev group, where everything can computerize ut ilizing content based , however it feels more like tasks \\ngroup than , it gives chaotic association of any pipelines, more the utilization cases , more the contents \\nshould be composed , so there are a few use cases, which will be sufficient to cover the req uirements of \\nlight-footed are taken and apparatuses are made by that and customization can occur over the device \\nutilizing DSL to mechanize the DevOps practice and Infra the board.  \\nQ116) What are the mists can be incorporated with Jenkins and what are the utilization cases?  \\nJenkins can be coordinated with various cloud suppliers for various use cases like dynamic Jenkins slaves, \\nDeploy to cloud conditions.  \\nA portion of the cloud can be incorporated are  \\n• AWS  \\n• Purplish blue  \\n• Google Cloud  \\n• OpenStack  \\nQ117) What are Docker volumes and what sort of volume ought to be utilized to accomplish \\nrelentless capacity?  \\nDocker volumes are the filesystem mount focuses made by client for a compartment or a volume can be \\nutilized by numerous holders, and there are distinctive s orts of volume mount accessible void dir, Post \\nmount, AWS upheld lbs volume, Azure volume, Google Cloud (or) even NFS, CIFS filesystems, so a volume \\nought to be mounted to any of the outer drives to accomplish determined capacity, in light of the fact \\nthat a lifetime of records inside compartment, is as yet the holder is available and if holder is erased, the \\ninformation would be lost.  \\nQ118) What are the Artifacts store can be incorporated with Jenkins?  \\nAny sort of Artifacts vault can be coordinated with Je nkins, utilizing either shell directions (or) devoted \\nmodules, some of them are Nexus, Jfrog.  \\nQ119) What are a portion of the testing apparatuses that can be coordinated with Jenkins and \\nnotice their modules?  \\nSonar module – can be utilized to incorporate t esting of Code quality in your source code.  \\nExecution module – this can be utilized to incorporate JMeter execution testing.  ', metadata={'source': 'ML questions dump.pdf', 'page': 122}),\n",
       " Document(page_content='Junit – to distribute unit test reports.  \\nSelenium module – can be utilized to incorporate with selenium for computerization testing . \\nQ120) What are the manufacture triggers accessible in Jenkins?  \\nFabricates can be run physically (or) either can naturally be activated by various sources like  \\nWebhooks - The webhooks are API calls from SCM, at whatever point a code is submitted into a vau lt (or) \\nshould be possible for explicit occasions into explicit branches.  \\nGerrit code survey trigger -Gerrit is an opensource code audit instrument, at whatever point a code \\nchange is endorsed after audit construct can be activated.  \\nTrigger Build Remotely – You can have remote contents in any machine (or) even AWS lambda capacities \\n(or) make a post demand to trigger forms in Jenkins.  \\nCalendar Jobs -Jobs can likewise be booked like Cron occupations.  \\nSurvey SCM for changes – Where your Jenkins searches for any progressions in SCM for the given interim, \\nif there is a change, a manufacture can be activated.  \\nUpstream and Downstream Jobs -Where a construct can be activated by another activity that is executed \\nalready.  \\nQ121) How to Version control Docker pictures?  \\nDocker pictures can be form controlled utilizing Tags, where you can relegate the tag to any picture \\nutilizing docker tag <image -id> order. Furthermore, on the off chance that you are pushing any docker \\ncenter library without labeling the default label would be doled out which is most recent, regardless of \\nwhether a picture with the most recent is available, it indicates that picture without the tag and reassign \\nthat to the most recent push picture.  \\nQ122) What is the utilization of Timestamper module in Jenkin s? \\nIt adds Timestamp to each line to the comfort yield of the assemble.  \\nQ123) Why you ought not execute an expand on ace?  \\nYou can run an expand on ace in Jenkins , yet it isn’t prudent, in light of the fact that the ace as of now \\nhas the duty of planning a ssembles and getting incorporate yields with JENKINS_HOME index, so on the \\noff chance that we run an expand on Jenkins ace, at that point it furthermore needs to manufacture \\napparatuses, and workspace for source code, so it puts execution over -burden in th e framework, if the \\nJenkins ace accidents, it expands the downtime of your fabricate and discharge cycle.  ', metadata={'source': 'ML questions dump.pdf', 'page': 123}),\n",
       " Document(page_content='Q124) What do the main benefits of DevOps?  \\nWith a single team composed of cross -functional comments simply working in collaboration, DevOps \\norganizati ons container produce including maximum speed, functionality, including innovation. Where \\ncontinue special benefits: Continuous software control. Shorter complexity to manage.  \\nQ125) What are the uses of DevOps tools?  \\n• Gradle. Your DevOps device stack will need a reliable build tool.  \\n• Git. Git is one from the most successful DevOps tools, widely applied across the specific \\nsoftware industry.  \\n• Jenkins. Jenkins is that go -to DevOps automation tool for many software community teams.  \\n• Bamboo.  \\n• Docker.  \\n• Kubernete s. \\n• Puppet Enterprise.  \\n• Ansible.  \\nQ126) What is DevOps beginner?  \\nDevOps is a society which supports collaboration between Development including Operations Team to \\ndeploy key to increase faster in an automated & repeatable way. In innocent words, DevOps backsi de is \\nestablished as an association of development and IT operations including excellent communication and \\ncollaboration.  \\nQ127) What is the roles and responsibilities of the DevOps engineer?  \\nDevOps Engineer manages with developers including the IT system t o manage the code releases. They \\nare both developers cases become interested in deployment including practice settings or sysadmins who \\nconvert a passion for scripting and coding more move toward the development front where all can \\nimprove that planning fr om test and deployment.  \\nQ128) Which is the top DevOps tools? and it’s Which tools have you worked on?  \\nDiscover about the trending Top DevOps Tools including Git. Well, if you live considering DevOps being a \\ntool when, you are wrong! DevOps does not a tool or software, it’s an appreciation that you can adopt for \\ncontinuous growth. file and, by practicing it you can simply coordinate this work among your team.  ', metadata={'source': 'ML questions dump.pdf', 'page': 124}),\n",
       " Document(page_content='Q129) Explain the typical characters involved in DevOps?  \\n• Commitment to the superior level in the org anization.  \\n• Need for silver to be delivered across the organization.  \\n• Version check software.  \\n• Automated tools to compliance to process.  \\n• Automated Testing  \\n• Automated Deployment  \\nQ130) What are your expectations from a career perspective of DevOps?  \\nTo be involved in the end to end delivery method and the most important phase of helping to change the \\nmanner so as to allow that development and operations teams to go together also understand each \\nother’s point of view.  \\nQ131) What does configuration management  under terms like infrastructure further review \\nsome popular tools used?  \\nIn Software Engineering Software Configuration Management is a unique task about tracking to make the \\nsetting configuration during the infrastructure with one change. It is done for d eploying, configuring and \\nmaintaining servers.  \\nQ132) How will you approach when each design must to implement DevOps?  \\nAs the application is generated and deployed, we do need to control its performance. Monitoring means \\nalso really important because it mig ht further to uncover some defects which might not have been \\ndetected earlier.  \\nQ133) Explain about from Continuous Testing  \\nFrom the above goal of Continuous Integration which is to take this application excuse to close users are \\nprimarily providing continu ous delivery. This backside is completed out any adequate number about unit \\ntesting and automation testing. Hence, we must validate that this system created and integrated with all \\nthe developers that work as required.  \\nQ134) Explain about from Continuous Delivery.  \\nContinuous Delivery means an extension of Constant Integration which primarily serves to make the \\nfeatures which some developers continue developing out on some end users because soon as possible. ', metadata={'source': 'ML questions dump.pdf', 'page': 125}),\n",
       " Document(page_content='During this process, it passes through  several stages of QA, Staging etc., and before for delivery to the \\nPRODUCTION system.  \\nQ135) What are the tasks also responsibilities of DevOps engineer?  \\nIn this role, you’ll work collaboratively including software engineering to use and operate our system s. \\nHelp automate also streamline our procedures and processes. Build also maintain tools for deployment, \\nmonitoring, including operations. And troubleshoot and resolve problems in our dev, search and \\nproduction environments.  \\nQ136) What is defined DevOps en gineer should know?  \\nDevOps Engineer goes including developers and that IT staff to manage this code releases. They live both \\ndevelopers who become involved through deployment including web services or sysadmins that become \\na passion for scripting and codin g more move into the development design where only can develop this \\nplanning from search also deployment.  \\nQ137) How much makes any DevOps engineer make?  \\nA lead DevOps engineer can get between $137,000 including $180,000, according to April 2018 job data \\nof Glassdoor. The common salary from any lead DevOps engineer based at the Big Apple is $141,452.  \\nQ138) What mean the specific skills required for a DevOps engineer?  \\nWhile tech abilities are a must, strong DevOps engineers further possess this ability to collaborate, multi -\\ntask, also always place that customer first. critical skills that all DevOps engineer requirements for success.  \\nQ139) What is DevOps also why is it important?  \\nImplementing the new approach would take in many advantages on an organization . A seamless \\ncollection up can be performed in the teams of developers, test managers, and operational executives \\nalso hence they can work in collaboration including each other to achieve a greater output on a project.  \\nQ140) What is means by DevOps lifecyc le? \\nDevOps means an agile connection between development including operations. It means any process \\nfollowed by this development because well because of help drivers clean of this starting of this design to \\nproduction support. Understanding DevOps means in complete excuse estimated DevOps lifecycle.  \\nTools for an efficient DevOps workflow. A daily workflow based at DevOps thoughts allows team ', metadata={'source': 'ML questions dump.pdf', 'page': 126}),\n",
       " Document(page_content='members to achieve content faster, be flexible just to both experiments also deliver value, also help each \\npart from t his organization use a learning mentality.  \\nQ142) Can you make DevOps without agile?  \\nDevOps is one about some key elements to assist you to achieve this. Can you do agile software evolution \\nwithout doing DevOps But managing agile software development and be ing agile are a couple really \\ndifferent things.  \\nQ143) What exactly defined is DevOps?  \\nDevOps is all of bringing commonly the structure also process of traditional operations, so being support \\ndeployment, including any tools, also practices of traditional c onstruction methods so as source control \\nalso versioning.  \\nQ144) Need for Continuous Integration:  \\n• Improves the quality of software.  \\n• Reduction in time taken to delivery  \\n• Allows dev team to detect and locate problems early  \\nQ145) Success factor for the Continuo us Integration  \\n• Maintain Code Repository  \\n• Automate the build  \\n• Perform daily checkin and commits to baseline  \\n• Test in clone environment  \\n• Keep the build fast  \\n• Make it easy to get the newest deliverables  \\nQ146) Can we copy Jenkins job from one server to other server?  \\nYes, we can do that using one of the following ways  \\n• We can copy the Jenkins jobs from one server to other server by copying the corresponding \\njobs folder.  \\n• We can make a copy of the existing job by making clone of a job directory with different name s \\n• Rename the existing job by renaming the directory  ', metadata={'source': 'ML questions dump.pdf', 'page': 127}),\n",
       " Document(page_content='Q147) How can we create the backup and copy in Jenkins?  \\nWe can copy or backup, we need to backup JENKINS_HOME directory which contains the details of all the \\njob configurations, build details etc.  \\nQ148) D ifference between “poll scm” and “build periodically”  \\nPoll SCM will trigger the build only if it detects the change in SCM, whereas Build Periodically will trigger \\nthe build once the given time period is elapsed.  \\nQ149) What is difference between docker ima ge and docker container?  \\nDocker image is a readonly template that contains the instructions for a container to start.  \\nDocker container is a runnable instance of a docker image  \\nQ150) What is Application Containerization?  \\nIt is a process of OS Level virtuali zation technique used to deploy the application without launching the \\nentire VM for each application where multiple isolated applications or services can access the same Host \\nand run on the same OS.  \\nGet Devops  100%  Practical  Training  \\nQ151) syntax for building docker image  \\ndocker  build –f <filename> -t imagename:version  \\nQ152) running docker image  \\ndocker run –dt –restart=always –p <hostport>:<containerport> -h <hostname> -v \\n<hostvolume>:<containervolume> imagename:version  \\nQ153) How to log into a container  \\ndocker exec –it <container ID> /bin/bash  \\nQ154) What is Puppet?  \\nPuppet is a Configuration Management tool, Puppet is used to automate administration tasks.  \\nQ155) What is Configuration Management?  \\nConfiguration Management is the System engineering process.  Configuration Management ap plied over \\nthe life cycle of a system provides visibility and control of its performance, functional, and physical \\nattributesrecording their status and in support of Change Management.  ', metadata={'source': 'ML questions dump.pdf', 'page': 128}),\n",
       " Document(page_content='Q156) List the Software Configuration Management Features.  \\n• Enforcement  \\n• Cooperating Enablement  \\n• Version Control Friendly  \\n• Enable Change Control Processes  \\nQ157) List out the 5 Best Software Configuration Management Tools.  \\n• CFEngine Configuration Tool.  \\n• CHEF Configuration Tool  \\n• Ansible Configuration Tool  \\n• Puppet Configuration Tool.  \\n• SALTSTACK Configuration Tool.  \\nQ158) Why should Puppet be chosen?  \\n• It has good community support  \\n• Easy to Learn Programming Language DSL  \\n• It is open source  \\nQ159) What is Saltstack?  \\nSaltStack is based on Python programming & Scripiting langu age. Its also a configuration tool.Saltstack \\nworks on a non -centralized model or a   master -client setup model. it provides a push and SSH methods \\nto communicate with clients.  \\nQ160) Why should Puppet to be chosen?  \\nThere are Some Reason puppet to be chosen.  \\n• Puppet is open source  \\n• Easy to Learn Programming Language DSL  \\n• Puppet has good community support  \\nQ161) Advantages of VCS  \\n1. Multiple people can work on the same project and it helps us to keep track of the files and \\ndocuments and their changes.  ', metadata={'source': 'ML questions dump.pdf', 'page': 129}),\n",
       " Document(page_content='2. We can merge the  changes from multiple developers to single stream.  \\n3. Helps us to revert to the earlier version if the current version is broke.  \\n4. Helps us to maintain multiple version of the software at the same location without rewriting.  \\nQ162) Advantages of DevOps  \\nBelow ar e the major advantages  \\nTechnical:  \\n1. Continuous software delivery  \\n2. Less Complexity  \\n3. Faster Resolution  \\nBusiness:  \\n1. Faster delivery of the features  \\n2. More stable operating environment  \\n3. Improved communication and collaboration between various teams  \\nQ163) Use cases where we can use DevOps  \\n1. Explain the legacy / old procedures that are followed to develop and deploy software  \\n2. Problems of that approach  \\n3. How can we solve the above issues using DevOps.  \\nFor the 1st and 2nd points, development of the application, problems in b uild and deployment, problems in \\noperations, problems in debugging and fixing the issues  \\nFor 3rd point explain various technologies we can use to ease the deployments, for development, explain \\nabout taking small features and development, how it helps for t esting and issue fixing.  \\nQ164) Major difference between Agile and DevOps  \\nAgile is the set of rules/principles and guidelines about how to develop a software. There are chances that \\nthis developed software works only on developer’s environment. But to relea se that software to public \\nconsumption and deploy in production environment, we will use the DevOps tools and Techniques for the \\noperation of that software.  \\nIn a nutshell, Agile is the set of rules for the development of a software, but DevOps focus more o n \\nDevelopment as well as Operation of the Developed software in various environments.  ', metadata={'source': 'ML questions dump.pdf', 'page': 130}),\n",
       " Document(page_content='Q165) What Are the Benefits Of  Nosql?  \\n• Non-relationals and schema -less data models  \\n• Low latency and high performance  \\n• Highly scalable  \\nQ166) What Are Adoptions Of Devops In Industry?  \\n• Use of the agile and other development processes and methods.  \\n• Demand for an increased rate of the production releases from application and business.  \\n• Wide availability of virtuals and cloud infrastructure from  both internal and external providers;  \\n• Increased usage of the data center ,automation and configuration management tools;  \\n• Increased focus on the test automation and continuous integration methods;  \\n• Best practices on the critical issues.  \\nQ167) How Is the Che f Used As a Cm Tool ?  \\nChef is the considered to be one of the preferred industry -wide CM tools. Facebook migrated its an \\ninfrastructure and backend IT to the Chef platform, for example. Explain how to the Chef helps you to \\navoid delays by automating proces ses. The scripts are written in Ruby. It can integrate with a cloud -based \\nplatforms and configure new systems. It provides many libraries for  the infrastructure development that \\ncan later to be deployed within a software. Thanks to its centralized managem ent system, one of the Chef \\nserver is enough to be used as the center for deploying various policies.  \\nQ168) Why Are the Configuration Management Processes And Tools Important ?  \\nTalk about to multiple software builds, releases, revisions, and versions for e ach  other software or \\ntestware that is being developed. Move on to explain the need for storing and maintaining data, keeping \\ntrack of the development builds and simplified troubleshooting. Don’t forget to mention that key CM \\ntools that can be used to the   achieve these objectives. Talk about how to tools like Puppet, Ansible, and \\nChef help in automating software deployment and configuration on several servers.  \\nQ169) Which Are the Some Of the Most Popular Devops Tools ?  \\nThe most popular DevOps tools includ ed` \\n• Selenium  \\n• Puppet  ', metadata={'source': 'ML questions dump.pdf', 'page': 131}),\n",
       " Document(page_content='• Chef \\n• Git \\n• Jenkins  \\n• Ansible  \\nQ170) What Are the Vagrant And Its Uses?  \\nVagrant used to virtual box as the hypervisor for virtual environments and in current scenario it is also \\nsupporting the KVM. Kernel -based Virtual Machine.  \\nVagrant is a tool that can created and managed environments for the testing and developing software. \\nDevops Training Free Demo  \\nQ171) How to Devops Is Helpful To Developers ?  \\nTo fix the bug and implements new features of the  quickly. It provides to the cla rity of communications \\namong team members.  \\nQ172) Name of The Popular Scripting Language Of the Devops ?  \\nPython  \\nQ173) List of The Agile Methodology Of the Devops?  \\n• DevOps is a process  \\n• Agile is the same as DevOps.  \\n• Separate group are framed.  \\n• It is problem solv ing. \\n• Developers managing production  \\n• DevOps is the development -driven release management  \\nQ174) Which Are The Areas of Devops Are Implemented?  \\n• Production Development  \\n• Creation of the productions feedback and its development  \\n• IT Operations development  ', metadata={'source': 'ML questions dump.pdf', 'page': 132}),\n",
       " Document(page_content='Q175) The Scope For SSH ?  \\n• SSH is a Secure Shell which provides users with a secure, encrypted mechanism to log into \\nsystems and transfer files.  \\n• To log out the remote machine and worked on command line.  \\n• To secure encrypted of the communications between two hosts over an insecure network.  \\nQ176) What Are The Advantages Of Devops With Respect To the Technical And Business \\nPerspective?  \\nTechnical benefits  \\n• Software delivery is continuous.  \\n• Reduces Complexity in problems.  \\n• Faster approach to resolve probl ems \\n• Manpower is reduced.  \\nBusiness benefits  \\n• High rate of delivering its features  \\n• Stable operating environments  \\n• More time gained to Add values.  \\n• Enabling faster feature time to market  \\nQ177) What Are The Core Operations Of the Devops In Terms Of the Development And \\nInfrastructure ?  \\nThe core operations of DevOps  \\n• Application development  \\n• Code developing  \\n• Code coverage  \\n• Unit testing  \\n• Packaging  \\n• Deployment With infrastructure  \\n• Provisioning  ', metadata={'source': 'ML questions dump.pdf', 'page': 133}),\n",
       " Document(page_content='• Configuration  \\n• Orchestration  \\n• Deployment  \\nQ178) What Are The Anti -patterns Of Devops?  \\nA pattern is common usage usually followed. If a pattern of thecommonly adopted by others does not \\nwork for your organization and you continue to blindly follow it, you are essentially adopting an anti -\\npattern. There are myths about DevOps.  \\nSome of them include  \\n• DevOps is a process  \\n• Agile equals DevOps  \\n• We need a separate DevOps group  \\n• Devops will solve all our problems  \\n• DevOps means Developers Managing Production  \\n• DevOps is Develo pment -driven release management  \\n• DevOps is not development driven.  \\n• DevOps is not IT Operations driven.  \\n• We can’t do DevOps – We’re Unique  \\n• We can’t do DevOps – We’re got the wrong people  \\nQ179)  What are The Most Important Thing Devops Helps Us Achieve?  \\nThe mos t important thing that the DevOps helps us achieve is to get the changes into production as \\nquickly as possible while that minimizing risks in software quality assurance and compliance. This is the \\nprimary objective of DevOps.  \\nFor example   clear communicat ion and better working relationships between teams i.e. both of   the Ops \\nteam and Dev team collaborate together to deliver good quality software which in turn leads to higher \\ncustomer satisfaction.  \\nQ180) How Can Make a Sure New Service Is Ready For The Products Launched?  \\n• Backup System  \\n• Recovery plans  ', metadata={'source': 'ML questions dump.pdf', 'page': 134}),\n",
       " Document(page_content='• Load Balancing  \\n• Monitoring  \\n• Centralized logging  \\nAre You Interest ed in DevOps  Course  ? Click here  \\nQ181) How to All These Tools Work for Together?  \\n• Given below is a generic logical of the flow where everything gets are automated for seamless \\ndelivery. However, its flow may vary from organization to the organization as per the \\nrequirement.  \\n• Developers develop the code and this source code is managed by Version Control System of \\nthe tools like Git etc.  \\n• Developers send to this code of the Git repository and any changes made in the code is \\ncommitted to this R epository.  \\n• Jenkins pulls this code from the repository using the Git plugin and build it using tools like Ant \\nor Maven.  \\n• Configuration managements tools like puppet deploys & provisions testing environment and \\nthen Jenkins releases this code on the test to environment on which testing is done using \\ntools like selenium.  \\n• Once the code are tested, Jenkins send it for the deployment on production to the server (even \\nproduction server are provisioned & maintained by tools like puppet).  \\n• After deployment Its contin uously monitored by tools like Nagios.  \\n• Docker containers provides testing environment to the test the build features.  \\nQ182) Which Are The Top Devops Tools?  \\nThe most popular DevOps tools are mentioned below  \\n• Git Version Control System tool  \\n• Jenkins Continuous Integration tool  \\n• Selenium Continuous Testing tool  \\n• Puppet, Chef, Ansible are Configuration Management and Deployment tools  \\n• Nagios Continuous Monitoring tool  ', metadata={'source': 'ML questions dump.pdf', 'page': 135}),\n",
       " Document(page_content='• Docker Containerization tool  \\nQ183) How to Devops Different From the Agile / Sdlc?  \\nAgile a re the set of the values and principles about how to produce i.e. develop software.  \\nExample   if you have some ideas and you want to  the turn those ideas into the working software, you can \\nuse the Agile values are principles as a way to do that. But, that software might only be working \\non  developer’s laptop or in a test environment. You want a way to quickly, easily and repeatably move \\nthat software into the production infrastructure, in a safe and simple way. To do that you needs are \\nDevOps tools and tech niques.  \\nYou can summarize by saying Agile of the software development methodology focuses on the \\ndevelopment for software but DevOps on the other hand is responsible for the development as well as \\ndeployment of the software to the safest and most reliable way to the possible. Here’s a blog that will \\ngive you more information of the evolutions of the DevOps.  \\nQ184) What Is The Need For Devops?  \\nAccording to me, this    should start by explaining the general market trend. Instead of the releasing big \\nsets of the  features, companies are trying to see if small features can be transported to their customers \\nthrough a series of the release trains. This have many advantages like quick feedback from the customers, \\nbetter quality of the software etc. which in turn leads  to the high customer satisfaction.  \\nTo achieve this, companies are required to  \\n• Increase deployment frequency  \\n• Lower failure rate of new releases  \\n• Shortened lead time between fixes  \\n• Faster mean time to recovery of the event of new release crashing  \\nQ185) What i s meant by Continuous Integration?  \\nIt’s the  development practice that requires developers to integrate code into a shared repository several \\ntimes a day. Each check -in  then verified by an automated build, allowing teams to the detect problems \\nearly.  \\nQ186) Mention some of the useful plugins in Jenkins.  \\nBelow, I have mentioned some important are Plugins:  \\n• Maven 2 project  ', metadata={'source': 'ML questions dump.pdf', 'page': 136}),\n",
       " Document(page_content='• Amazon EC2  \\n• HTML publisher  \\n• Copy artifact  \\n• Join \\n• Green Balls  \\nQ187) What is Version control?  \\nIts the system that records changes are the fil e or set of the files over time so that you can recall specific \\nversions later.  \\nQ188) What are the uses of Version control ?  \\nRevert files back to a previous state. Revert to the entire project back to a previous state.  \\nCompare changes over time.  \\nSee who la st modified the something that might to be causing a problem.  \\nWho introduced an issue and when.  \\nQ189) What are the containers?  \\nContainers are the of lightweight virtualization, heavier than ‘chroot’ but lighter than ‘hypervisors’. They \\nprovide isolation am ong processes  \\nQ190) What is meant by Continuous Integration?  \\nIt is a development practice that requires are  developers to integrate code into the shared repository \\nseveral times a day.  \\nQ191)  What’s a PTR in DNS?  \\nPointer (PTR) record to used for the revers  DNS (Domain Name System) lookup.  \\nQ192) What testing is necessary to insure a new service is ready for production?  \\nContinuous testing  \\nQ193) What is Continuous Testing?  \\nIt is the process of  executing on tests as part of the software delivery pipelines to obtain can immediate \\nfor  feedback is the business of the risks associated with in the latest  build.  ', metadata={'source': 'ML questions dump.pdf', 'page': 137}),\n",
       " Document(page_content='Q194) What is Automation Testing?  \\nAutomation testing or Test Automation is a process of  the automating that  manual process to test the \\napplication/system under test.  \\nQ195) What are the key elements of continuous testing?  \\nRisk assessments, policy analysis, requirements traceabilities, advanced analysis, test optimisation, and \\nservice virtualisations  \\nQ196) What are the Testing types supported b y Selenium?  \\nRegression testing and functional testing  \\nAlso Read>> Top Selenium Interview Questions & Answers  \\nQ197) What is Puppet?  \\nIt is a Configuration Management tool which is used to the automate administration of the tasks.  \\nQ198) How does HTTP work?  \\nThe HTTP protocol are works in a client and server model like most other protocols. A web browser using \\nwhich a request is initiated is called as a client and a web servers software which are the responds to that \\nrequest is called a server. World Wide Web Consortium of the Internet Engineering Task Force are two \\nimportants spokes are the standardization of the HTTP protocol.  \\nQ199) Describe two -factor authentication?  \\nTwo-factors authentication are the security process in which the user to provides two means of  the \\nidentification from separate categories of credentials.  \\nQ200) What is git add?  \\nadds the file changes to the staging area  \\nBecome  an DevOpsCertified  Expert  in 25Hours  \\nQ201) What is git commit?  \\nCommits the changes to the HEAD (staging area)  \\nQ202) What is git push?  \\nSends the changes to the remote repository  ', metadata={'source': 'ML questions dump.pdf', 'page': 138}),\n",
       " Document(page_content='Q203) What is git checkout?  \\nSwitch branch or restore working files  \\nQ204) What is git branch?  \\nCreates a branch  \\nQ205) What is git fetch?  \\nFetch the latest history from the remote server and updates the local repo  \\nQ206) What is git merge?  \\nJoins two or more branches together  \\nQ207) What is git pull?  \\nFetch from and integrate with another repository or a local branch (git fetch + git merge)  \\nQ208) What is git rebase?  \\nProcess of moving or combining a sequence of commits to a new base commit  \\nQ209) What is git revert?  \\nTo revert a commit that has already been published  and made public  \\nQ210 What is git clone?  \\nAns: clones the git repository and creates a working copy in the local machine  \\nQ211)   What is the difference between the Annie Playbook book and the characters?  \\nRoles  \\nThe characters are a restructured entity of a pl ay. Plays are on playbooks.  \\nA set of functions to accomplish the specific role. Maps between hosts and roles.  \\nExample: Common, Winners. Example: site.yml, fooservers.yml, webservers.yml.  \\nQ212) How do I see all the ansible_ variables list?  \\nBy naturally coll ecting “facts” about the machines, these facts can be accessed in Playbooks and in \\ntemplates. To see a list of all the facts about a computer, you can run a “setup” block as an ad hoc activity:  \\nAnsible -m system hostname  \\nIt will print a dictionary of all t he facts available for that particular host.  ', metadata={'source': 'ML questions dump.pdf', 'page': 139}),\n",
       " Document(page_content='Q213) What is Doctor?  \\nDocax is a container technology that connects your application and all its functions into the form of \\ncontainers to ensure that you are running uninterrupted in any situation of your use.  \\nQ214)   What is the Tagore film?  \\nTucker is the source of the dagger container. Or in other words, dagger pictures are used to create \\ncontainers.  \\n Q215) What is the tooger container?  \\nDogger Container is a phenomenon of the film.  \\n Q216)   Do we consider Dev Devils as a smart way?  \\nOf course, we !! The only difference between dynamic algorithms and DevObs is that the dynamic process \\nis implemented for the development section and activates both DevOps development and functionality.  \\n Q217)   What are the benefits of using Git?  \\nData personality and copy  \\nGet high  \\nonly one. A directory directory in the repository  \\nHigh disk usage and network performance  \\nJoint friendship  \\nGit can use any kind of projects.  \\nQ218) What is kernel?  \\nA kernel, the software that can easily chang e the hardware interfaces of your computer.  \\nQ219)  What is the difference between grep -i and grep -v? \\nI accept this value  \\nL) ls | grep -i docker  \\nDockerfile  \\ndocker.tar.gz  \\nls | grep -v docker  \\nDesktop  \\nDockerfile  ', metadata={'source': 'ML questions dump.pdf', 'page': 140}),\n",
       " Document(page_content='Documents  \\nDownloads  \\nYou can not find anything w ith name docker.tar.gz  \\nQ220) You can define a specific location for the file  \\nThis feature is generally used to give the server a replacement location. Let me tell you on the computer \\nbelow and I want to create 1GB swap space,  \\ndd if = / dev / zero = = / swa pfile1 bs = 1G count = 1  \\nGet DevOps  Training  with 100%  practical  Classes  \\nQ221)   What is the concept of sudo in  Linux?  \\nPseudo   is an application for Unix -and Linux -based systems that provide the ability to allow specific users \\nto use specific system commands in the system’s root   level.  \\nQ222) What is Jenkins pipe?  \\nJenkins pipeline (or simply “tube”) is an additiona l package that supports and activates continuous \\ndelivery tube in Jenkins.  \\nQ223)  How to stop and restart the toxin container?  \\nStop container: stop container container ID  \\nReboot the Tucker Container now: Docer Re -container ID  \\nQ224) Which sites are running b y Tagore?  \\nDocax is running on Linux and Cloud platforms only:  \\nUbuntu 12.04 LTS +  \\nFedora 20+  \\nRHEL 6.5+  \\nCentOS 6+  \\nGentoo  \\nArchLinux  \\nopenSUSE 12.3+  \\nCRUX 3.0+  \\nCloud:  \\nAmazon EC2  ', metadata={'source': 'ML questions dump.pdf', 'page': 141}),\n",
       " Document(page_content='Google Compute Engine  \\nMicrosoft Asur  \\nRackspace  \\nSince support is not supported, do no t work on Windows or Mac for token production, yes, even on \\nwindows you can use it for testing purposes  \\nQ225) What are the tools used for taxi networking?  \\nWe usually use karfs and taxi bear to do taxi networking.  \\nQ226) What does Tucker write?  \\nYou would like to have a number of taxiers containers, and at that time you need to create a file that \\ncreates a docer and type the command to make a taxi -up. It runs all containers mentioned in the docer \\ncompose file.  \\nQ227) What is a scrum?  \\nUsing scrime b ased on your complex software and product development task as small particles, it uses \\nreboots and additional procedures. Each replay is two weeks. Scrum has three characters: product owner, \\nscrum master and team  \\nQ228) Purpose for SSH?  \\nSSH is a secure shel l that allows users to login to a secure, encrypted mechanism into computers and \\ntransmitting files.Exit the remote machine and work on the command line.  \\nProtect encrypted communications between the two hosts on an unsafe network.  \\nQ229) Are DevOps implemen ted? \\nProduct development  \\nCreating product feedback and its development  \\nIT Activities Development.  \\nQ230) Do you want to list the active modes of DevOps?  \\nDevOps is a process  \\nLike the active DevOps.  \\nA separate group is configured.  \\nThis will solve the problem.  ', metadata={'source': 'ML questions dump.pdf', 'page': 142}),\n",
       " Document(page_content='Manufacturers manufacturing production  \\nDevOps is a development -driven output management  \\nQ231) Do you list the main difference between active and DevOffice?  \\nAgile:  \\nThere is something about dynamic software development  \\nDevops:  \\nDevOps is about software deployment and management.  \\nDevOps does not replace the active or lean. By removing waste, by removing gloves and improving \\nregulations, it allows the production of rapid and continuous products.  \\nQ232) For the popular scripting language of DevOps?  \\nPython  \\nQ233) How does DevOps help developers?  \\nTo correct the defect and immediately make innovative attributes.  \\nThis is the accuracy of the coordination between the members of the group.  \\nQ234) What is Vegand and its Uses?  \\nVirtual virtual box has been used as a hyperversion for virtual environments and in the current scenario it \\nsupports KVM. Kernel -based virtual machine  \\nVegant is a tool for creating and managing the environment for making software and experiments. \\nTutorials Tutorial Free Demo  \\nQ235) What is the main difference between Linux and Unix operating systems?  \\nUnix:  \\nIt belongs to the multitasking, multiuser operating system family.  \\nThese are often used on web servers and workstations.  \\nIt was originally derived from AT & T Unix, which was started by th e Bell Labs Research Center in the \\n1970s by Ken Thompson, Dennis Ritchie, and many others.  \\nOperating systems are both open source, but the comparison is relatively similar to Unix Linux.  \\nLinux:  \\nLinux may be familiar to each programming language.  ', metadata={'source': 'ML questions dump.pdf', 'page': 143}),\n",
       " Document(page_content='These pers onal computers are used.  \\nThe Unix operating system is based on the kernel.  \\nQ236) How can we ensure how to prepare a new service for the products launched?  \\nBackup system  \\nRecovery plans  \\nLoad balance  \\nTracking  \\nCentralized record  \\nQ237) What is the benefit of No SQL? \\nIndependent and schema -less data model  \\nLow latency and high performance  \\nVery scalable  \\nQ238) What is the adoption of Devokos in the profession  \\n1. Use of active and other developmental processes and methods.  \\n2. An increased ratio of production output is required from use and business.  \\n3. Virtual and Cloud Infrastructure Transfers from Internal and Outdoor Providers;  \\n4. Increased use of data center, automation and configuration management tools;  \\n5.  Focusing on testing automation and serial coordination sy stems;  \\n6.   Best Practices in Important Problems  \\nQ239) What are the benefits of NoSQL database on RDBMS?  \\nBenefits:  \\n1. ETL is very low  \\n2. Support for structured text is provided  \\n3.  Changes in periods are handled  \\n4.  Key Objectives Function.  \\n5.  The ability to measure hor izontally  \\n6.  Many data structures are provided.  \\n7.  Vendors may be selected.  ', metadata={'source': 'ML questions dump.pdf', 'page': 144}),\n",
       " Document(page_content='Q240) The first 10 capabilities of a person in the position of DevOp should be.  \\nThe best in system administration  \\nVirtualization experience  \\nGood technical skills  \\nGreat script  \\nGood deve lopment skills  \\nChef in the automation tool experience  \\nPeople management  \\nCustomer service  \\nReal-time cloud movements  \\nWho’s worried about who  \\nQ241) What is PTR in DNS?  \\nThe PNS (PTR) registration is used to turn the search DNS (Domain Name System).  \\nQ242) What do you know about DevOps?  \\nYour answer should be simple and straightforward. Start by explaining the growing importance of \\nDevOps in information technology. Considering that the efforts of the developments and activities to \\naccelerate the delivery of softwa re products should be integrated, the minimum failure rate. DevOps is a \\nvalue -practical procedure in which the design and performance engineers are able to capture the product \\nlevel or service life cycle across the design, from design and to the design lev el \\nQ243) Why was Dev’s so important in the past few years?  \\nBefore discussing the growing reputation of DevOps, discuss the current industry scenario. The big \\nplayers like Netflix and Facebook begin with some examples of how this business can help to develop and \\nuse unwanted applications. Facebook’s continuous use and coding license models, and how to measure it, \\nwhile using Facebook to ensure the quality of the experience. Hundreds of lines are implemented without \\naffecting ranking, stability and security. Dipops Training Course  \\nYour next application must be Netfli x. This streaming and on -the-video video company follows similar \\nprocedures with complete automated processes and systems. Specify user base of these two companies: \\nFacebook has 2 billion users, Netflix provides online content for more than 100 million use rs worldwide. \\nReduced lead time between the best examples of bugs, bug fixes, runtime and continuous supplies and \\nthe overall reduction of human costs.  ', metadata={'source': 'ML questions dump.pdf', 'page': 145}),\n",
       " Document(page_content='Q244) What are some of the most popular DevOps tools?  \\nThe most popular DevOps tools include:  \\nSelenium  \\nPuppet \\nChef \\nGit information  \\nJenkins  \\nAnsible  \\nTucker Tipps Online Training  \\nQ245) What is Version Control, and why should VCS use?  \\nDefine the control bar and talk about any changes to one or more files and store them in a centralized \\nrepository. VCS Tools remem bers previous versions and helps to:  \\nMake sure you do not go through changes over time.  \\nTurn on specific files or specific projects to the older version.  \\nExplore the problems or errors of a particular change.  \\nUsing VCS, developers provide flexibility to wo rk simultaneously on a particular file, and all changes are \\nlogically connected.  \\nQ246) Is There a Difference Between Active and DevOps? If yes, please explain  \\nAs a DevOps Engineer, interview questions like this are very much expected. Start by explaining t he clear \\noverlap between DevOps and Agile. Although the function of DevOps is always synonymous with dynamic \\nalgorithms, there is a clear difference between the two. Agile theories are related to the soft product or \\ndevelopment of the software. On the othe r hand, DevOps is handled with development, ensuring quick \\nturnaround times, minimal errors and reliability by installing the software continuously.  \\nQ247) Why are structural management processes and tools important?  \\nTalk about many software developments, r eleases, edits and versions for each software or testware. \\nDescribe the need for data storage and maintenance, development of developments and tracking errors \\neasily. Do not forget to mention key CM tools that can be used to achieve these goals. Talk about  how \\nthe tools, such as buffet, aseat, and chef are useful in automating software deployment and configuration \\non multiple servers.  ', metadata={'source': 'ML questions dump.pdf', 'page': 146}),\n",
       " Document(page_content='Q248) How is the chef used as a CM tool?  \\nChef is considered one of the preferred professional CM Tools. Facebook has changed  its infrastructure \\nand the Shef platform keeps track of IT, for example. Explain how the chef helps to avoid delays by \\nautomating processes. The scripts are written in ruby. It can be integrated into cloud -based platforms and \\nconfigures new settings. It p rovides many libraries for infrastructure development, which will then be \\ninstalled in a software. Thanks to its centralized management system, a chef server is sufficient to use \\nvarious policies as the center of ordering.  \\nQ249) How do you explain the conc ept of “Infrastructure Index” (IAC)?  \\nThis is a good idea to talk about IAC as a concept, sometimes referred to as a programming program, \\nwhere the infrastructure is similar to any other code. The traditional approach to managing infrastructure \\nis how to ta ke a back seat and how to handle manual structures, unusual tools and custom scripts   \\nQ250) List the essential DevOps tools.  \\nGit information  \\nJenkins  \\nSelenium  \\nPuppet  \\nChef \\nAnsible  \\nNagios  \\nLaborer  \\nMonit  \\nEl-Elistorsch, Lestastash, Gibbon  \\nCollectd  / Collect  \\nGit Information (Gitwidia)  \\nQ251) What are the main characters of DevOps engineers based on growth and infrastructure?  \\nDevOps Engineer’s major work roles  \\nApplication Development  \\nDeveloping code  \\nCode coverage  \\nUnit testing  ', metadata={'source': 'ML questions dump.pdf', 'page': 147}),\n",
       " Document(page_content='Packaging  \\nPreparing with infrastructure  \\nContinuous integration  \\nContinuous test  \\nContinuous sorting  \\nProvisioning  \\nConfiguration  \\nOrchestration  \\nDeployment  \\nQ252) What are the advantages of DevOps regarding technical and business perspective?  \\nTechnical Advantages:  \\nSoftware delivery conti nues.  \\nProblems reduce austerity.  \\nFast approach to solving problems  \\nHumans are falling.  \\nBusiness Benefits:  \\nThe higher the rate for its features  \\nFixed operating systems  \\nIt took too long to add values.  \\nRun fast time for the market  \\nLearn more about DevOps bene fits from this information blog.  \\nQ253) Purpose for SSH?  \\nSSH is a secure shell that allows users to login to a secure, encrypted mechanism into computers and \\ntransmitting files.  \\nExit the remote machine and work on the command line.  \\nProtect encrypted communi cations between the two hosts on an unsafe network.  \\nQ254) Which part of DevOps is implemented?  \\nProduct development  \\nCreating product feedback and its development  \\nIT Activities Development  ', metadata={'source': 'ML questions dump.pdf', 'page': 148}),\n",
       " Document(page_content='Q255) List the DevOps’s active algorithm.  \\nDevOps is a process  \\nLike the active DevOps.  \\nA separate group is configured.  \\nThis will solve the problem.  \\nManufacturers manufacturing production  \\nDevOps is a development -driven output management  \\nQ256) List the main difference between active and devOps.  \\nAgile:  \\nThere is something  about dynamic software development  \\nDevops:  \\nDevOps is about software deployment and management.  \\nDevOps does not replace the active or lean. By removing waste, by removing gloves and improving \\nregulations, it allows the production of rapid and continuous pr oducts.  \\nQ257) For the popular scripting language of DevOps.  \\nPython  \\nQ258) How does DevOps help developers?  \\nCorrect the error and activate new features quickly.  \\nIt provides clarity of clarity between the members of the group.  \\nQ259) What are the speed and its  benefits?  \\nVirtual box has been used as a hyper version for virtual environments and in the current scenario, it \\nsupports KVM. Kernel -based virtual machine  \\nVegant is a tool for creating and managing the environment for making software and experiments.  \\nQ260 ) What is the use of Anuj?  \\nIt is mainly used for information technology infrastructure to manage or use applications for remote \\napplications. We want to sort an app on the nodes of 100 by executing one command, then the animation \\nis actually in the picture , but you need to know or run some knowledge on the animated script.  ', metadata={'source': 'ML questions dump.pdf', 'page': 149}),\n",
       " Document(page_content='Q261) Provide a Deployment Use Cases in Kubernetes  \\nDeployment Use Cases in Kubernetes are given below:  \\nUse Case 1 - Create a Deployment:  On the creation of deployment, Pods are created au tomatically by \\nReplicaSet in the background.  \\nUse Case 2 - Update Deployment: Creation of new ReplicaSet happens and now the deployment is \\nupdated. Deployment revisions are updated through these new ReplicaSet.  \\nUse Case 3 - Rollback Deployment: If the current  deployment state is not steady, rollback of deployment \\nhappens. But we can see the container images are updated.  \\nUse Case 4 - Scale a Deployment: Based on the requirement, scaling up or scaling down can be performed \\non each and every deployment.  \\nUse Case 5 - Pause the Deployment: To apply various fixes, deployment can be paused and later resumed.  \\nQ262) Give the different methods of pipelines made in Jenkins  \\nExplanatory Pipelines and Scripted Pipelines are the two methods of pipelines made in Jenkins.  \\nQ263) C an you list out some of the core operations of DevOps?  \\nUnit Testing, Deployment, Code Building, Packaging, and Code coverage are the core operations of \\nDevOps.  ', metadata={'source': 'ML questions dump.pdf', 'page': 150}),\n",
       " Document(page_content='Q264) For a DevOps Engineer, which is the most important scripting language?  \\nAny simple and user -friendly scripting language would suite for a DevOps Engineer. For example, Python \\nis becoming popular while working on DevOps.  \\n \\nQ266) What are the checks to be done when a Linux build server become slow suddenly?  \\nPerform a check on the following items:  \\n1. System Level Troubleshooting: You need to make checks on various factors like application server \\nlog file, WebLogic logs, Web Server Log, Application Log file, HTTP to find if there are any issues \\nin server receive or response time for deliberateness. Chec k for any memory leakage of \\napplications.  \\n2. Application Level Troubleshooting: Perform a check on Disk space, RAM and I/O read -write issues.  \\n3. Dependent Services Troubleshooting: Check if there is any issues on Network, Antivirus, Firewall, \\nand SMTP server res ponse time.  \\nQ267) In Ubuntu, how will you enable startup sound?  \\nFollow the below steps to enable startup sound in Ubuntu:  \\n1. In Ubuntu, click on “Control Gear” and click on “Startup Applications”.  ', metadata={'source': 'ML questions dump.pdf', 'page': 151}),\n",
       " Document(page_content='2. Startup Application Preference window appears. To add an entry , click on “Add”  \\n3. Provide the information in the fields such as Command, Name, and Comment. Once the processes \\nare done, logout and login again.  \\nQ268) Provide the steps to create launchers on an Ubuntu desktop  \\nBelow are the steps to create launchers on an U buntu Desktop:  \\nIn Ubuntu system, press Alt+F2.  \\nType “ gnome -desktop -item-edit –create -new~/desktop ”. You will get a GUI dialog box which will create a \\nlauncher on Ubuntu desktop.  \\nQ269) Mention some of the top rated DevOps tools  \\nNagios, Jenkins, Docker, Git, Puppet, Chef, and Selenium are some of the topmost DevOps Tools.  \\nQ270) Write a shell script to add two numbers  \\nBelow is the shell script to add two numbers  \\necho “Enter no 1”read a  \\necho “Enter no 2”  \\nread b  \\nc= ‘expr $a + $b’  \\necho “ $a+ $b=$c”  \\nQ271) What are the technical benefits of DevOps?  \\nWith DevOps, you can deliver the features quickly, possible to add values as we have more time and \\ncreate firm operating environments.  \\nQ272) Give some benefits of Git  \\nBelow are some of the useful benefits of  Git: \\n1. As Git is one of the best -distributed version control system, you will be able to track changes \\nmade to a file.  \\n2. You can revert the changes whenever it is required  ', metadata={'source': 'ML questions dump.pdf', 'page': 152}),\n",
       " Document(page_content='3. Central cloud repository is available where the users can commit changes and share with  others in \\nthe team.  \\nQ273) What is the Git command to add one or more files to staging?  \\nTo add one or more files to staging, use the command “git add <filename.> git add**”  \\nQ274) Provide the Git command to send the modifications to the master branch of you r remote \\nrepository  \\nWhen you need to send the modifications to the master branch, use the command “git push origin \\nmaster”  \\nQ275) What is Maven?  \\nMaven is a DevOps tool used for building Java applications which helps the developer with the entire \\nprocess of a software project. Using Maven, you can compile the course code, perform functionals and \\nunit testing, and upload packages to remote repositories.  \\nGet DevOps  Online  Training  with Real Time  Projects  \\nQ276) What is the command to install Maven in Ubuntu system?  \\nTo install Maven in Ubuntu system, use the command “sudo apt -get install mvn” or “sudo apt -get install \\nmaven”.  \\nQ277) How will you validate whether the maven is done correctly?  \\nTo confirm the installation of Maven, use the command “mvn -version”.  \\nQ278) Provide few differences between DevOps and Agile:  \\nThe below tables provides a very few differences between DevOps and Agile:  \\nS. No  Parameters  DevOps  Agile  \\n1. Purpose  DevOps to manage the overall \\nengineering process  Agile to manage complex \\nprojects  \\n2. Team Size  A huge team is required to \\ninvolve with the work and \\ncommunicate with the A smaller team is well enough  ', metadata={'source': 'ML questions dump.pdf', 'page': 153}),\n",
       " Document(page_content='stakeholders to resolve the \\nissues  \\n3. Implementation  DevOps mainly focus on \\ncollaboration and hence there \\nare no frequently permitted \\nframewo rk There are few frameworks in \\nAgile which are safe and fray  \\n4. Feedback  Feedback is received from \\ninternal team  Feedback is received from \\ncustomers  \\n5. Automation  DevOps has a primary goal as \\nAutomation.  Agile do no focus on automation \\nthough it is helpful  \\n6. Tools used  Chef, Puppet, AWS are some of \\nthe popular DevOps tools  Kanboard, Jira, Bugzilla are some \\nof the popular Agile tools  \\nQ279) What is JFrog Artifactory in DevOps?  \\nJFrog Artifactory  is a binary repository manager which is useful to store the build process outcomes. JFrog \\nafford replication, high availability, disaster recovery, and scalability which works with many cloud \\nstorages.  \\nQ280) Write a script in Python for DevOps learners to  find palindrome of a sequence  \\nBelow is the script in Python for DevOps learners to find palindrome of a sequence  \\na=input (“enter sequence”)b=a [: : -1] if a==b:  \\nprint (“palindrome”)  \\nelse:  \\nprint (“not palindrome”)  \\nQ281) Give an example for Fibonacci series  \\nBelow is an example for Fibonacci series:  ', metadata={'source': 'ML questions dump.pdf', 'page': 154}),\n",
       " Document(page_content='# Enter number of terms needed                                             #0, 1, 1, 2, 3, 5…….       a=int(input(“Enter the \\nterms”))  \\nf=0 \\ns=1 \\nif a<=0:  \\nprint(“The required series is \\\\n” ,f)  \\nelse:  \\nprint (f, s, end=” “)  \\nfor x in range (2, a):  \\nnext=f+s  \\nprint(next, end= “ “)  \\nf=s \\ns=net  \\nQ282) Explain NumPy in DevOps  \\nThere are many packages in Python and NumPy - Numerical Python is one among them. This is useful for \\nscientific computing containing powerful n -dimensional array object. We can get tools from NumPy to \\nintegrate C, C++ and so on.  \\nQ283) What are the available cloud computing platforms in DevOps?  \\nMicrosoft Azure, Google Cloud, and Amazon Web Services are the top three cloud computing platforms \\nin DevOp s. \\nQ284) Describe Ansible  \\nAnsible  is a very simple automation engine which is useful in automating tasks like configuration \\nmanagement, intra -service orchestration, cloud provisioning, and application deployment. Ansible does \\nnot use any additional custom security infrastructure or agents  and hence this becomes very simple for \\ndeployment. By connecting to nodes and pushing out Ansible modules (small programs), you can see the \\nworking of Ansible.  \\nQ285) Provide an example of how a simple Ansible playbook appears  \\nBelow is an example of simple  ansible -playbook:  \\n#Simple Ansible Playbook  ', metadata={'source': 'ML questions dump.pdf', 'page': 155}),\n",
       " Document(page_content='• Run command1 on server1  \\n• Run command2 on server2  \\n• Run command3 on server3  \\n• Run command4 on server4  \\n• Restarting Server1  \\n• Restarting Server2  \\n• Restarting Server3  \\n• Restarting Server4  \\nQ286) Provide an example of how a complex ansible -playbook appears  \\nBelow is an example of complex ansible -playbook:  \\n#Complex Ansible Playbook  \\n• Deploy 50 VMs on Public Cloud  \\n• Deploy 50 VMs on Private Cloud  \\n• Provision Storage to all VMs  \\n• Setup Monitoring Components  \\n• Setup Cluster Configuration  \\n• Install and Configure backup clients on VMs  \\nQ287) Can you provide a sample YAML format?  \\nRefer to the below sample YAML format:  \\n#Simple Ansible Playbook1.yml  \\n            – \\n            name: Play 1  \\nhost: loca lhost \\ntasks:  \\n• name: Execute command ‘date’  \\ncommand: date  \\n• name: Execute script on server  ', metadata={'source': 'ML questions dump.pdf', 'page': 156}),\n",
       " Document(page_content='script: mytest_script.sh  \\n• name: install httpd service  \\nyum:  \\n      name: httpd  \\n      state: present  \\n• name: Start web server  \\nservice:  \\n      name: httpd  \\n      state: started  \\nQ288) Explain Docker  \\nAn open source that automates the application deployment is called a Docker. You can see the Docker \\ncontainer can be seen running in both Windows and Linux systems. Docker technology is promoted to \\nwork with vendors like cloud,  windows, Linux and Microsoft. Containers are deployed by Docker at all \\nlayers of the hybrid cloud.  \\nQ289) Is it possible to consider DevOps as Agile Methodology?  \\nYes, it is possible to consider DevOps as Agile Methodology, still we have differences between  these two. \\nImplementation of DevOps is possible on both development and operations section whereas Agile \\nmethodology implementation is possible only on development section.  \\nQ290) List out the tools helpful for docker networking  \\nDocker swarm and Kubernetes  are the tools helpful for docker networking.  \\nQ291) Provide the difference between git pull and git fetch  \\nIf there are any changes or commits done in the central repository branch, Git pull command performs a \\npull of those changes and update the local repo sitory targeted branch. Git fetch is little similar to git pull \\nbut has a slight difference. When using the git fetch command, all the new commits are pulled from the \\ndesired branch and those get stored in the new branch of your local repository.  You can make use of git \\nmerge once git fetch is done to see the changes in your target branch. Once the merging is done \\nbetween fetched branch and target branch, the target branch gets updated. Just remember the equation \\n“git merge + git fetch = git pull”.  ', metadata={'source': 'ML questions dump.pdf', 'page': 157}),\n",
       " Document(page_content='Q292) E xplain Git stash drop  \\nTo remove the stashed items, use the Git command “stash drop”. By default, it will remove eradicate the \\nlast added stash and also when a specific item is added as an argument, it will be removed.  \\nQ293) Provide few branching strategies  \\nTask Branching, Feature branching and Release branching are the three branching strategies.  \\n1. Task Branching: Along with the task key comprised in the branch name, each task is employed on \\nits own branch.  \\n2. Feature branching: The whole changes for a  feature is placed inside a branch. Once the feature \\nbranch is completely tested and evaluated using automated tests, a merge happens between the \\nbranch and master.  \\n3. Release branching: When the develop branch has acquired enough features for a release, it i s \\npossible to clone that specific branch and form a release branch. Once the release branch is \\nformed, we cannot add any new features and the next release cycle starts.  \\nQ294) Define Jenkins  \\nJenkins, a continuous integration tool is an open source written i n Java language. When we experience \\nchanges, tracking the version control system, initiating and monitoring a build system are some of the \\nprocess carried out by Jenkins. On successful tracking and monitoring, notifications and reports are \\nprovided to aler t the respective squad.  \\nQ295) Can you give me an example of simple Jenkins pipeline?  \\nBelow is the best example of simple Jenkins pipeline:  \\npipeline {     agent any  \\nstages {  \\nstage(‘build’) {  \\nsteps {  \\necho “Compiled Successfully! !”;  \\n} ', metadata={'source': 'ML questions dump.pdf', 'page': 158}),\n",
       " Document(page_content='} \\n} \\nQ296) Give me the procedure to create Jenkins jobs  \\nLet us have an example to create simple WelcomeGuys application.  \\n1. Open the Jenkins dashboard and click on “New Item”.  \\n2. Now enter the item name, for example, WelcomeGuys.  \\n• Select “Freestyle project” and click “Ok”  \\n1. You will get a different screen where you need to enter few more details of the job like project \\nname, description, Advanced project options, source code management, branches to build, and \\nrepository browser.  \\n2. Click “Save” to apply the changes made.  \\n3. Click on Bu ild Now option once the job details are saved.  \\n4. When the build is scheduled, the build starts to run and the build history section indicates the \\nprogress of build.  \\nQ297) What are the points to check when an application is not coming up?  \\nPerform the followin g checks when an application is not coming up:  \\n1. Validate all the file logs  \\n2. Check whether the web server is receiving the user’s request or not  \\n3. Check the status of process id  \\n4. Analyse the network connection  ', metadata={'source': 'ML questions dump.pdf', 'page': 159}),\n",
       " Document(page_content='Q298) What are the tasks performed by Puppet Slave and Puppet Master?  \\nThe below image shows the task details performed by Puppet Slave and Puppet Master:  \\n \\nQ299) What are pre -requisites to install and configure Puppet Master server on Linux?  \\nBoth client node and master node must be accessible. Make s ure you have internet access to both the \\nnodes so that you can install packages from puppet labs repositories. Better to disable the firewalls if \\nenabled just to avoid few issues at the time of configurations.  \\nQ300) In puppet, where you can find codedir?  \\nIn windows, you can find in the location  “%PROGRAMDATA% \\\\PuppetLabs \\\\code”. In Linux/Unix, you can \\nfind in the location  “/etc/puppetlabs/code ”. \\nQ301) Define IaC  \\nIaC stands for Infrastructure as Code. This indicates the automation of IT operations like buildin g, \\ndeploying, and managing with the help of code, instead of handling with manual process. Below is a \\ndiagrammatic representation of IaC.  \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 160}),\n",
       " Document(page_content='Q302) Can you provide a diagrammatic explanation on Chef Architecture?  \\nBelow is the diagrammatic explanation of chef  architecture  \\n \\nQ303) Give a solution when a resource action is not specified in Chef  \\nChef applied the default action when a resource action is not specified in Chef. Below is the best example:  \\n      File ‘C: \\\\User \\\\Administrator \\\\chef -repo \\\\settings.ini’ do       action :create  \\n              content ‘greetings=welcome all’  \\n                     end ', metadata={'source': 'ML questions dump.pdf', 'page': 161}),\n",
       " Document(page_content='Q304) Explain NRPE available in Nagios  \\nNRPE stands for Nagios Remote Plugin Executor. This is an addon  intended to permit the users to execute \\nNagios plugins. The vital reason for using this addon is to monitor local resources such as memory usage, \\nCPU load and so on.  \\nQ305) Through SDLC, how Docker provides steady computing environment?  \\nBelow are the steps  on how Docker provides steady computing environment:  \\n1. A Docker image is created is built by a Docker file and all the project codes are contained in that \\nimage.  \\n2. You can create many Docker containers by running that image.  \\n3. Now, you can upload the image on D ocker Hub and anyone can pull the image from Docker Hub \\nto build a container.  \\n \\nQ306) Explain Docker Compose  \\nTo configure and run applications easily that are made up of multiple containers, you can use Docker \\nCompose. Let us have the below example for Doc ker Compose:  \\nThree different containers where on running on web app, second on postgres and third container running \\non redis. All these three are in one single YAML file. Now to run all three connected containers with a \\nsingle command, Docker Compose is us ed. \\nQ307) What is the command in git to modify the commit message?  \\nUse the command “amend” to modify the commit message.  \\nQ308) Give us two methods to install Jenkins  \\nBelow are the top two methods to install Jenkins:  \\n1. Download Jenkins archive file  ', metadata={'source': 'ML questions dump.pdf', 'page': 162}),\n",
       " Document(page_content='2. In tomcat, you need to deploy Jenkins.war to web apps folder.  \\nQ309) What is the command to connect a container to a network?  \\nUse the command “docker run -itd –network=multi -host-network busybox” to connect a container to a \\nnetwork.  \\nQ310) What is the usage of chmod command in DevOps?  \\nTo modify the access authorizations of file system objects, chmod is used which is a system call and \\ncommand. You can also modify special mode flags.  \\nQ311) What is the information available in Node status?  \\nCapacity and Allocatab le, Addresses, and conditions are the information available in Node status. The \\ncommand to display the Node status information is “kubectl describe node <node -name -here>.  \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 163}),\n",
       " Document(page_content=\"Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463XInterview Question Series # 2\\nPython Programming\\nNumpy\\n1. Why is python numpy better than lists?\\nPython numpy arrays should be considered instead of a list because they are fast, \\nconsume less memory and convenient with lots of functionality.\\n2. Describe the map function in Python? \\nmap function executes the function given as the first argument on all the elements of the \\niterable given as the second argument.\\n3.Generate array of ‘100’ random numbers sampled from a standard normal \\ndistribution using Numpy\\nnp.random.rand(100) will create 100 random numbers generated from standard normal \\ndistribution with mean 0 and standard deviation 1.\\n4. How to count the occurrence of each value in a numpy array?\\nUse numpy.bincount()\\n>>> arr = numpy.array([0, 5, 5, 0, 2, 4, 3, 0, 0, 5, 4, 1, 9, 9])\\n>>> numpy.bincount(arr)\\nThe argument to bincount() must consist of booleans or positive integers. Negative \\nintegers are invalid.\\n5. Does Numpy Support Nan?\\nnan, short for “not a number”, is a special floating point value defined by the IEEE -754 \\nspecification. Python numpy supports nan but the definition of nan is more system \\ndependent and some systems don't have an all round support for it like older cray and vax \\ncomputers.\\n6.What does ravel() function in numpy do?\\nIt combines multiple numpy arrays into a single array\\n7.What is the meaning of axis=0 and axis=1?\\nAxis = 0 is meant for reading rows, Axis = 1 is meant for reading columns\", metadata={'source': 'ML questions dump.pdf', 'page': 164}),\n",
       " Document(page_content=\"Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X8. What is numpy and describe its use cases?\\nNumpy is a package library for Python, adding support for large, multi -dimensional arrays \\nand matrices, along with a large collection of high level mathematical functions. In simple \\nwords, Numpy is an optimized version of Python lists like Financial functions, Linear \\nAlgebra, Statistics, Polynomials, Sorting and Searching etc.\\n9.How to remove from one array those items that exist in another?\\n>>> a = np.array([5, 4, 3, 2, 1])\\n>>> b = np.array([4, 8, 9, 10, 1])\\n# From 'a' remove all of 'b'\\n>>> np.setdiff1d(a,b)\\n# Output:\\n>>> array([5, 3, 2])\\n10. How to sort a numpy array by a specific column in a 2D array? \\n#Choose column 2 as an example\\n>>> import numpy as np\\n>>> arr = np.array([[1, 2, 3], [4, 5, 6], [0,0,1]])\\n>>> arr[arr[:,1].argsort()]\\n# Output\\n>>> array([[0, 0, 1], [1, 2, 3], [4, 5, 6]])\\n11.How to reverse a numpy array in the most efficient way?\\n>>> import numpy as np\\n>>> arr = np.array([9, 10, 1, 2, 0])\\n>>> reverse_arr = arr[:: -1]\\n12.How to calculate percentiles when using numpy?\\n>>> import numpy as np\\n>>> arr = np.array([11, 22, 33, 44 ,55 ,66, 77])\\n>>> perc = np.percentile(arr, 40)  #Returns the 40th percentile\\n>>> print(perc)\\n13.What Is The Difference Between Numpy And Scipy?\\nNumPy would contain nothing but the array data type and the most basic operations: \\nindexing, sorting, reshaping, basic element wise functions, et cetera. All numerical code \\nwould reside in SciPy. SciPy contains more fully -featured versions of the linear algebra \\nmodules, as well as many other numerical algorithms.\", metadata={'source': 'ML questions dump.pdf', 'page': 165}),\n",
       " Document(page_content='Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X14.What Is The Preferred Way To Check For An Empty (zero Element) Array?\\nFor a numpy array, use the size attribute. The size attribute is helpful for determining the \\nlength of numpy array:\\n>>> arr = numpy.zeros((1,0))\\n>>> arr.size\\n15.What Is The Difference Between Matrices And Arrays?\\nMatrices can only be two -dimensional, whereas arrays can have any number of \\ndimensions\\n16. How can you find the indices of an array where a condition is true?\\nGiven an array a, the condition arr > 3 returns a boolean array and since False is \\ninterpreted as 0 in Python and NumPy.\\n>>> import numpy as np\\n>>> arr = np.array([[9,8,7],[6,5,4],[3,2,1]])\\n>>> arr > 3\\n>>> array([[True, True, True],\\n[ True,  True,  True],\\n[False,  False,  False]], dtype=bool)\\n17. How to find the maximum and minimum value of a given flattened array?\\n>>> import numpy as np\\n>>> a = np.arange(4).reshape((2,2))\\n>>> max_val = np.amax(a)\\n>>> min_val = np.amin(a)\\n18. Write a NumPy program to calculate the difference between the maximum and the \\nminimum values of a given array along the second axis.\\n>>> import numpy as np\\n>>> arr = np.arange(16).reshape((4, 7))\\n>>> res = np.ptp(arr, 1)\\n19. Find median of a numpy flattened array \\n>>> import numpy as np\\n>>> arr = np.arange(16).reshape((4, 5))\\n>>> res =  np.median(arr)', metadata={'source': 'ML questions dump.pdf', 'page': 166}),\n",
       " Document(page_content='Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X20. Write a NumPy program to compute the mean, standard deviation, and variance of \\na given array along the second axis\\nimport numpy as np\\n>>> import numpy as np\\n>>> x = np.arange(16)\\n>>> mean = np.mean(x)\\n>>> std = np.std(x)\\n>>> var= np.var(x)\\n21. Calculate covariance matrix between two numpy arrays\\n>>> import numpy as np\\n>>> x = np.array([2, 1, 0])\\n>>> y = np.array([2, 3, 3])\\n>>> cov_arr = np.cov(x, y)\\n22.Compute Compute pearson product -moment correlation coefficients of two \\ngiven numpy arrays\\n>>> import numpy as np\\n>>> x = np.array([0, 1, 3])\\n>>> y = np.array([2, 4, 5])\\n>>> cross_corr = np.corrcoef(x, y)\\n23. Develop a numpy program to compute the histogram of nums against the bins\\n>>> import numpy as np\\n>>> nums = np.array([0.5, 0.7, 1.0, 1.2, 1.3, 2.1])\\n>>> bins = np.array([0, 1, 2, 3])\\n>>> np.histogram(nums, bins)\\n24.Get the powers of an array values element -wise\\n>>> import numpy as np\\n>>> x = np.arange(7)\\n>>> np.power(x, 3)\\n25. Write a NumPy program to get true division of the element -wise array inputs\\n>>> import numpy as np\\n>>> x = np.arange(10)\\n>>> np.true_divide(x, 3)', metadata={'source': 'ML questions dump.pdf', 'page': 167}),\n",
       " Document(page_content=\"Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463XPandas\\n26.What is a series in pandas?\\nA Series is defined as a one -dimensional array that is capable of storing various data \\ntypes. The row labels of the series are called the index. By using a 'series' method, we \\ncan easily convert the list, tuple, and dictionary into series. A Series cannot contain \\nmultiple columns.\\n27.What features make Pandas such a reliable option to store tabular data?\\nMemory Efficient, Data Alignment, Reshaping, Merge and join and Time Series.\\n28.What is reindexing in pandas?\\nReindexing is used to conform DataFrame to a new index with optional filling logic. It \\nplaces NA/NaN in that location where the values are not present in the previous index. It \\nreturns a new object unless the new index is produced as equivalent to the current one, \\nand the value of copy becomes False. It is used to change the index of the rows and \\ncolumns of the DataFrame.\\n29.How will you create a series from dict in Pandas?\\nA Series is defined as a one -dimensional array that is capable of storing various data \\ntypes.\\n>>> import pandas as pd    \\n>>> info = {'x' : 0., 'y' : 1., 'z' : 2.}    \\n>>> a = pd.Series(info)\\n30.How can we create a copy of the series in Pandas?\\nUse pandas.Series.copy method\\n>>> import pandas as pd\\n>>> pd.Series.copy(deep=True)\\n31.What is groupby in Pandas?\\nGroupBy is used to split the data into groups. It groups the data based on some criteria. \\nGrouping also provides a mapping of labels to the group names. It has a lot of variations \\nthat can be defined with the parameters and makes the task of splitting the data quick and \\neasy.\\n32.What is vectorization in Pandas?\", metadata={'source': 'ML questions dump.pdf', 'page': 168}),\n",
       " Document(page_content=\"Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463XVectorization is the process of running operations on the entire array. This is done to \\nreduce the amount of iteration performed by the functions. Pandas have a number of \\nvectorized functions like aggregations, and string functions that are optimized to operate \\nspecifically on series and DataFrames. So it is preferred to use the vectorized pandas \\nfunctions to execute the operations quickly.\\n33.Mention the different types of Data Structures in Pandas\\nPandas provide two data structures, which are supported by the pandas library, Series, \\nand DataFrames. Both of these data structures are built on top of the NumPy.\\n34.What Is Time Series In pandas\\nA time series is an ordered sequence of data which basically represents how some \\nquantity changes over time. pandas contains extensive capabilities and features for \\nworking with time series data for all domains.\\n35.How to convert pandas dataframe to numpy array?\\nThe function to_numpy() is used to convert the DataFrame to a NumPy array.\\nDataFrame.to_numpy(self, dtype=None, copy=False)\\nThe dtype parameter defines the data type to pass to the array and the copy ensures the \\nreturned value is not a view on another array.\\n36.Write a Pandas program to get the first 5 rows of a given DataFrame\\n>>> import pandas as pd\\n>>> exam_data  = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', \\n'Matthew', 'Laura', 'Kevin', 'Jonas'],}\\nlabels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\\n>>> df = pd.DataFrame(exam_data , index=labels)\\n>>> df.iloc[:5]\\n37.Develop a Pandas program to create and display a one -dimensional array -\\nlike object containing an array of data.\\n>>> import pandas as pd\\n>>> pd.Series([2, 4, 6, 8, 10])\\n38.Write a Python program to convert a Panda module Series to Python list and \\nit's type .\\n>>> import pandas as pd\\n>>> ds = pd.Series([2, 4, 6, 8, 10])\", metadata={'source': 'ML questions dump.pdf', 'page': 169}),\n",
       " Document(page_content=\"Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X>>> type(ds)\\n>>> ds.tolist()\\n>>> type(ds.tolist())\\n39.Develop a Pandas program to add, subtract, multiple and divide two Pandas \\nSeries.\\n>>> import pandas as pd\\n>>> ds1 = pd.Series([2, 4, 6, 8, 10])\\n>>> ds2 = pd.Series([1, 3, 5, 7, 9])\\n>>> sum = ds1 + ds2\\n>>> sub = ds1 -ds2\\n>>> mul = ds1 * ds2\\n>>> div = ds1 / ds2\\n40.Develop a Pandas program to compare the elements of the two Pandas \\nSeries.\\n>>> import pandas as pd\\n>>> ds1 = pd.Series([2, 4, 6, 8, 10])\\n>>> ds2 = pd.Series([1, 3, 5, 7, 10])\\n>>> ds1 == ds2\\n>>> ds1 > ds2\\n>>> ds1 < ds2\\n41.Develop a Pandas program to change the data type of given a column or a \\nSeries.\\n>>> import pandas as pd\\n>>> s1 = pd.Series(['100', '200', 'python', '300.12', '400'])\\n>>> s2 = pd.to_numeric(s1, errors='coerce')\\n>>> s2\\n42.Write a Pandas program to convert Series of lists to one Series\\n>>> import pandas as pd\\n>>> s = pd.Series([ ['Red', 'Black'], ['Red', 'Green', 'White'] , ['Yellow']])\\n>>> s = s.apply(pd.Series).stack().reset_index(drop=True)\\n43.Write a Pandas program to create a subset of a given series based on value \\nand condition\\n>>> import pandas as pd\\n>>> s = pd.Series([0, 1,2,3,4,5,6,7,8,9,10])\\n>>> n = 6\", metadata={'source': 'ML questions dump.pdf', 'page': 170}),\n",
       " Document(page_content=\"Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X>>> new_s = s[s < n]\\n>>> new_s\\n44.Develop a Pandas code to alter the order of index in a given series\\n>>> import pandas as pd\\n>>> s = pd.Series(data = [1,2,3,4,5], index = ['A', 'B', 'C','D','E'])\\n>>> s.reindex(index = ['B','A','C','D','E'])\\n45.Write a Pandas code to get the items of a given series not present in another \\ngiven series.\\n>>> import pandas as pd\\n>>> sr1 = pd.Series([1, 2, 3, 4, 5])\\n>>> sr2 = pd.Series([2, 4, 6, 8, 10])\\n>>> result = sr1[~sr1.isin(sr2)]\\n>>> result\\n46.What is the difference between the two data series df[‘Name’] and df.loc[:, \\n‘Name’]?\\n>>> First one is a view of the original dataframe and second one is a copy of the original \\ndataframe.\\n47.Write a Pandas program to display the most frequent value in a given series \\nand replace everything else as “replaced” in the seri es.\\n>>> import pandas as pd\\n>>> import numpy as np\\n>>> np.random.RandomState(100)\\n>>> num_series = pd.Series(np.random.randint(1, 5, [15]))\\n>>> result = num_series[~num_series.isin(num_series.value_counts().index[:1])] = \\n'replaced'\\n48.Write a Pandas program to find the positions of numbers that are multiples \\nof 5 of a given series.\\n>>> import pandas as pd\\n>>> import numpy as np\\n>>> num_series = pd.Series(np.random.randint(1, 10, 9))\\n>>> result = np.argwhere(num_series % 5==0)\\n49.How will you add a column to a pandas DataFrame?\", metadata={'source': 'ML questions dump.pdf', 'page': 171}),\n",
       " Document(page_content=\"Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X# importing the pandas library    \\n>>> import pandas as pd      \\n>>> info = {'one' : pd.Series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e']),    \\n'two' : pd.Series([1, 2, 3, 4, 5, 6], index=['a', 'b', 'c', 'd', 'e', 'f'])}    \\n>>> info = pd.DataFrame(info)\\n# Add a new column to an existing DataFrame object\\n>>> info['three']=pd.Series([20,40,60],index=['a','b','c'])\\n50.How to iterate over a Pandas DataFrame?\\nYou can iterate over the rows of the DataFrame by using for loop in combination with an \\niterrows() call on the DataFrame.\\nPython Language\\n51.What type of language is python? Programming or scripting?\\nPython is capable of scripting, but in general sense, it is considered as a general -purpose \\nprogramming language.\\n52.Is python case sensitive?\\nYes, python is a case sensitive language.\\n53.What is a lambda function in python?\\nAn anonymous function is known as a lambda function. This function can have any \\nnumber of parameters but can have just one statement.\\n54.What is the difference between xrange and xrange in python?\\nxrange and range are the exact same in terms of functionality.The only difference is that \\nrange returns a Python list object and x range returns an xrange object. \\n55.What are docstrings in python?\\nDocstrings are not actually comments, but they are documentation strings . These \\ndocstrings are within triple quotes. They are not assigned to any variable and therefore, \\nat times, serve the purpose of comments as well.\\n56.Whenever Python exits, why isn’t all the memory deallocated?\\nWhenever Python exits, especially those Python modules which are having circular \\nreferences to other objects or the objects that are referenced from the global namespaces \", metadata={'source': 'ML questions dump.pdf', 'page': 172}),\n",
       " Document(page_content='Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463Xare not always de -allocated or freed. It is impossible to de -allocate those portions of \\nmemory that are reserved by the C library. On exit, because of having its own efficient \\nclean up mechanism, Python would try to de -allocate/destroy every other object.\\n57.What does this mean: *args, **kwargs? And why would we use it?\\nWe use *args when we aren’t sure how many arguments are going to be passed to a \\nfunction, or if we want to pass a stored list or tuple of arguments to a function. **kwargs is \\nused when we don’t know how many keyword arguments will be passed to a function, or \\nit can be used to pass the values of a dictionary as keyword arguments. \\n58.What is the difference between deep and shallow copy?\\nShallow copy is used when a new instance type gets created and it keeps the values that \\nare copied in the new instance. Shallow copy is used to copy the reference pointers just \\nlike it copies the values.\\nDeep copy is used to store the values that are already copied. Deep copy doesn’t copy \\nthe reference pointers to the objects. It makes the reference to an object and the new \\nobject that is pointed by some other object gets stored.\\n59.Define encapsulation in Python?\\nEncapsulation means binding the code and the data together. A Python class in an \\nexample of encapsulation.\\n60.Does python make use of access specifiers?\\nPython does not deprive access to an instance variable or function. Python lays down the \\nconcept of prefixing the name of the variable, function or method with a single or double \\nunderscore to imitate the behavior of protected and private access specifiers. \\n61.What are the generators in Python?\\nGenerators are a way of implementing iterators. A generator function is a normal function \\nexcept that it contains yield expression in the function definition making it a generator \\nfunction.\\n62.How will you remove the duplicate elements from the given list?\\nThe set is another type available in Python. It doesn’t allow copies and provides some \\ngood functions to perform set operations like union, difference etc.\\n>>> list(set(a))\\n63.Does Python allow arguments Pass by Value or Pass by Reference?', metadata={'source': 'ML questions dump.pdf', 'page': 173}),\n",
       " Document(page_content='Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463XNeither the arguments are Pass by Value nor does Python supports Pass by reference. \\nInstead, they are Pass by assignment. The parameter which you pass is originally a \\nreference to the object not the reference to a fixed memory location. But the reference is \\npassed by value. Additionally, some data types like strings and tuples are immutable \\nwhereas others are mutable.\\n64.What is slicing in Python?\\nSlicing in Python is a mechanism to select a range of items from Sequence types like \\nstrings, list, tuple, etc.\\n65.Why is the “pass” keyword used in Python?\\nThe “pass” keyword is a no -operation statement in Python. It signals that no action is \\nrequired. It works as a placeholder in compound statements which are intentionally left \\nblank.\\n66.What is PEP8 and why is it important?\\nPEP stands for Python Enhancement Proposal. A PEP is an official design document \\nproviding information to the Python Community, or describing a new feature for Python or \\nits processes. PEP 8 is especially important since it documents the style guidelines for \\nPython Code. Apparently contributing in the Python open -source community requires you \\nto follow these style guidelines sincerely and strictly.\\n67.What are decorators in Python?\\nDecorators in Python are essentially functions that addfunctionality to an existing function \\nin Python without changing the structure of the function itself. They are represented by the \\n@decorator_name in Python and are called in bottom -up fashion\\n68.What is the key difference between lists and tuples in python ?\\nThe key difference between the two is that while lists are mutable, tuples on the other \\nhand are immutable objects.\\n69.What is self in Python?\\nSelf is a keyword in Python used to define an instance or an object of a class. In Python, \\nit is explicitly used as the first parameter, unlike in Java where it is optional. It helps in \\ndistinguishing between the methods and attributes of a class from its local variables.\\n70.What is PYTHONPATH in Python?\\nPYTHONPATH is an environment variable which you can set to add additional directories \\nwhere Python will look for modules and packages. This is especially useful in maintaining \\nPython libraries that you do not wish to install in the global default location.\\n71.What is the difference between .py and .pyc files?', metadata={'source': 'ML questions dump.pdf', 'page': 174}),\n",
       " Document(page_content='Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X.py files contain the source code of a program. Whereas, .pyc file contains the bytecode \\nof your program. We get bytecode after compilation of .py file (source code). .pyc files are \\nnot created for all the files that you run. It is only created for the files that you import. \\n72.Explain how you can access a module written in Python from C?\\nYou can access a module written in Python from C by following method,\\nModule = =PyImport_ImportModule(\"<modulename>\");\\n73.What is namespace in Python?\\nIn Python, every name introduced has a place where it lives and can be hooked for. This \\nis known as namespace. It is like a box where a variable name is mapped to the object \\nplaced. Whenever the variable is searched out, this box will be searched, to get the \\ncorresponding object.\\n74.What is pickling and unpickling?\\nPickle module accepts any Python object and converts it into a string representation and \\ndumps it into a file by using the dump function, this process is called pickling. While the \\nprocess of retrieving original Python objects from the stored string representation is called \\nunpickling.\\n75.How is Python interpreted?\\nPython language is an interpreted language. The Python program runs directly from the \\nsource code. It converts the source code that is written by the programmer into an \\nintermediate language, which is again translated into machine language that has to be \\nexecuted.\\nJupyter Notebook\\n76.What is the main use of a Jupyter notebook?\\nJupyter Notebook is an open -source web application that allows us to create and share \\ncodes and documents. It provides an environment, where you can document your code, \\nrun it, look at the outcome, visualize data and see the results without leaving the \\nenvironment.\\n77.How do I increase the cell width of the Jupyter/ipython notebook in my \\nbrowser?\\n>>> from IPython.core.display import display, HTML\\n>>> display(HTML(\"<style>.container { width:100% !important; }</style>\"))', metadata={'source': 'ML questions dump.pdf', 'page': 175}),\n",
       " Document(page_content=\"Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X78.How do I convert an IPython Notebook into a Python file via command line?\\n>>> jupyter nbconvert --to script [YOUR_NOTEBOOK].ipynb\\n79.How to measure execution time in a jupyter notebook?\\n>>> %%time is inbuilt magic command\\n80.How to run a jupyter notebook from the command line?\\n>>> jupyter nbconvert --to python nb.ipynb\\n81.How to make inline plots larger in jupyter notebooks?\\nUse figure size.\\n>>> fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\\n82.How to display multiple images in a jupyter notebook?\\n>>>for ima in images:\\n>>>plt.figure()\\n>>>plt.imshow(ima)\\n83.Why is the Jupyter notebook interactive code and data exploration friendly?\\nThe ipywidgets package provides many common user interface controls for exploring code \\nand data interactively.\\n84.What is the default formatting option in jupyter notebook?\\nDefault formatting option is markdown\\n85.What are kernel wrappers in jupyter?\\nJupyter brings a lightweight interface for kernel languages that can be wrapped in Python. \\nWrapper kernels can implement optional methods, notably for code completion and code \\ninspection.\\n86.What are the advantages of custom magic commands?\\nCreate IPython extensions with custom magic commands to make interactive computing \\neven easier. Many third -party extensions and magic commands exist, for example, the \\n%%cython magic that allows one to write Cython code directly in a notebook.\\n87.Is the jupyter architecture language dependent?\\nNo. It is language independent.\", metadata={'source': 'ML questions dump.pdf', 'page': 176}),\n",
       " Document(page_content=\"Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X88.Which tools allow jupyter notebooks to easily convert to pdf and html?\\nNbconvert converts it to pdf and html while Nbviewer renders the notebooks on the web \\nplatforms.\\n89.What is a major disadvantage of a Jupyter notebook?\\nIt is very hard to run long asynchronous tasks. Less Secure.\\n90.In which domain is the jupyter notebook widely used?\\nIt is mainly used for data analysis and machine learning related tasks.\\n91.What are alternatives to jupyter notebook?\\nPyCharm interact, VS Code  Python Interactive etc.\\n92.Where can you make configuration changes to the jupyter notebook?\\nIn the config file located at ~/.ipython/profile_default/ipython_config.py\\n93.Which magic command is used to run python code from jupyter notebook?\\n%run can execute python code from .py files\\n94.How to pass variables across the notebooks?\\nThe %store command lets you pass variables between two different notebooks.\\n>>> data = 'this is the string I want to pass to different notebook'\\n>>> %store data\\n# Stored 'data' (str)\\n# In new notebook\\n>>> %store -r data\\n>>> print(data)\\n95.Export the contents of a cell/Show the contents of an external script\\nUsing the %%writefile magic saves the contents of that cell to an external file. %pycat \\ndoes the opposite and shows you (in a popup) the syntax highlighted contents of an \\nexternal file.\\n96.What inbuilt tool we use for debugging python code in a jupyter notebook?\\nJupyter has its own interface for The Python Debugger (pdb). This makes it possible to go \\ninside the function and investigate what happens there.\", metadata={'source': 'ML questions dump.pdf', 'page': 177}),\n",
       " Document(page_content=\"Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X97.How to make high resolution plots in a jupyter notebook?\\n>>> %config InlineBackend.figure_format ='retina'\\n98.How can one use latex in a jupyter notebook?\\nWhen you write LaTeXin a Markdown cell, it will be rendered as a formula using MathJax.\\n99. What is a jupyter lab?\\nIt is a next generation user interface for conventional jupyter notebooks. Users can drag \\nand drop cells, arrange code workspace and live previews. It’s still in the early stage of \\ndevelopment.\\n100. What is the biggest limitation for a Jupyter notebook?\\nCode versioning, management and debugging is not scalable in current jupyter notebook.\", metadata={'source': 'ML questions dump.pdf', 'page': 178}),\n",
       " Document(page_content='Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463XReferences\\n[1] https://www.edureka.co\\n[2] https://www.kausalvikash.in\\n[3] https://www.wisdomjobs.com\\n[4] https://blog.edugrad.com\\n[5]https://stackoverflow.com\\n[6]http://www.ezdev.org\\n[7]https://www.techbeamers.com\\n[8]https://www.w3resource.com\\n[9]https://www.javatpoint.com\\n[10]https://analyticsindiamag.com\\n[11]https://www.onlineinterviewquestions.com\\n[12]https://www.geeksforgeeks.org\\n[13]https://www.springpeople.com\\n[14]https://atraininghub.com\\n[15]https://www.interviewcake.com\\n[16]https://www.techbeamers.com\\n[17]https://www.tutorialspoint.com\\n[18]https://programmingwithmosh.com\\n[19]https://www.interviewbit.com\\n[20]https://www.guru99.com\\n[21]https://hub.packtpub.com\\n[22]https://analyticsindiamag.com\\n[23]https://www.dataquest.io\\n[24]https://www.infoworld.com', metadata={'source': 'ML questions dump.pdf', 'page': 179}),\n",
       " Document(page_content='Top 100 Machine Learning Questions & Answers  \\nSteve Nouri \\n \\n \\n \\nQ1 Explain the difference between supervised and unsupervised machine         \\nlearning?  \\nIn supervised machine learning algorithms, we have to provide labeled data, for example,             \\nprediction of stock market prices, whereas in unsupervised we need not have labeled data, for                \\nexample, classification of emails into spam and non-spam. \\nQ2 What are the parametric models? Give an example. \\nParametric models are those with a finite number of parameters. To predict new data, you only                 \\nneed to know the parameters of the model. Examples include linear regression, logistic             \\nregression, and linear SVMs. \\nNon-parametric models are those with an unbounded number of parameters, allowing for more              \\nflexibility. To predict new data, you need to know the parameters of the model and the state of                   \\nthe data that has been observed. Examples include decision trees, k-nearest neighbors, and             \\ntopic models using latent Dirichlet analysis. \\nQ3 What is the difference between classification\\n  \\nand regression? \\nClassification is used to produce discrete results, classification is used to classify data into some                \\nspecific categories. For example, classifying emails into spam and non-spam categories. \\nWhereas, We use regression analysis when we are dealing with continuous data, for example               \\npredicting stock prices at a certain point in time.  \\n \\nQ4 What Is Overfitting, and How Can You Avoid It?  \\nOverfitting is a situation that occurs when a model learns the training set too well, taking up                  \\nrandom fluctuations in the training data as concepts. These impact the model’s ability to              \\ngeneralize and don’t apply to new data.  \\nWhen a model is given the training data, it shows 100 percent accuracy—technically a slight               \\nloss. But, when we use the test data, there may be an error and low efficiency. This condition is                   \\nknown as overfitting. \\nThere are multiple ways of avoiding overfitting, such as: \\n● Regularization. It involves a cost term for the features involved with the objective function \\n● Making a simple model. With lesser variables and parameters, the variance can be              \\nreduced  \\n● Cross-validation methods like k-folds can also be used \\n● If some model parameters are likely to cause overfitting, techniques for regularization            \\nlike LASSO can be used that penalize these parameters \\n \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 180}),\n",
       " Document(page_content=\" \\n \\n \\nQ5 What is meant by ‘Training set’ and ‘Test Set’? \\nWe split the given data set into two different sections namely,’Training set’ and ‘Test Set’. \\n‘Training set’ is the portion of the dataset used to train the model. \\n‘Testing set’ is the portion of the dataset used to test the trained model.  \\nQ6 How Do You Handle Missing or Corrupted Data in a Dataset? \\nOne of the easiest ways to handle missing or corrupted data is to drop those rows or columns or                   \\nreplace them entirely with some other value. \\nThere are two useful methods in Pandas: \\n● IsNull() and dropna() will help to find the columns/rows with missing data and drop them \\n● Fillna() will replace the wrong values with a placeholder value \\n \\nQ7 Explain Ensemble learning.  \\nIn ensemble learning, many base models like classifiers and regressors are generated and             \\ncombined together so that they give better results. It is used when we build component                \\nclassifiers that are accurate and independent. There are sequential as well as parallel ensemble               \\nmethods. \\nQ8 Explain the Bias-Variance Tradeoff. \\nPredictive models have a tradeoff between bias (how well the model fits the data) and variance                 \\n(how much the model changes based on changes in the inputs). \\nSimpler models are stable (low variance) but they don't get close to the truth (high bias). \\nMore complex models are more prone to overfitting (high variance) but they are expressive              \\nenough to get close to the truth (low bias). The best model for a given problem usually lies                  \\nsomewhere in the middle. \\nQ9 What is the difference between stochastic gradient descent (SGD) and           \\ngradient descent (GD)? \\nBoth algorithms are methods for finding a set of parameters that minimize a loss function by                \\nevaluating parameters against data and then making adjustments. \\nIn standard gradient descent, you'll evaluate all training samples for each set of parameters.              \\nThis is akin to taking big, slow steps toward the solution. \\nIn stochastic gradient descent, you'll evaluate only 1 training sample for the set of parameters               \\nbefore updating them. This is akin to taking small, quick steps toward the solution. \\n \\nQ10 How Can You Choose a Classifier Based on a Training Set Data Size? \\nWhen the training set is small, a model that has a right bias and low variance seems to work                   \\nbetter because they are less likely to overfit.  \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  \", metadata={'source': 'ML questions dump.pdf', 'page': 181}),\n",
       " Document(page_content=\"For example, Naive Bayes works best when the training set is large. Models with low bias and                 \\nhigh variance tend to perform better as they work fine with complex relationships. \\n \\nQ11 What are 3 data preprocessing techniques to handle outliers? \\n1. Winsorize (cap at threshold). \\n2. Transform to reduce skew (using Box-Cox or similar). \\n3. Remove outliers if you're certain they are anomalies or measurement errors. \\n \\nQ12 How much data should you allocate for your training, validation, and test             \\nsets? \\nYou have to find a balance, and there's no right answer for every problem. \\nIf your test set is too small, you'll have an unreliable estimation of model performance               \\n(performance statistic will have high variance). If your training set is too small, your actual model                 \\nparameters will have a high variance. \\nA good rule of thumb is to use an 80/20 train/test split. Then, your train set can be further split                    \\ninto train/validation or into partitions for cross-validation. \\n \\nQ13 What Is a False Positive and False Negative and How Are They Significant? \\nFalse positives are those cases which wrongly get classified as True but are False.  \\nFalse negatives are those cases which wrongly get classified as False but are True. \\nIn the term ‘False Positive,’ the word ‘Positive’ refers to the ‘Yes’ row of the predicted value in                  \\nthe confusion matrix. The complete term indicates that the system has predicted it as a positive,                \\nbut the actual value is negative. \\n \\nQ14 Explain the difference between L1 and L2 regularization.  \\nL2 regularization tends to spread error among all the terms, while L1 is more binary/sparse, with                \\nmany variables either being assigned a 1 or 0 in weighting. L1 corresponds to setting a                \\nLaplacean prior to the terms, while L2 corresponds to a Gaussian prior. \\n \\nQ15 What’s a Fourier transform? \\nA Fourier transform is a generic method to decompose generic functions into a superposition of               \\nsymmetric functions. Or as this more intuitive tutorial puts it, given a smoothie, it’s how we find                  \\nthe recipe. The Fourier transform finds the set of cycle speeds, amplitudes, and phases to               \\nmatch any time signal. A Fourier transform converts a signal from time to frequency domain —                \\nit’s a very common way to extract features from audio signals or other time series such as                 \\nsensor data. \\n \\nQ16 What is deep learning, and how does it contrast with other machine learning              \\nalgorithms? \\nDeep learning is a subset of machine learning that is concerned with neural networks: how to                \\nuse backpropagation and certain principles from neuroscience to more accurately model large            \\nsets of unlabelled or semi-structured data. In that sense, deep learning represents an             \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  \", metadata={'source': 'ML questions dump.pdf', 'page': 182}),\n",
       " Document(page_content='unsupervised learning algorithm that learns representations of data through the use of neural              \\nnets. \\n \\nQ17 What’s the difference between a generative and discriminative model? \\nA generative model will learn categories of data while a discriminative model will simply learn               \\nthe distinction between different categories of data. Discriminative models will generally           \\noutperform generative models on classification tasks. \\nQ18 What Are the Applications of Supervised Machine Learning in Modern           \\nBusinesses? \\nApplications of supervised machine learning include: \\n●Email Spam Detection  \\nHere we train the model using historical data that consists of emails categorized as               \\nspam or not spam. This labeled information is fed as input to the model. \\n●Healthcare Diagnosis  \\nBy providing images regarding a disease, a model can be trained to detect if a person is                  \\nsuffering from the disease or not. \\n●Sentiment Analysis  \\nThis refers to the process of using algorithms to mine documents and determine              \\nwhether they’re positive, neutral, or negative in sentiment.  \\n●Fraud Detection  \\nTraining the model to identify suspicious patterns, we can detect instances of possible              \\nfraud. \\n \\nQ19 What Is Semi-supervised Machine Learning?  \\nSupervised learning uses data that is completely labeled, whereas unsupervised learning uses             \\nno training data. \\nIn the case of semi-supervised learning, the training data contains a small amount of labeled               \\ndata and a large amount of unlabeled data. \\nQ20. What Are Unsupervised Machine Learning Techniques?  \\nThere are two techniques used in unsupervised learning: clustering and association. \\nClustering \\n● Clustering problems involve data to be divided into subsets. These subsets, also called              \\nclusters, contain data that are similar to each other. Different clusters reveal different              \\ndetails about the objects, unlike classification or regression. \\nAssociation \\n● In an association problem, we identify patterns of associations between different           \\nvariables or items. \\n● For example, an eCommerce website can suggest other items for you to buy, based on               \\nthe prior purchases that you have made, spending habits, items in your wishlist, other              \\ncustomers’ purchase habits, and so on. \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 183}),\n",
       " Document(page_content=' \\n \\n \\nQ21 What Is ‘naive’ in the Naive Bayes Classifier? \\nThe classifier is called ‘naive’ because it makes assumptions that may or may not turn out to be                  \\ncorrect.  \\nThe algorithm assumes that the presence of one feature of a class is not related to the presence                  \\nof any other feature (absolute independence of features), given the class variable. \\nFor instance, a fruit may be considered to be a cherry if it is red in color and round in shape,                     \\nregardless of other features. This assumption may or may not be right (as an apple also                 \\nmatches the description). \\nQ22 Explain Latent Dirichlet Allocation (LDA). \\nLatent Dirichlet Allocation (LDA) is a common method of topic modeling, or classifying             \\ndocuments by subject matter. \\nLDA is a generative model that represents documents as a mixture of topics that each have                \\ntheir own probability distribution of possible words. \\nThe \"Dirichlet\" distribution is simply a distribution of distributions. In LDA, documents are             \\ndistributions of topics that are distributions of words. \\nQ23 Explain Principle Component Analysis (PCA). \\nPCA is a method for transforming features in a dataset by combining them into uncorrelated               \\nlinear combinations. \\nThese new features, or principal components, sequentially maximize the variance represented           \\n(i.e. the first principal component has the most variance, the second principal component has              \\nthe second most, and so on). \\nAs a result, PCA is useful for dimensionality reduction because you can set an arbitrary               \\nvariance cutoff. \\n \\nQ24 What’s the F1 score? How would you use it? \\nThe F1 score is a measure of a model’s performance. It is a weighted average of the precision                  \\nand recall of a model, with results tending to 1 being the best, and those tending to 0 being the                    \\nworst. You would use it in classification tests where true negatives don’t matter much. \\n \\n \\n \\n \\n \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 184}),\n",
       " Document(page_content=' \\n \\nQ25 When should you use classification over regression? \\nClassification produces discrete values and dataset to strict categories, while regression gives             \\nyou continuous results that allow you to better distinguish differences between individual points.             \\nYou would use classification over regression if you wanted your results to reflect the              \\nbelongingness of data points in your dataset to certain explicit categories (ex: If you wanted to                 \\nknow whether a name was male or female rather than just how correlated they were with male                 \\nand female names.) \\n \\nQ26 How do you ensure you’re not overfitting with a model? \\nThis is a simple restatement of a fundamental problem in machine learning: the possibility of               \\noverfitting training data and carrying the noise of that data through to the test set, thereby                 \\nproviding inaccurate generalizations. \\nThere are three main methods to avoid overfitting: \\n1- Keep the model simpler: reduce variance by taking into account fewer variables and              \\nparameters, thereby removing some of the noise in the training data. \\n2- Use cross-validation techniques such as k-folds cross-validation. \\n3- Use regularization techniques such as LASSO that penalize certain model parameters if             \\nthey’re likely to cause overfitting. \\n \\nQ27 How Will You Know Which Machine Learning Algorithm to Choose for Your             \\nClassification Problem? \\nWhile there is no fixed rule to choose an algorithm for a classification problem, you can follow                 \\nthese guidelines: \\n● If accuracy is a concern, test different algorithms and cross-validate them \\n● If the training dataset is small, use models that have low variance and high bias \\n● If the training dataset is large, use models that have high variance and little bias \\n \\nQ28 How Do You Design an Email Spam Filter? \\nBuilding a spam filter involves the following process: \\n● The email spam filter will be fed with thousands of emails  \\n● Each of these emails already has a label: ‘spam’ or ‘not spam.’ \\n● The supervised machine learning algorithm will then determine which type of emails are             \\nbeing marked as spam based on spam words like the lottery, free offer, no money, full                \\nrefund, etc. \\n● The next time an email is about to hit your inbox, the spam filter will use statistical                 \\nanalysis and algorithms like Decision Trees and SVM to determine how likely the email              \\nis spam \\n● If the likelihood is high, it will label it as spam, and the email won’t hit your inbox \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 185}),\n",
       " Document(page_content=\"● Based on the accuracy of each model, we will use the algorithm with the highest                \\naccuracy after testing all the models \\nQ29 What evaluation approaches would you work to gauge the effectiveness of a             \\nmachine learning model? \\nYou would first split the dataset into training and test sets, or perhaps use cross-validation               \\ntechniques to further segment the dataset into composite sets of training and test sets within the                 \\ndata. You should then implement a choice selection of performance metrics: here is a fairly               \\ncomprehensive list. You could use measures such as the F1 score, the accuracy, and the                \\nconfusion matrix. What’s important here is to demonstrate that you understand the nuances of               \\nhow a model is measured and how to choose the right performance measures for the right                \\nsituations. \\n \\nQ30 How would you implement a recommendation system for our company’s           \\nusers? \\nA lot of machine learning interview questions of this type will involve the implementation of               \\nmachine learning models to a company’s problems. You’ll have to research the company and its                \\nindustry in-depth, especially the revenue drivers the company has, and the types of users the               \\ncompany takes on in the context of the industry it’s in. \\n \\nQ31 Explain bagging. \\nBagging, or Bootstrap Aggregating, is an ensemble method in which the dataset is first divided                \\ninto multiple subsets through resampling. \\nThen, each subset is used to train a model, and the final predictions are made through voting or                  \\naveraging the component models. \\nBagging is performed in parallel. \\nQ32 What is the ROC Curve and what is AUC (a.k.a. AUROC)? \\nThe ROC (receiver operating characteristic) the performance plot for binary classifiers of True             \\nPositive Rate (y-axis) vs. False Positive Rate (x- \\naxis). \\nAUC is the area under the ROC curve, and it's a common performance metric for evaluating                \\nbinary classification models. \\nIt's equivalent to the expected probability that a uniformly drawn random positive is ranked              \\nbefore a uniformly drawn random negative. \\n \\n \\n \\n \\n \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  \", metadata={'source': 'ML questions dump.pdf', 'page': 186}),\n",
       " Document(page_content=' \\nQ33 Why is Area Under ROC Curve (AUROC) better than raw accuracy as an              \\nout-of-sample evaluation metric? \\nAUROC is robust to class imbalance, unlike raw accuracy. \\nFor example, if you want to detect a type of cancer that\\'s prevalent in only 1% of the population,                   \\nyou can build a model that achieves 99% accuracy by simply classifying everyone has              \\ncancer-free. \\nQ34 What are the advantages and disadvantages of neural networks? \\nAdvantages\\n : Neural networks (specifically deep NNs) have led to performance breakthroughs           \\nfor unstructured datasets such as images, audio, and video. Their incredible flexibility allows             \\nthem to learn patterns that no other ML algorithm can learn. \\nDisadvantages\\n : However, they require a large amount of training data to converge. It\\'s also              \\ndifficult to pick the right architecture, and the internal \"hidden\" layers are incomprehensible. \\nQ35 Define Precision and Recall. \\nPrecision \\n● Precision is the ratio of several events you can correctly recall to the total number of                 \\nevents you recall (mix of correct and wrong recalls). \\n● Precision = (True Positive) / (True Positive + False Positive) \\nRecall \\n● A recall is the ratio of a number of events you can recall the number of total events. \\n● Recall = (True Positive) / (True Positive + False Negative) \\n \\nQ36 What Is Decision Tree Classification? \\nA decision tree builds classification (or regression) models as a tree structure, with datasets              \\nbroken up into ever-smaller subsets while developing the decision tree, literally in a tree-like              \\nway with branches and nodes. Decision trees can handle both categorical and numerical data.  \\n \\nQ37  What Is Pruning in Decision Trees, and How Is It Done? \\nPruning is a technique in machine learning that reduces the size of decision trees. It reduces the                 \\ncomplexity of the final classifier, and hence improves predictive accuracy by the reduction of               \\noverfitting.  \\nPruning can occur in: \\n● Top-down fashion. It will traverse nodes and trim subtrees starting at the root \\n● Bottom-up fashion. It will begin at the leaf nodes \\nThere is a popular pruning algorithm called reduced error pruning, in which: \\n● Starting at the leaves, each node is replaced with its most popular class \\n● If the prediction accuracy is not affected, the change is kept \\n● There is an advantage of simplicity and speed \\n \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 187}),\n",
       " Document(page_content=' \\n \\nQ38 What Is a Recommendation System? \\nAnyone who has used Spotify or shopped at Amazon will recognize a recommendation system:              \\nIt’s an information filtering system that predicts what a user might want to hear or see based on                  \\nchoice patterns provided by the user. \\nQ39 What Is Kernel SVM? \\nKernel SVM is the abbreviated version of the kernel support vector machine. Kernel methods              \\nare a class of algorithms for pattern analysis, and the most common one is the kernel SVM. \\n \\nQ40 What Are Some Methods of Reducing Dimensionality? \\nYou can reduce dimensionality by combining features with feature engineering, removing           \\ncollinear features, or using algorithmic dimensionality reduction. \\nNow that you have gone through these machine learning interview questions, you must have              \\ngot an idea of your strengths and weaknesses in this domain. \\nQ41 What Are the Three Stages of Building a Model in Machine Learning? \\nThe three stages of building a machine learning model are: \\n●Model Building Choose a suitable algorithm for the model and train it according to the                \\nrequirement  \\n●Model Testing\\n  Check the accuracy of the model through the test data  \\n●Applying the Mode\\n Make the required changes after testing and use the final model for                \\nreal-time projects. Here, it’s important to remember that once in a while, the model               \\nneeds to be checked to make sure it’s working correctly. It should be modified to make                \\nsure that it is up-to-date. \\n \\nQ42 How is KNN different from k-means clustering? \\nK-Nearest Neighbors is a supervised classification algorithm, while k-means clustering is an             \\nunsupervised clustering algorithm. While the mechanisms may seem similar at first, what this              \\nreally means is that in order for K-Nearest Neighbors to work, you need labeled data you want                 \\nto classify an unlabeled point into (thus the nearest neighbor part). K-means clustering requires              \\nonly a set of unlabeled points and a threshold: the algorithm will take unlabeled points and                \\ngradually learn how to cluster them into groups by computing the mean of the distance between                 \\ndifferent points. \\n \\nQ43 Mention the difference between Data Mining and Machine learning? \\nMachine learning relates to the study, design, and development of the algorithms that give               \\ncomputers the capability to learn without being explicitly programmed. While data mining can              \\nbe defined as the process in which the unstructured data tries to extract knowledge or unknown                \\ninteresting patterns.  During this processing machine, learning algorithms are used. \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 188}),\n",
       " Document(page_content=' \\nQ44 What are the different Algorithm techniques in Machine Learning? \\nThe different types of techniques in Machine Learning are \\n● Supervised Learning \\n● Unsupervised Learning \\n● Semi-supervised Learning \\n● Reinforcement Learning \\n● Transduction \\n● Learning to Learn \\n \\nQ45 You are given a data set. The data set has missing values that spread along 1                 \\nstandard deviation from the median. What percentage of data would remain            \\nunaffected? Why? \\nThis question has enough hints for you to start thinking! Since the data is spread across the                 \\nmedian, let’s assume it’s a normal distribution. We know, in a normal distribution, ~68% of the                \\ndata lies in 1 standard deviation from mean (or mode, median), which leaves ~32% of the data                 \\nunaffected. Therefore, ~32% of the data would remain unaffected by missing values. \\n \\nQ46 What are PCA, KPCA, and ICA used for? \\nPCA (Principal Components Analysis), KPCA ( Kernel-based Principal Component Analysis)          \\nand ICA ( Independent Component Analysis) are important feature extraction techniques used            \\nfor dimensionality reduction. \\n \\nQ47 What are support vector machines? \\nSupport vector machines are supervised learning algorithms used for classification and           \\nregression analysis. \\n \\nQ48 What is batch statistical learning? \\nStatistical learning techniques allow learning a function or predictor from a set of observed data                \\nthat can make predictions about unseen or future data. These techniques provide guarantees             \\non the performance of the learned predictor on the future unseen data based on a statistical                \\nassumption on the data generating process. \\n \\nQ49 What is the bias-variance decomposition of classification error in the           \\nensemble method? \\nThe expected error of a learning algorithm can be decomposed into bias and variance. A bias                \\nterm measures how closely the average classifier produced by the learning algorithm matches             \\nthe target function. The variance term measures how much the learning algorithm’s prediction             \\nfluctuates for different training sets. \\n \\n \\n \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 189}),\n",
       " Document(page_content=' \\nQ50 When is Ridge regression favorable over Lasso regression? \\nYou can quote ISLR’s authors Hastie, Tibshirani who asserted that, in the presence of few               \\nvariables with medium / large sized effect, use lasso regression. In presence of many variables                \\nwith small/medium-sized effects, use ridge regression. \\nConceptually, we can say, lasso regression (L1) does both variable selection and parameter              \\nshrinkage, whereas Ridge regression only does parameter shrinkage and end up including all              \\nthe coefficients in the model. In the presence of correlated variables, ridge regression might be               \\nthe preferred choice. Also, ridge regression works best in situations where the least square              \\nestimates have higher variance. Therefore, it depends on our model objective. \\n \\nQ51 You’ve built a random forest model with 10000 trees. You got delighted after              \\ngetting training error as 0.00. But, the validation error is 34.23. What is going on?               \\nHaven’t you trained your model perfectly? \\nThe model has overfitted. Training error 0.00 means the classifier has mimicked the training              \\ndata patterns to an extent, that they are not available in the unseen data. Hence, when this                 \\nclassifier was run on an unseen sample, it couldn’t find those patterns and returned predictions                \\nwith higher error. In a random forest, it happens when we use a larger number of trees than                  \\nnecessary. Hence, to avoid this situation, we should tune the number of trees using               \\ncross-validation. \\n \\nQ50 What is a convex hull? \\nIn the case of linearly separable data, the convex hull represents the outer boundaries of the                \\ntwo groups of data points. Once the convex hull is created, we get maximum margin hyperplane                \\n(MMH) as a perpendicular bisector between two convex hulls. MMH is the line which attempts to                \\ncreate the greatest separation between two groups. \\n \\nQ51 What do you understand by Type I vs Type II error? \\nType I error is committed when the null hypothesis is true and we reject it, also known as a                   \\n‘False Positive’. Type II error is committed when the null hypothesis is false and we accept it,                 \\nalso known as ‘False Negative’. \\nIn the context of the confusion matrix, we can say Type I error occurs when we classify a value                   \\nas positive (1) when it is actually negative (0). Type II error occurs when we classify a value as                   \\nnegative (0) when it is actually positive(1). \\n \\nQ52. In k-means or kNN, we use euclidean distance to calculate the distance             \\nbetween nearest neighbors. Why not manhattan distance? \\nWe don’t use manhattan distance because it calculates distance horizontally or vertically only. It              \\nhas dimension restrictions. On the other hand, the euclidean metric can be used in any space to                 \\ncalculate distance. Since the data points can be present in any dimension, euclidean distance is                \\na more viable option. \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 190}),\n",
       " Document(page_content='Example: Think of a chessboard, the movement made by a bishop or a rook is calculated by                  \\nmanhattan distance because of their respective vertical & horizontal movements. \\n \\nQ53 Do you suggest that treating a categorical variable as a continuous variable             \\nwould result in a better predictive model? \\nFor better predictions, the categorical variable can be considered as a continuous variable only              \\nwhen the variable is ordinal in nature. \\n \\nQ54 OLS is to linear regression. The maximum likelihood is logistic regression.            \\nExplain the statement. \\nOLS and Maximum likelihood are the methods used by the respective regression methods to              \\napproximate the unknown parameter (coefficient) value. In simple words, \\nOrdinary least square(OLS) is a method used in linear regression which approximates the              \\nparameters resulting in minimum distance between actual and predicted values. Maximum            \\nLikelihood helps in choosing the values of parameters which maximizes the likelihood that the               \\nparameters are most likely to produce observed data. \\n \\nQ55 When does regularization becomes necessary in Machine Learning? \\nRegularization becomes necessary when the model begins to overfit/underfit. This technique            \\nintroduces a cost term for bringing in more features with the objective function. Hence, it tries to                  \\npush the coefficients for many variables to zero and hence reduce the cost term. This helps to                 \\nreduce model complexity so that the model can become better at predicting (generalizing). \\n \\nQ56 What is Linear Regression? \\nLinear Regression is a supervised Machine Learning algorithm. It is used to find the linear               \\nrelationship between the dependent and the independent variables for predictive analysis.  \\nQ57 What is the Variance Inflation Factor? \\nVariance Inflation Factor (VIF) is the estimate of the volume of multicollinearity in a collection of                 \\nmany regression variables. \\nVIF = Variance of the model / Variance of the model with a single independent variable \\nWe have to calculate this ratio for every independent variable. If VIF is high, then it shows the                  \\nhigh collinearity of the independent variables. \\n \\nQ58 We know that one hot encoding increases the dimensionality of a dataset,             \\nbut label encoding doesn’t. How? \\nWhen we use \\u200bone-hot encoding\\n , there is an increase in the dimensionality of a dataset. The                 \\nreason for the increase in dimensionality is that, for every class in the categorical variables, it                \\nforms a different variable. \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 191}),\n",
       " Document(page_content=' \\nQ59 What is a Decision Tree? \\nA decision tree is used to explain the sequence of actions that must be performed to get the                  \\ndesired output. It is a hierarchical diagram that shows the actions. \\nQ60 What is the Binarizing of data? How to Binarize? \\nIn most of the Machine Learning Interviews, apart from theoretical questions, interviewers focus             \\non the implementation part. So, this ML Interview Questions focused on the implementation of              \\nthe theoretical concepts. \\nConverting data into binary values on the basis of threshold values is known as the binarizing of                  \\ndata. The values that are less than the threshold are set to 0 and the values that are greater                   \\nthan the threshold are set to 1. This process is useful when we have to perform feature                 \\nengineering, and we can also use it for adding unique features. \\n \\nQ61 What is cross-validation? \\nCross-validation is essentially a technique used to assess how well a model performs on a new                 \\nindependent dataset. The simplest example of cross-validation is when you split your data into               \\ntwo groups: training data and testing data, where you use the training data to build the model                 \\nand the testing data to test the model. \\n \\nQ62 When would you use random forests Vs SVM and why? \\nThere are a couple of reasons why a random forest is a better choice of the model than a                   \\nsupport vector machine: \\n● Random forests allow you to determine the feature importance. SVM’s can’t do this. \\n● Random forests are much quicker and simpler to build than an SVM. \\n● For multi-class classification problems, SVMs require a one-vs-rest method, which is           \\nless scalable and more memory intensive. \\n \\nQ63 What are the drawbacks of a linear model? \\nThere are a couple of drawbacks of a linear model: \\n● A linear model holds some strong assumptions that may not be true in the application. It                \\nassumes a linear relationship, multivariate normality, no or little multicollinearity, no            \\nauto-correlation, and homoscedasticity \\n● A linear model can’t be used for discrete or binary outcomes. \\n● You can’t vary the model flexibility of a linear model. \\n \\n \\n \\n \\n \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 192}),\n",
       " Document(page_content=' \\nQ64 Do you think 50 small decision trees are better than a large one? Why? \\nAnother way of asking this question is “Is a random forest a better model than a decision tree?”                  \\nAnd the answer is yes because a random forest is an ensemble method that takes many weak                 \\ndecision trees to make a strong learner. Random forests are more accurate, more robust, and               \\nless prone to overfitting. \\n \\nQ65 What is a kernel? Explain the kernel trick \\nA kernel is a way of computing the dot product of two vectors \\xa0x and\\u1ae3y in some (possibly very                     \\nhigh dimensional) feature space, which is why kernel functions are sometimes called            \\n“generalized dot product”  \\nThe kernel trick is a method of using a linear classifier to solve a non-linear problem by                 \\ntransforming linearly inseparable data to linearly separable ones in a higher dimension. \\nQ66 State the differences between causality and correlation? \\nCausality applies to situations where one action, say X, causes an outcome, say Y, whereas                \\nCorrelation is just relating one action (X) to another action(Y) but X does not necessarily cause                 \\nY. \\nQ67 What is the exploding gradient problem while using the backpropagation           \\ntechnique? \\nWhen large error gradients accumulate and result in large changes in the neural network              \\nweights during training, it is called the exploding gradient problem. The values of weights can               \\nbecome so large as to overflow and result in NaN values. This makes the model unstable and                 \\nthe learning of the model to stall just like the vanishing gradient problem. \\nQ68 What do you mean by Associative Rule Mining (ARM)? \\nAssociative Rule Mining is one of the techniques to discover patterns in data like features                \\n(dimensions) which occur together and features (dimensions) which are correlated. \\n \\nQ69 What is Marginalisation? Explain the process. \\nMarginalizationarginalisation is summing the probability of a random variable X given the joint              \\nprobability distribution of X with other variables. It is an application of the law of total probability. \\n \\nQ70 Why is the rotation of components so important in Principle Component            \\nAnalysis(PCA)? \\nRotation in PCA is very important as it maximizes the separation within the variance obtained by                 \\nall the components because of which interpretation of components would become easier. If the              \\ncomponents are not rotated, then we need extended components to describe the variance of               \\nthe components. \\n \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 193}),\n",
       " Document(page_content=' \\nQ71 What is the difference between regularization and normalisation?  \\nNormalisation adjusts the data; regularisation adjusts the prediction function. If your data is on               \\nvery different scales (especially low to high), you would want to normalise the data. Alter each                \\ncolumn to have compatible basic statistics. This can be helpful to make sure there is no loss of                  \\naccuracy. One of the goals of model training is to identify the signal and ignore the noise if the                    \\nmodel is given free rein to minimize error, there is a possibility of suffering from overfitting.                \\nRegularization imposes some control on this by providing simpler fitting functions over complex              \\nones. \\nQ72 When does the linear regression line stop rotating or finds an optimal spot where it                \\nis fitted on data?  \\nA place where the highest RSquared value is found, is the place where the line comes to rest.                  \\nRSquared represents the amount of variance captured by the virtual linear regression line with               \\nrespect to the total variance captured by the dataset. \\nQ73 How does the SVM algorithm deal with self-learning?  \\nSVM has a learning rate and expansion rate which takes care of this. The learning rate                \\ncompensates or penalises the hyperplanes for making all the wrong moves and expansion rate               \\ndeals with finding the maximum separation area between classes. \\nQ74 How do you handle outliers in the data? \\nOutlier is an observation in the data set that is far away from other observations in the data set.                   \\nWe can discover outliers using tools and functions like box plot, scatter plot, Z-Score, IQR score                \\netc. and then handle them based on the visualization we have got. To handle outliers, we can                 \\ncap at some threshold, use transformations to reduce skewness of the data and remove outliers               \\nif they are anomalies or errors. \\nQ75 Name and define techniques used to find similarities in the recommendation system.  \\nPearson correlation and Cosine correlation are techniques used to find similarities in             \\nrecommendation systems. \\nQ76 Why would you Prune your tree? \\nIn the context of data science or AIML, pruning refers to the process of reducing redundant                \\nbranches of a decision tree. Decision Trees are prone to overfitting, pruning the tree helps to                 \\nreduce the size and minimizes the chances of overfitting. Pruning involves turning branches of a               \\ndecision tree into leaf nodes and removing the leaf nodes from the original branch. It serves as                 \\na tool to perform the tradeoff. \\n \\n \\n \\n \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 194}),\n",
       " Document(page_content=' \\nQ77 Mention some of the EDA Techniques? \\nExploratory Data Analysis (EDA) helps analysts to understand the data better and forms the               \\nfoundation of better models.  \\nVisualization \\n● Univariate visualization \\n● Bivariate visualization \\n● Multivariate visualization \\nMissing Value Treatmen\\n t – Replace missing values with Either Mean/Median \\nOutlier Detection – Use Boxplot to identify the distribution of Outliers, then Apply IQR to set the                  \\nboundary for IQR \\n \\nQ78 What is data augmentation? Can you give some examples?  \\nData augmentation is a technique for synthesizing new data by modifying existing data in such a                \\nway that the target is not changed, or it is changed in a known way. \\nCV is one of the fields where data augmentation is very useful. There are many modifications                \\nthat we can do to images: \\n● Resize \\n● Horizontal or vertical flip \\n● Rotate \\n● Add noise \\n● Deform \\n● Modify colors \\nEach problem needs a customized data augmentation pipeline. For example, on OCR, doing             \\nflips will change the text and won’t be beneficial; however, resizes and small rotations may help. \\nQ79 What is Inductive Logic Programming in Machine Learning (ILP)? \\nInductive Logic Programming (ILP) is a subfield of machine learning which uses logic              \\nprogramming representing background knowledge and examples. \\nQ80 What is the difference between inductive machine learning and deductive machine            \\nlearning? \\nThe difference between inductive machine learning and deductive machine learning are as            \\nfollows: machine-learning where the model learns by examples from a set of observed             \\ninstances to draw a generalized conclusion whereas in deductive learning the model first draws               \\nthe conclusion and then the conclusion is drawn. \\n \\n \\n \\n \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 195}),\n",
       " Document(page_content=' \\nQ81 Difference between machine learning and deep learning \\nMachine learning is a branch of computer science and a method to implement artificial               \\nintelligence. This technique provides the ability to automatically learn and improve from             \\nexperiences without being explicitly programmed. \\nDeep learning can be said as a subset of machine learning. It is mainly based on the artificial                  \\nneural network where data is taken as an input and the technique makes intuitive decisions               \\nusing the artificial neural network. \\n \\nQ82 What Are The Steps Involved In Machine Learning Project? \\nAs you plan for doing a machine learning project. There are several important steps you must                \\nfollow to achieve a good working model and they are data collection, data preparation, choosing               \\na machine learning model, training the model, model evaluation, parameter tuning and lastly             \\nprediction. \\n \\nQ83 Differences between Artificial Intelligence and Machine Learning? \\nArtificial intelligence is a broader prospect than machine learning. Artificial intelligence mimics            \\nthe cognitive functions of the human brain. The purpose of AI is to carry out a task in an                   \\nintelligent manner based on algorithms. On the other hand, machine learning is a subclass of                \\nartificial intelligence. To develop an autonomous machine in such a way so that it can learn                \\nwithout being explicitly programmed is the goal of machine learning. \\n \\nQ84 Steps Needed to Choose the Appropriate Machine Learning Algorithm for           \\nyour Classification problem. \\nFirstly, you need to have a clear picture of your data, your constraints, and your problems                \\nbefore heading towards different machine learning algorithms. Secondly, you have to           \\nunderstand which type and kind of data you have because it plays a primary role in deciding                  \\nwhich algorithm you have to use. \\nFollowing this step is the data categorization step, which is a two-step process – categorization                \\nby input and categorization by output. The next step is to understand your constraints; that is,                \\nwhat is your data storage capacity? How fast the prediction has to be? etc. \\nFinally, find the available machine learning algorithms and implement them wisely. Along with             \\nthat, also try to optimize the hyperparameters which can be done in three ways – grid search,                 \\nrandom search, and Bayesian optimization. \\n \\n \\n \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 196}),\n",
       " Document(page_content=\" \\nQ85 Explain Backpropagation in Machine Learning. \\nA very important question for your machine learning interview. Backpropagation\\n is the             \\nalgorithm for computing artificial neural networks (ANN). It is used by the gradient descent               \\noptimization that exploits the chain rule. By calculating the gradient of the loss function, the                \\nweight of the neurons is adjusted to a certain value. To train a multi-layered neural network is                 \\nthe prime motivation of backpropagation so that it can learn the appropriate internal             \\ndemonstrations. This will help them learn to map any input to its respective output arbitrarily. \\n \\nQ86 What is the Convex Function? \\nThis question is very often asked in machine learning interviews. A convex function is a               \\ncontinuous function, and the value of the midpoint at every interval in its given domain is less                  \\nthan the numerical mean of the values at the two ends of the interval. \\n \\nQ87 What’s the Relationship between True Positive Rate and Recall? \\nThe True positive rate in machine learning is the percentage of the positives that have been                \\nproperly acknowledged, and recall is just the count of the results that have been correctly               \\nidentified and are relevant. Therefore, they are the same things, just having different names. It is                 \\nalso known as sensitivity. \\n \\nQ88 List some Tools for Parallelizing Machine Learning Algorithms. \\nAlthough this question may seem very easy, make sure not to skip this one because it is also                   \\nvery closely related to artificial intelligence and thereby, AI interview questions. Almost all             \\nmachine learning algorithms are easy to serialize. Some of the basic tools for parallelizing are                \\nMatlab, Weka, R, Octave, or the Python-based sci-kit learn.  \\n \\nQ89 What do you mean by Genetic Programming? \\nGenetic Programming (GP) is almost similar to an Evolutionary Algorithm, a subset of machine              \\nlearning. Genetic programming software systems implement an algorithm that uses random            \\nmutation, a fitness function, crossover, and multiple generations of evolution to resolve a              \\nuser-defined task. The genetic programming model is based on testing and choosing the best               \\noption among a set of results. \\n \\nQ90 What do you know about Bayesian Networks? \\nBayesian Networks also referred to as 'belief networks' or 'casual networks', are used to               \\nrepresent the graphical model for probability relationship among a set of variables. \\nFor example, a Bayesian network can be used to represent the probabilistic relationships             \\nbetween diseases and symptoms. As per the symptoms, the network can also compute the               \\nprobabilities of the presence of various diseases. \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  \", metadata={'source': 'ML questions dump.pdf', 'page': 197}),\n",
       " Document(page_content='Efficient algorithms can perform inference or learning in Bayesian networks. Bayesian networks            \\nwhich relate the variables (e.g., speech signals or protein sequences) are called dynamic             \\nBayesian networks. \\nQ91 Which are the two components of the Bayesian logic program? \\nA Bayesian logic program consists of two components: \\n●Logical It contains a set of Bayesian Clauses, which capture the qualitative structure of              \\nthe domain. \\n●Quantitative\\n  It is used to encode quantitative information about the domain. \\n \\nQ92 How is machine learning used in day-to-day life? \\nMost of the people are already using machine learning in their everyday life. Assume that you                \\nare engaging with the internet, you are actually expressing your preferences, likes, dislikes             \\nthrough your searches. All these things are picked up by cookies coming on your computer,               \\nfrom this, the behavior of a user is evaluated. It helps to increase the progress of a user through                   \\nthe internet and provide similar suggestions. \\nThe navigation system can also be considered as one of the examples where we are using                \\nmachine learning to calculate a distance between two places using optimization techniques.             \\nSurely, people are going to more engage with machine learning in the near future \\n \\nQ93 Define Sampling. Why do we need it? \\nAnswer: Sampling is a process of choosing a subset from a target population that would serve                \\nas its representative. We use the data from the sample to understand the pattern in the                \\ncommunity as a whole. Sampling is necessary because often, we can not gather or process the                 \\ncomplete data within a reasonable time. \\n \\nQ94 What does the term decision boundary mean? \\nAnswer: A decision boundary or a decision surface is a hypersurface which divides the              \\nunderlying feature space into two subspaces, one for each class. If the decision boundary is a                 \\nhyperplane, then the classes are linearly separable. \\n \\nQ95 Define entropy? \\nAnswer: Entropy is the measure of uncertainty associated with random variable Y. It is the               \\nexpected number of bits required to communicate the value of the variable. \\n \\nQ96 Indicate the top intents of machine learning? \\nAnswer: The top intents of machine learning are stated below, \\n● The system gets information from the already established computations to give           \\nwell-founded decisions and outputs. \\n● It locates certain patterns in the data and then makes certain predictions on it to provide                \\nanswers on matters. \\n \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 198}),\n",
       " Document(page_content='Q97 Highlight the differences between the Generative model and the          \\nDiscriminative model? \\nThe aim of the Generative model is to generate new samples from the same distribution and                \\nnew data instances, Whereas, the Discriminative model highlights the differences between           \\ndifferent kinds of data instances. It tries to learn directly from the data and then classifies the                 \\ndata. \\n \\nQ98 Identify the most important aptitudes of a machine learning engineer? \\nMachine learning allows the computer to learn itself without being decidedly programmed. It              \\nhelps the system to learn from experience and then improve from its mistakes. The intelligence               \\nsystem, which is based on machine learning, can learn from recorded data and past incidents.               \\nIn-depth knowledge of statistics, probability, data modelling, programming language, as well as            \\nCS, Application of ML Libraries and algorithms, and software design is required to become a               \\nsuccessful machine learning engineer. \\n \\nQ99 What is feature engineering? How do you apply it in the process of              \\nmodelling? \\nFeature engineering is the process of transforming raw data into features that better represent              \\nthe underlying problem to the predictive models, resulting in improved model accuracy on             \\nunseen data. \\n \\nQ100 How can learning curves help create a better model? \\nLearning curves give the indication of the presence of overfitting or underfitting. \\nIn a learning curve, the training error and cross-validating error are plotted against the number               \\nof training data points.  \\n \\n \\n \\n \\n \\n \\nReferences \\n \\n1 springboard.com 2 \\u200bsimplilearn.com 3 geeksforgeeks.org 4 \\u200belitedatascience.com 5          \\nanalyticsvidhya.com 6\\u200bguru99.com 7\\u200bintellipaat.com 8 towardsdatascience.com 9         \\nmygreatlearning.com 10 \\u200bmindmajix.com 11 toptal.com 12 \\u200bglassdoor.co.in 13\\u200budacity.com 14           \\neducba.com 15\\u200banalyticsindiamag.com 16\\u200bubuntupit.com 17\\u200bjavatpoint.com 18 quora.com 19           \\nhackr.io 20 kaggle.com \\nSteve Nouri  \\u200b                           \\u200bhttps://www.linkedin.com/in/stevenouri/  ', metadata={'source': 'ML questions dump.pdf', 'page': 199}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Top 100 NLP Questions \\n     Steve Nouri  \\n \\n \\nQ1. Which of the following techniques can be used for keyword normalization in \\nNLP, the process of converting a keyword into its base form? \\na. Lemmatization \\nb. Soundex \\nc. Cosine Similarity \\nd. N-grams \\n \\nAnswer : a) Lemmatization helps to get to the base form of a word, e.g. are playing -> play, eating \\n-> eat, etc.Other options are meant for different purposes. \\n \\nQ2. Which of the following techniques can be used to compute the distance \\nbetween two word vectors in NLP? \\na. Lemmatization \\nb. Euclidean distance \\nc. Cosine Similarity \\nd. N-grams \\n \\nAnswer : b) and c) \\nDistance between two word vectors can be computed using Cosine similarity and Euclidean \\nDistance.  Cosine Similarity establishes a cosine angle between the vector of two words. A cosine \\nangle close to each other between two word vectors indicates the words are similar and vice a \\nversa. \\nE.g. cosine angle between two words “Football” and “Cricket” will be closer to 1 as compared to \\nangle between the words “Football” and “New Delhi”  \\n \\nQ3. What are the possible features of a text corpus in NLP? \\na. Count of the word in a document \\nb. Vector notation of the word \\nc. Part of Speech Tag \\nd. Basic Dependency Grammar \\ne. All of the above \\n \\nAnswer : e)All of the above can be used as features of the text corpus. \\n \\n \\nQ4. You created a document term matrix on the input data of 20K documents for a \\nMachine learning model. Which of the following can be used to reduce the \\ndimensions of data? ', metadata={'source': 'ML questions dump.pdf', 'page': 200}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  1. Keyword Normalization \\n2. Latent Semantic Indexing \\n3. Latent Dirichlet Allocation \\n \\na. only 1 \\nb. 2, 3 \\nc. 1, 3 \\nd. 1, 2, 3 \\n  \\nAnswer : d) \\n \\nQ5. Which of the text parsing techniques can be used for noun phrase detection, \\nverb phrase detection, subject detection, and object detection in NLP. \\na. Part of speech tagging \\nb. Skip Gram and N-Gram extraction \\nc. Continuous Bag of Words \\nd. Dependency Parsing and Constituency Parsing \\n \\nAnswer : d) \\n \\nQ6. Dissimilarity between words expressed using cosine similarity will have values \\nsignificantly higher than 0.5 \\na. True \\nb. False \\n \\nAnswer : a) \\n \\nQ7. Which one of the following are keyword Normalization techniques in NLP \\na.  Stemming \\nb.  Part of Speech \\nc. Named entity recognition \\nd. Lemmatization \\n \\nAnswer : a) and d) \\nPart of Speech (POS) and Named Entity Recognition(NER) are not keyword Normalization \\ntechniques. Named Entity help you extract Organization, Time, Date, City, etc..type of entities \\nfrom the given sentence, whereas Part of Speech helps you extract Noun, Verb, Pronoun, \\nadjective, etc..from the given sentence tokens. \\n \\nQ8. Which of the below are NLP use cases? \\na. Detecting objects from an image \\nb. Facial Recognition \\nc. Speech Biometric \\nd. Text Summarization ', metadata={'source': 'ML questions dump.pdf', 'page': 201}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/   \\nAnswer :  (d) \\na) And b) are Computer Vision use cases, and c) is Speech use case. \\nOnly d) Text Summarization is an NLP use case. \\n \\nQ9. In a corpus of N documents, one randomly chosen document contains a total \\nof T terms and the term “hello” appears K times.  \\nWhat is the correct value for the product of TF (term frequency) and IDF (inverse-document-\\nfrequency), if the term “hello” appears in approximately o ne-third of the total documents? \\na. KT * Log(3) \\nb. T * Log(3) / K \\nc. K * Log(3) / T \\nd. Log(3) / KT \\n \\nAnswer : (c) \\nformula for TF is K/T \\nformula for IDF is log(total docs / no of docs containing “data”)  \\n= log(1 / (⅓))  \\n= log (3) \\nHence correct choice is Klog(3)/T \\n \\nQ10. In NLP, The algorithm decreases the weight for commonly used words and \\nincreases the weight for words that are not used very much in a collection of \\ndocuments \\na. Term Frequency (TF) \\nb. Inverse Document Frequency (IDF) \\nc. Word2Vec \\nd. Latent Dirichlet Allocation (LDA) \\n \\nAnswer : b) \\n \\n \\n \\n \\n \\nQ11. In NLP, The process of removing words like “and”, “is”, “a”, “an”, “the” from \\na sentence is called as \\na. Stemming \\nb. Lemmatization \\nc. Stop word \\nd. All of the above \\n \\nAnswer : c) In Lemmatization, all the stop words such as a, an, the, etc.. are removed. One can \\nalso define custom stop words for removal. ', metadata={'source': 'ML questions dump.pdf', 'page': 202}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/   \\nQ12. In NLP, The process of converting a sentence or paragraph into tokens is \\nreferred to as Stemming \\na. True \\nb. False \\n \\nAnswer : b) The statement describes the process of tokenization and not stemming, hence it is \\nFalse. \\n \\nQ13. In NLP, Tokens are converted into numbers before giving to any Neural \\nNetwork \\na. True \\nb. False \\n \\nAnswer : a) In NLP, all words are converted into a number before feeding to a Neural Network. \\n \\nQ14 Identify the odd one out \\na. nltk \\nb. scikit learn \\nc. SpaCy \\nd. BERT \\n \\nAnswer : d) All the ones mentioned are NLP libraries except BERT, which is a word embedding \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nQ15 TF-IDF helps you to establish? \\na. most frequently occurring word in the document \\nb. most important word in the document \\n \\nAnswer : b) TF-IDF helps to establish how important a particular word is in the context of the \\ndocument corpus. TF-IDF takes into account the number of times the word appears in the \\ndocument and offset by the number of documents that appear in the corpus. \\n● TF is the frequency of term divided by a total number of terms in the document. \\n● IDF is obtained by dividing the total number of documents by the number of documents \\ncontaining the term and then taking the logarithm of that quotient. ', metadata={'source': 'ML questions dump.pdf', 'page': 203}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  ● Tf.idf is then the multiplication of two values TF and IDF. \\n \\nQ16 In NLP, The process of identifying people, an organization from a given \\nsentence, paragraph is called \\na. Stemming \\nb. Lemmatization \\nc. Stop word removal \\nd. Named entity recognition \\n \\nAnswer : d) \\n \\nQ17 Which one of the following is not a pre-processing technique in NLP \\na. Stemming and Lemmatization \\nb. converting to lowercase \\nc. removing punctuations \\nd. removal of stop words \\ne. Sentiment analysis \\n \\nAnswer : e) Sentiment Analysis is not a pre-processing technique. It is done after pre-processing \\nand is an NLP use case. All other listed ones are used as part of statement pre-processing. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nQ18 In text mining, converting text into tokens and then converting them into an \\ninteger or floating-point vectors can be done using \\na. CountVectorizer \\nb.  TF-IDF \\nc. Bag of Words \\nd. NERs \\n \\nAnswer : a) CountVectorizer helps do the above, while others are not applicable. \\ntext =[“Rahul is an avid writer, he enjoys studying understanding and presen ting. He loves to \\nplay”] \\nvectorizer = CountVectorizer() \\nvectorizer.fit(text) \\nvector = vectorizer.transform(text) ', metadata={'source': 'ML questions dump.pdf', 'page': 204}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  print(vector.toarray()) \\noutput  \\n[[1 1 1 1 2 1 1 1 1 1 1 1 1 1]] \\nThe second section of the interview questions covers advanced NLP techniques such as \\nWord2Vec, GloVe  word embeddings, and advanced models such as GPT, ELMo, BERT, XLNET \\nbased questions, and explanations. \\n \\nQ19. In NLP, Words represented as vectors are called as Neural Word Embeddings \\na. True \\nb. False \\nAnswer : a) Word2Vec, GloVe based models build word embedding vectors that are \\nmultidimensional. \\n \\nQ20. In NLP, Context modeling is supported with which one of the following word \\nembeddings \\n1. a. Word2Vec \\n2. b) GloVe \\n3. c) BERT \\n4. d) All of the above \\nAnswer : c) Only BERT (Bidirectional Encoder Representations from Transformer) supports \\ncontext modelling where the previous and next sentence context is taken into consideration. In \\nWord2Vec, GloVe only word embeddings are considered and previous and next sentence context \\nis not considered. \\n \\n \\n \\n \\n \\n \\nQ21. In NLP, Bidirectional context is supported by which of the following \\nembedding \\na. Word2Vec \\nb. BERT \\nc. GloVe \\nd. All the above \\nAnswer : b) Only BERT provides a bidirectional context. The BERT model uses the previous and \\nthe next sentence to arrive at the context.Word2Vec and GloVe are word embeddings, they do \\nnot provide any context.  \\n \\nQ22. Which one of the following Word embeddings can be custom trained for a \\nspecific subject in NLP \\na. Word2Vec \\nb. BERT ', metadata={'source': 'ML questions dump.pdf', 'page': 205}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  c. GloVe \\nd. All the above \\n \\nAnswer : b) BERT allows Transform Learning on the existing pre-trained models and hence can \\nbe custom trained for the given specific subject, unlike Word2Vec and GloVe where existing word \\nembeddings can be used, no transfer learning on text is possible. \\n \\nQ23. Word embeddings capture multiple dimensions of data and are represented \\nas vectors \\na. True \\nb. False \\n \\nAnswer : a) \\n \\nQ24. In NLP, Word embedding vectors help establish distance between two tokens \\na. True \\nb. False \\n \\nAnswer : a) One can use Cosine similarity to establish distance between two vectors represented \\nthrough Word Embeddings \\n \\nQ25. Language Biases are introduced due to historical data used during training of \\nword embeddings, which one amongst the below is not an example of bias \\na. New Delhi is to India, Beijing is to China \\nb. Man is to Computer, Woman is to Homemaker \\n \\nAnswer : a) \\nStatement b) is a bias as it buckets Woman into Homemaker, whereas statement a) is not a \\nbiased statement. \\n \\nQ26. Which of the following will be a better choice to address NLP use cases such \\nas semantic similarity, reading comprehension, and common sense reasoning \\na. ELMo \\nb. Open AI’s GPT  \\nc. ULMFit \\n \\nAnswer :  b) Open AI’s GPT is able to learn complex pattern in data by using the Transformer \\nmodels Attention mechanism and hence is more suited for complex use cases such as semantic \\nsimilarity, reading comprehensions, and common sense reasoning. \\n \\nQ27. Transformer architecture was first introduced with? \\na. GloVe \\nb. BERT ', metadata={'source': 'ML questions dump.pdf', 'page': 206}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  c. Open AI’s GPT  \\nd. ULMFit \\n \\nAnswer : c) ULMFit has an LSTM based Language modeling architecture. This got replaced into \\nTransformer architecture with Open AI’s GPT  \\n \\nQ28. Which of the following architecture can be trained faster and needs less \\namount of training data \\na. LSTM based Language Modelling \\nb. Transformer architecture \\n \\nAnswer :  b) Transformer architectures were supported from GPT onwards and were faster to \\ntrain and needed less amount of data for training too. \\n \\nQ29. Same word can have multiple word embeddings possible with ____________? \\na. GloVe \\nb. Word2Vec \\nc. ELMo \\nd. nltk \\n \\nAnswer : c) EMLo word embeddings supports same word with multiple embeddings, this helps \\nin using the same word in a different context and thus captures the context than just meaning of \\nthe word unlike in GloVe and Word2Vec. Nltk is not a word embedding. \\n \\n \\n \\n \\nQ30 For a given token, its input representation is the sum of embedding from the \\ntoken, segment and position embedding \\na. ELMo \\nb. GPT \\nc. BERT \\nd. ULMFit \\n \\nAnswer :  c) BERT uses token, segment and position embedding. \\n \\n \\nQ31. Trains two independent LSTM language model left to right and right to left and \\nshallowly concatenates them \\na. GPT \\nb. BERT \\nc. ULMFit \\nd. ELMo ', metadata={'source': 'ML questions dump.pdf', 'page': 207}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/   \\nAnswer : d) ELMo tries to train two independent LSTM language models (left to right and right to \\nleft) and concatenates the results to produce word embedding. \\n \\nQ32. Uses unidirectional language model for producing word embedding \\na. BERT \\nb. GPT \\nc. ELMo \\nd. Word2Vec \\n \\nAnswer : b) GPT is a unidirectional model and word embedding are produced by training on \\ninformation flow from left to right. ELMo is bidirectional but shallow. Word2Vec provides simple \\nword embedding. \\n \\nQ33. In this architecture, the relationship between all words in a sentence is \\nmodelled irrespective of their position. Which architecture is this? \\na. OpenAI GPT \\nb. ELMo \\nc. BERT \\nd. ULMFit \\n \\nAnswer : c)BERT Transformer architecture models the relationship between each word and all \\nother words in the sentence to generate attention scores. These attention scores are later used \\nas weights for a weighted average of all words’ representations which is fed into a full y-connected \\nnetwork to generate a new representation. \\n \\nQ34. List 10 use cases to be solved using NLP techniques? \\n● Sentiment Analysis \\n● Language Translation (English to German, Chinese to English, etc..) \\n● Document Summarization \\n● Question Answering \\n● Sentence Completion \\n● Attribute extraction (Key information extraction from the documents) \\n● Chatbot interactions \\n● Topic classification \\n● Intent extraction \\n● Grammar or Sentence correction \\n● Image captioning \\n● Document Ranking \\n● Natural Language inference \\n \\nQ35. Transformer model pays attention to the most important word in Sentence \\na. True \\nb. False ', metadata={'source': 'ML questions dump.pdf', 'page': 208}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Answer : a) Attention mechanisms in the Transformer model are used to model the relationship \\nbetween all words and also provide weights to the most important word. \\n \\nQ36. Which NLP model gives the best accuracy amongst the following? \\na. BERT \\nb. XLNET \\nc. GPT-2 \\nd. ELMo \\n \\nAnswer : b) XLNET has given best accuracy amongst all the models. It has outperformed BERT \\non 20 tasks and achieves state of art results on 18 tasks including sentiment analysis, question \\nanswering, natural language inference, etc. \\n \\nQ37. Permutation Language models is a feature of \\na. BERT \\nb. EMMo \\nc. GPT \\nd. XLNET \\n \\nAnswer : d) XLNET provides permutation-based language modelling and is a key difference from \\nBERT. In permutation language modeling, tokens are predicted in a random manner and not \\nsequential. The order of prediction is not necessarily left to right and can be right to left. The \\noriginal order of words is not changed but a prediction can be random.  \\nThe conceptual difference between BERT and XLNET can be seen from the following diagram. \\n \\nQ38. Transformer XL uses relative positional embedding \\na. True \\nb. False \\na) Instead of embedding having to represent the absolute position of a word, Transformer XL uses \\nan embedding to encode the relative distance between the words. This embedding is used to \\ncompute the attention score between any 2 words that could be separated by n words before or \\nafter. \\n \\nQ39. What is Naive Bayes algorithm, When we can use this algorithm in NLP? \\nNaive Baye s algorithm is a collection of classifiers which works on the principles of the Bayes’ \\ntheorem. This series of NLP model forms a family of algorithms that can be used for a wide range \\nof classification tasks including sentiment prediction, filtering of spam, classifying documents and \\nmore. \\nNaive Bayes algorithm converges faster and requires less training data. Compared to other \\ndiscriminative models like logistic regression, Naive Bayes model it takes lesser time to train. This \\nalgorithm is perfect for use while working with multiple classes and text classification where the \\ndata is dynamic and changes frequently. \\n \\nQ40. Explain Dependency Parsing in NLP? ', metadata={'source': 'ML questions dump.pdf', 'page': 209}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Dependency Parsing, also known as Syntactic parsing in NLP is a process of assigning syntactic \\nstructure to a sentence and identifying its dependency parses. This process is crucial to \\nunderstand the correlations between the “head” words in the syntactic structure.  \\nThe process of dependency parsing can be a little complex considering how any sentence can \\nhave more than one dependency parses. Multiple parse trees are known as ambiguities. \\nDependency parsing needs to resolve these ambiguities in order to effectively assign a syntactic \\nstructure to a sentence. \\nDependency parsing can be used in the semantic analysis of a sentence apart from the syntactic \\nstructuring. \\n \\nQ41. What is text Summarization? \\nText summarization is the process of shortening a long piece of text with its meaning and effect \\nintact. Text summarization intends to create a summary of any given piece of text and outlines \\nthe main points of the document. This technique has improved in recent times and is capable of \\nsummarizing volumes of text successfully. \\nText summarization has proved to a blessing since machines can summarise large volumes of \\ntext in no time which would otherwise be really time-consuming. There are two types of text \\nsummarization: \\n● Extraction-based summarization \\n● Abstraction-based summarization \\n \\n \\nQ42. What is NLTK? How is it different from Spacy? \\nNLTK or Natural Language Toolkit is a series of libraries and programs that are used for symbolic \\nand statistical natural language processing. This toolkit contains some of the most powerful \\nlibraries that can work on different ML techniques to break down and understand human \\nlanguage. NLTK is used for Lemmatization, Punctuation, Character count, Tokenization, and \\nStemming. The difference between NLTK and Spacey are as follows: \\n● While NLTK has a collection of programs to choose from, Spacey contains only the best-\\nsuited algorithm for a problem in its toolkit \\n● NLTK supports a wider range of languages compared to Spacey (Spacey supports only 7 \\nlanguages) \\n● While Spacey has an object-oriented library, NLTK has a string processing library \\n● Spacey can support word vectors while NLTK cannot \\n \\nQ43. What is information extraction? \\nInformation extraction in the context of Natural Language Processing refers to the technique of \\nextracting structured information automatically from unstructured sources to ascribe meaning to \\nit. This can include extracting information regarding attributes of entities, relationship between \\ndifferent entities and more. The various models of information extraction includes: \\n● Tagger Module \\n● Relation Extraction Module \\n● Fact Extraction Module \\n● Entity Extraction Module ', metadata={'source': 'ML questions dump.pdf', 'page': 210}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  ● Sentiment Analysis Module \\n● Network Graph Module \\n● Document Classification & Language Modeling Module \\n \\nQ44. What is Bag of Words? \\nBag of Words is a commonly used model that depends on word frequencies or occurrences to \\ntrain a classifier. This model creates an occurrence matrix for documents or sentences \\nirrespective of its grammatical structure or word order.  \\n \\nQ45. What is Pragmatic Ambiguity in NLP? \\nPragmatic ambiguity refers to those words which have more than one meaning and their use in \\nany sentence can depend entirely on the context. Pragmatic ambiguity can result in multiple \\ninterpretations of the same sentence. More often than not, we come across sentences which have \\nwords with multiple meanings, making the sentence open to interpretation. This multiple \\ninterpretation causes ambiguity and is known as Pragmatic ambiguity in NLP. \\n \\nQ46. What is a Masked Language Model? \\nMasked language models help learners to understand deep representations in downstream tasks \\nby taking an output from the corrupt input. This model is often used to predict the words to be \\nused in a sentence. \\nQ48. What are the best NLP Tools? \\nSome of the best NLP tools from open sources are: \\n● SpaCy \\n● TextBlob \\n● Textacy \\n● Natural language Toolkit \\n● Retext \\n● NLP.js \\n● Stanford NLP \\n● CogcompNLP \\n \\nQ49. What is POS tagging? \\nParts of speech tagging better known as POS tagging refers to the process of identifying specific \\nwords in a document and group them as part of speech, based on its context. POS tagging is also \\nknown as grammatical tagging since it involves understanding grammatical structures and \\nidentifying the respective component. \\nPOS tagging is a complicated process since the same word can be different parts of speech \\ndepending on the context. The same generic process used for word mapping is quite ineffective \\nfor POS tagging because of the same reason. \\n \\nQ50. What is NES? \\nName entity recognition is more commonly known as NER is the process of identifying specific \\nentities in a text document which are more informative and have a unique context. These often \\ndenote places, people, organisations, and more. Even though it seems like these entities are ', metadata={'source': 'ML questions dump.pdf', 'page': 211}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  proper nouns, the NER process is far from identifying just the nouns. In fact, NER involves entity \\nchunking or extraction wherein entities are segmented to categorise them under different \\npredefined classes. This step further helps in extracting information. \\n \\nQ51 Explain the Masked Language  Model? \\nMasked language modelling is the process in which the output is taken from the corrupted input. \\nThis model helps the learners to master the deep representations in downstream tasks. You can \\npredict a word from the other words of the sentence using this model. \\n \\nQ52 What is pragmatic analysis in NLP?  \\nPragmatic Analysis: It deals with outside word knowledge, which means knowledge that is \\nexternal to the documents and/or queries. Pragmatics analysis that focuses on what was \\ndescribed is reinterpreted by what it actually meant, deriving the various aspects of language that \\nrequire real-world knowledge. \\n \\n \\n \\n \\nQ53 What is perplexity in NLP?  \\nThe word \"perplexed\" means \"puzzled\" or \"confused\", thus Perplexity in general means the \\ninability to tackle something complicated and a problem that is not specified. Therefore, Perplexity \\nin NLP is a way to determine the extent of uncertainty in predicting some text. \\nIn NLP, perplexity is a way of evaluating language models. Perplexity can be high and low; Low \\nperplexity is ethical because the inability to deal with any complicated problem is less while high \\nperplexity is terrible because the failure to deal with a complicated is high. \\n \\nQ54 What is ngram in NLP?  \\nN-gram in NLP is simply a sequence of n words, and we also conclude the sentences which \\nappeared more frequently, for example, let us consider the progression of these three words: \\n● New York (2 gram) \\n● The Golden Compass (3 gram) \\n● She was there in the hotel (4 gram) \\nNow from the above sequence, we can easily conclude that sentence (a) appeared more \\nfrequently than the other two sentences, and the last sentence(c) is not seen that often. Now if \\nwe assign probability in the occurrence of an n-gram, then it will be advantageous. It would help \\nin making next-word predictions and in spelling error corrections. \\n \\nQ55 Explain differences between AI, Machine Learning and NLP ', metadata={'source': 'ML questions dump.pdf', 'page': 212}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/   \\n \\n \\n \\n \\n \\n \\nQ56 Why self-attention is awesome? \\n“In terms of computational complexity, self -attention layers are faster than recurrent layers when \\nthe sequence length n is smaller than the representation dimensionality d, which is most often the \\ncase with sentence representations used by state-of-the-art models in machine translations, such \\nas word-piece and byte- pair representations.” —  from Attention is all you need \\n \\nQ57 What are stop words? \\nStop words are said to be useless data for a search engine. Words such as articles, prepositions, \\netc. are considered as stop words. There are stop words such as was, were, is, am, the, a, an, \\nhow, why, and many more. In Natural Language Processing, we eliminate the stop words to \\nunderstand and analyze the meaning of a sentence. The removal of stop words is one of the most \\nimportant tasks for search engines. Engineers design the algorithms of search engines in such a \\nway that they ignore the use of stop words. This helps show the relevant search result for a query. \\n \\nQ58 What is Latent Semantic Indexing (LSI)? \\n', metadata={'source': 'ML questions dump.pdf', 'page': 213}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Latent semantic indexing is a mathematical technique used to improve the accuracy of the \\ninformation retrieval process. The design of LSI algorithms allows machines to detect the hidden \\n(latent) correlation between semantics (words). To enhance information understanding, machines \\ngenerate various concepts that associate with the words of a sentence. \\nThe technique used for information understanding is called singular value decomposition. It is \\ngenerally used to handle static and unstructured data. The matrix obtained for singular value \\ndecomposition contains rows for words and columns for documents. This method best suits to \\nidentify components and group them according to their types. \\nThe main principle behind LSI is that words carry a similar meaning when used in a similar context. \\nComputational LSI models are slow in comparison to other models. However, they are good at \\ncontextual awareness that helps improve the analysis and understanding of a text or a document. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nQ60 What are Regular Expressions? \\nA regular expression is used to match and tag words. It consists of a series of characters for \\nmatching strings. \\nSuppose, if A and B are regular expressions, then the following are true for them: \\n● If {ɛ} is a regular language, then ɛ is a regular expression for it. \\n● If A and B are regular expressions, then A + B is also a regular expression within the \\nlanguage {A, B}. \\n● If A and B are regular expressions, then the concatenation of A and B (A.B) is a regular \\nexpression. \\n● If A is a regular expression, then A* (A occurring multiple times) is also a regular \\nexpression. \\n \\nQ61 What are unigrams, bigrams, trigrams, and n-grams in NLP? \\nWhen we parse a sentence one word at a time, then it is called a unigram. The sentence parsed \\ntwo words at a time is a bigram. \\nWhen the sentence is parsed three words at a time, then it is a trigram. Similarly, n-gram refers \\nto the parsing of n words at a time. \\nExample: To understand unigrams, bigrams, and trigrams, you can refer to the below diagram: \\n \\nQ62 What are the steps involved in solving an NLP problem? \\nBelow are the steps involved in solving an NLP problem: \\n1. Gather the text from the available dataset or by web scraping ', metadata={'source': 'ML questions dump.pdf', 'page': 214}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  2. Apply stemming and lemmatization for text cleaning \\n3. Apply feature engineering techniques \\n4. Embed using word2vec \\n5. Train the built model using neural networks or other Machine Learning techniques \\n6. Evaluate the model’s performance  \\n7. Make appropriate changes in the model \\n8. Deploy the model \\n \\nQ63. There have some various common elements of natural language processing. \\nThose elements are very important for understanding NLP properly, can you please \\nexplain the same in details with an example? \\nAnswer: \\nThere have a lot of components normally using by natural language processing (NLP). Some of \\nthe major components are explained below: \\n● Extraction of Entity: It actually identifying and extracting some critical data from the \\navailable information which help to segmentation of provided sentence on identifying each \\nentity. It can help in identifying one human that it’s fictional or real, same kind of reality \\nidentification for any organization, events or any geographic location etc. \\n● The analysis in a syntactic way: it mainly helps for maintaining ordering properly of the \\navailable words. \\nQ64 In the case of processing natural language, we normally mentioned one \\ncommon terminology NLP and binding every language with the same terminology \\nproperly. Please explain in details about this NLP terminology with an example? \\nAnswer: \\nThis is the basic NLP Interview Questions asked in an interview. There have some several factors \\navailable in case of explaining natural language processing. Some of the key factors are given \\nbelow: \\n● Vectors and Weights: Google Word vectors, length of TF-IDF, varieties documents, word \\nvectors, TF-IDF. \\n● Structure of Text: Named Entities, tagging of part of speech, identifying the head of the \\nsentence. \\n● Analysis of sentiment: Know about the features of sentiment, entities available for the \\nsentiment, sentiment common dictionary. \\n● Classification of Text: Learning supervising, set off a train, set of validation in Dev, Set of \\ndefine test, a feature of the individual text, LDA. \\n● Reading of Machine Language: Extraction of the possible entity, linking with an individual \\nentity, DBpedia, some libraries like Pikes or FRED. \\n \\nQ65 Explain briefly about word2vec \\nWord2Vec  embeds words in a lower-dimensional vector space using a shallow neural network. \\nThe result is a set of word-vectors where vectors close together in vector space have similar \\nmeanings based on context, and word-vectors distant to each other have differing meanings. For \\nexample, apple and orange would be close together and apple and gravity would be relatively far. ', metadata={'source': 'ML questions dump.pdf', 'page': 215}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  There are two versions of this model based on skip-grams (SG) and continuous-bag-of-words \\n(CBOW). \\n \\nQ66 What are the metrics used to test an NLP model? \\nAccuracy, Precision, Recall and F1. Accuracy is the usual ratio of the prediction to the desired \\noutput. But going just be accuracy is naive considering the complexities involved. \\n \\nQ67 What are some ways we can preprocess text input? \\nHere are several preprocessing steps that are commonly used for NLP tasks: \\n● case normalization: we can convert all input to the same case (lowercase or uppercase) \\nas a way of reducing our text to a more canonical form \\n● punctuation/stop word/white space/special characters removal: if we don’t think these \\nwords or characters are relevant, we can remove them to reduce the feature space \\n● lemmatizing/stemming: we can also reduce words to their inflecti onal forms (i.e. walks → \\nwalk) to further trim our vocabulary \\n● generalizing irrelevant information: we can replace all numbers with a <NUMBER> token \\nor all names with a <NAME> token \\n \\n \\nQ68 How does the encoder-decoder structure work for language modelling? \\nThe encoder-decoder structure is a deep learning model architecture responsible for several state \\nof the art solutions, including Machine Translation. \\nThe input sequence is passed to the encoder where it is transformed to a fixed-dimensional vector \\nrepresentation using a neural network. The transformed input is then decoded using another \\nneural network. Then, these outputs undergo another transformation and a softmax layer. The \\nfinal output is a vector of probabilities over the vocabularies. Meaningful information is extracted \\nbased on these probabilities. \\n \\nQ69 What are attention mechanisms and why do we use them? \\nThis was a followup to the encoder-decoder question. Only the output from the last time step is \\npassed to the decoder, resulting in a loss of information learned at previous time steps. This \\ninformation loss is compounded for longer text sequences with more time steps. \\nAttention mechanisms are a function of the hidden weights at each time step. When we use \\nattention in encoder-decoder networks, the fixed-dimensional vector passed to the decoder \\nbecomes a function of all vectors outputted in the intermediary steps. \\nTwo commonly used attention mechanisms are additive attention and multiplicative attention. As \\nthe names suggest, additive attention is a weighted sum while multiplicative attention is a \\nweighted multiplier of the hidden weights. During the training process, the model also learns \\nweights for the attention mechanisms to recognize the relative importance of each time step. \\n \\nQ70 How would you implement an NLP system as a service, and what are some \\npitfalls you might face in production? \\nThis is less of a NLP question than a question for productionizing machine learning models. There \\nare however certain intricacies to NLP models. ', metadata={'source': 'ML questions dump.pdf', 'page': 216}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Without diving too much into the productionization aspect, an ideal Machine Learning service will \\nhave: \\n● endpoint(s) that other business systems can use to make inference \\n● a feedback mechanism for validating model predictions \\n● a database to store predictions and ground truths from the feedback \\n● a workflow orchestrator which will (upon some signal) re-train and load the new model for \\nserving based on the records from the database + any prior training data \\n● some form of model version control to facilitate rollbacks in case of bad deployments \\n● post-production accuracy and error monitoring \\n \\n \\n \\n \\n \\n \\n \\n \\nQ71 How can we handle misspellings for text input? \\nBy using word embeddings trained over a large corpus (for instance, an extensive web scrape of \\nbillions of words), the model vocabulary would include common misspellings by design. The \\nmodel can then learn the relationship between misspelled and correctly spelled words to \\nrecognize their semantic similarity. \\nWe can also preprocess the input to prevent misspellings. Terms not found in the model \\nvocabulary can be mapped to the “closest” vocabulary term using:  \\n● edit distance between strings \\n● phonetic distance between word pronunciations \\n● keyword distance to catch common typos \\n \\nQ72 Which of the following models can perform tweet classification with regards \\nto context mentioned above? \\nA) Naive Bayes \\nB) SVM \\nC) None of the above \\nSolution: (C) \\nSince, you are given only the data of tweets and no other information, which means there is no \\ntarget variable present. One cannot train a supervised learning model, both svm and naive bayes \\nare supervised learning techniques. \\n \\nQ73 You have created a document term matrix of the data, treating every tweet as \\none document. Which of the following is correct, in regards to document term \\nmatrix? \\n1. Removal of stopwords from the data will affect the dimensionality of data \\n2. Normalization of words in the data will reduce the dimensionality of data ', metadata={'source': 'ML questions dump.pdf', 'page': 217}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  3. Converting all the words in lowercase will not affect the dimensionality of the data \\nA) Only 1 \\nB) Only 2 \\nC) Only 3 \\nD) 1 and 2 \\nE) 2 and 3 \\nF) 1, 2 and 3 \\nSolution: (D) \\nChoices A and B are correct because stopword removal will decrease the number of features in \\nthe matrix, normalization of words will also reduce redundant features, and, converting all words \\nto lowercase will also decrease the dimensionality. \\n  \\n \\n \\n \\n \\nQ74 Which of the following features can be used for accuracy improvement of a \\nclassification model? \\nA) Frequency count of terms \\nB) Vector Notation of sentence \\nC) Part of Speech Tag \\nD) Dependency Grammar \\nE) All of these \\nSolution: (E) \\nAll of the techniques can be used for the purpose of engineering features in a model. \\n  \\nQ75 What percentage of the total statements are correct with regards to Topic \\nModeling? \\n1. It is a supervised learning technique \\n2. LDA (Linear Discriminant Analysis) can be used to perform topic modeling \\n3. Selection of number of topics in a model does not depend on the size of data \\n4. Number of topic terms are directly proportional to size of the data \\nA) 0 \\nB) 25 \\nC) 50 \\nD) 75 \\nE) 100 \\nSolution: (A) \\nLDA is unsupervised learning model, LDA is latent Dirichlet allocation, not Linear discriminant \\nanalysis. Selection of the number of topics is directly proportional to the size of the data, while \\nnumber of topic terms is not directly proportional to the size of the data. Hence none of the \\nstatements are correct. \\n  ', metadata={'source': 'ML questions dump.pdf', 'page': 218}),\n",
       " Document(page_content=\"Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Q76 In Latent Dirichlet Allocation model for text classification purposes, what does \\nalpha and beta hyperparameter represent- \\nA) Alpha: number of topics within documents, beta: number of terms within topics False \\nB) Alpha: density of terms generated within topics, beta: density of topics generated within terms \\nFalse \\nC) Alpha: number of topics within documents, beta: number of terms within topics False \\nD) Alpha: density of topics generated within documents, beta: density of terms generated within \\ntopics True \\nSolution: (D) \\nOption D is correct \\n \\n \\n \\n \\n \\nQ77 What is the problem with ReLu? \\n● Exploding gradient(Solved by gradient clipping) \\n● Dying ReLu — No learning if the activation is 0 (Solved by parametric relu) \\n● Mean and variance of activations is not 0 and 1.(Partially solved by subtracting around 0.5 \\nfrom activation. Better explained in fastai videos) \\n \\nQ78 What is the difference between learning latent features using SVD and getting \\nembedding vectors using deep network? \\nSVD uses linear combination of inputs while a neural network uses nonlinear combination. \\n \\nQ79 What is the information in the hidden and cell state of LSTM? \\nHidden stores all the information till that time step and cell state stores particular information that \\nmight be needed in the future time step. \\nNumber of parameters in an LSTM model with bias \\n4(\\xa0h+h²+h) where \\xa0 is input vectors size and h is output vectors size a.k.a. hidden \\nThe point to see here is that mh dictates the model size as m>>h. Hence it's important to have a \\nsmall vocab. \\nTime complexity of LSTM \\nseq_length*hidden² \\nTime complexity of transfomer \\nseq_length²*hidden \\nWhen hidden size is more than the seq_length(which is normally the case), transfomer is faster \\nthan LSTM. \\n \\nQ80 When is self-attention not faster than recurrent layers? \\nWhen the sequence length is greater than the representation dimensions. This is rare. \\n \\nQ81 What is the benefit of learning rate warm-up? \", metadata={'source': 'ML questions dump.pdf', 'page': 219}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Learning rate warm-up is a learning rate schedule where you have low (or lower) learning rate at \\nthe beginning of training to avoid divergence due to unreliable gradients at the beginning. As the \\nmodel becomes more stable, the learning rate would increase to speed up convergence. \\n \\nQ82 What’s the difference between hard and soft parameter sharing in multi -task \\nlearning? \\nHard sharing is where we train for all the task at the same time and update our weights using all \\nthe losses whereas soft sharing is where we train for one task at a time. \\n \\nQ83 What’s the difference between BatchNorm and LayerNorm?  \\nBatchNorm computes the mean and variance at each layer for every minibatch whereas \\nLayerNorm computes the mean and variance for every sample for each layer independently. \\nBatch normalisation allows you to set higher learning rates, increasing speed of training as it \\nreduces the unstability of initial starting weights. \\nQ84 Difference between BatchNorm and LayerNorm? \\nBatchNorm — Compute the mean and var at each layer for every minibatch \\nLayerNorm — Compute the mean and var for every single sample for each layer independently \\n \\nQ85 Why does the transformer block have LayerNorm instead of BatchNorm? \\nLooking at the advantages of LayerNorm, it is robust to batch size and works better as it works at \\nthe sample level and not batch level. \\n \\nQ86 What changes would you make to your deep learning code if you knew there \\nare errors in your training data? \\nWe can do label smoothening where the smoothening value is based on % error. If any particular \\nclass has known error, we can also use class weights to modify the loss. \\n \\nQ87 What are the tricks used in ULMFiT? (Not a great questions but checks the \\nawareness) \\n● LM tuning with task text \\n● Weight dropout \\n● Discriminative learning rates for layers \\n● Gradual unfreezing of layers \\n● Slanted triangular learning rate schedule \\nThis can be followed up with a question on explaining how they help. \\n \\nQ88 Tell me a language model whi ch doesn’t use dropout  \\nALBERT v2 — This throws a light on the fact that a lot of assumptions we take for granted are \\nnot necessarily true. The regularisation effect of parameter sharing in ALBERT is so strong that \\ndropouts are not needed. (ALBERT v1 had dropouts.) \\n \\nQ89 What are the differences between GPT and GPT-2? (From Lilian Weng) ', metadata={'source': 'ML questions dump.pdf', 'page': 220}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  ● Layer normalization  was moved to the input of each sub-block, similar to a residual unit of \\ntype “building block”  (differently from the original type  “bottleneck” , it has batch \\nnormalization applied before weight layers). \\n● An additional layer normalization was added after the final self-attention block. \\n● A modified initialization was constructed as a function of the model depth. \\n● The weights of residual layers were initially sca led by a factor of 1/√n where n is the \\nnumber of residual layers. \\n● Use larger vocabulary size and context size. \\n \\nQ90 What are the differences between GPT and BERT? \\n \\n● GPT is not bidirectional and has no concept of masking \\n● BERT adds next sentence prediction task in training and so it also has a segment \\nembedding \\nQ91 What are the differences between BERT and ALBERT v2? \\n● Embedding matrix factorisation(helps in reducing no. of parameters) \\n● No dropout \\n● Parameter sharing(helps in reducing no. of parameters and regularisation) \\nQ92 How does parameter sharing in ALBERT affect the training and inference time? \\nNo effect. Parameter sharing just decreases the number of parameters. \\nQ93 How would you reduce the inference time of a trained NN model? \\n● Serve on GPU/TPU/FPGA \\n● 16 bit quantisation and served on GPU with fp16 support \\n● Pruning to reduce parameters \\n● Knowledge distillation (To a smaller transformer model or simple neural network) \\n● Hierarchical softmax/Adaptive softmax \\n● You can also cache results as explained here. \\nQ94 Would you use BPE with classical models? \\nOf course! BPE is a smart tokeniser and it can help us get a smaller vocabulary which can help \\nus find a model with less parameters. \\nQ95 How would you make an arxiv papers search engine? (I was asked — How \\nwould you make a plagiarism detector?) \\n \\nGet top k results with TF-IDF similarity and then rank results with \\n● semantic encoding + cosine similarity \\n● a model trained for ranking \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 221}),\n",
       " Document(page_content='Steve Nouri                          https://www.linkedin.com/in/stevenouri/  Q96 Get top k results with TF-IDF similarity and then rank results with \\n● semantic encoding + cosine similarity \\n● a model trained for ranking \\nQ97 How would you make a sentiment classifier? \\nThis is a trick question. The interviewee can say all things such as using transfer learning and \\nlatest models but they need to talk about having a neutral class too otherwise you can have really \\ngood accuracy/f1 and still, the model will classify everything into positive or negative. \\nThe truth is that a lot of news is neutral and so the training needs to have this class. The \\ninterviewee should also talk about how he will create a dataset and his training strategies like the \\nselection of language model, language model fine-tuning and using various datasets for multi-\\ntask learning. \\nQ98 What is the difference between regular expression and regular grammar? \\nA regular expression is the representation of natural language in the form of mathematical \\nexpressions containing a character sequence. On the other hand, regular grammar is the \\ngenerator of natural language, defining a set of defined rules and syntax which the strings in the \\nnatural language must follow. \\n \\nQ99 Why should we use Batch Normalization? \\nOnce the interviewer has asked you about the fundamentals of deep learning architectures, they \\nwould move on to the key topic of improving your deep learning model’s performance.  \\nBatch Normalization is one of the techniques used for reducing the training time of our deep \\nlearning algorithm. Just like normalizing our input helps improve our logistic regression model, we \\ncan normalize the activations of the hidden layers in our deep learning model as well: \\n \\nQ100 How is backpropagation different in RNN compared to ANN? \\nIn Recurrent Neural Networks, we have an additional loop at each node: \\nThis loop essentially includes a time component into the network as well. This helps in capturing \\nsequential information from the data, which could not be possible in a generic artificial neural \\nnetwork. \\nThis is why the backpropagation in RNN is called Backpropagation through Time, as in \\nbackpropagation at each time step. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', metadata={'source': 'ML questions dump.pdf', 'page': 222}),\n",
       " Document(page_content='Steve NouriTop 100 Questions on Computer Vision\\nBy Steve Nouri\\nQ1 Which of the following is a challenge when dealing with computer vision \\nproblems?\\nVariations due to geometric changes (like pose, scale, etc), Variations due to photometric factors \\n(like illumination, appearance, etc) and Image occlusion. All the above -mentioned options are \\nchallenges in computer vision\\nQ2 Consider an image with width and height as 100×100. Each pixel in the image \\ncan have a color from Grayscale, i.e. values. How much space would this image \\nrequire for storing?\\nThe answer will be 8x100x100 because 8 bits will be required to represent a number from 0 -256\\nQ3 Why do we use convolutions for images rather than just FC layers? \\nFirstly, convolutions preserve, encode, and actually use the spatial information from the image. If \\nwe used only FC layers we would have no relative spatial information. Secondly, Convolutional \\nNeural Networks (CNNs) have a partially built -in translation in -variance, since each convolution \\nkernel acts as it\\'s own filter/feature detector.\\nQ4 What makes CNN’s translation -invariant? \\nAs explained above, each convolution kernel acts as it\\'s own filter/feature detector. So let\\'s say \\nyou\\'re doing object detection, it doesn\\'t matter where in the image the object is since we\\'re going \\nto apply the convolution in a sliding window fashion across the entire image anyways.\\nQ5 Why do we have max -pooling in classification CNNs? \\nfor a role in Computer Vision. Max -pooling in a CNN allows you to reduce computation since your \\nfeature maps are smaller after the pooling. You don\\'t lose too much semantic information since \\nyou\\'re taking the maximum activation. There\\'s also a theory that max -pooling contributes a bit to \\ngiving CNN’s more translation in -variance. Check out this great video from Andrew Ng on the\\nbenefits of max -pooling.\\nQ6 Why do segmentation CNN’s typically have an encoder -decoder \\nstyle/structure? \\nThe encoder CNN can basically be thought of as a feature extraction network, while the decoder \\nuses that information to predict the image segments by \"decoding\" the features and upscaling to \\nthe original image size.', metadata={'source': 'ML questions dump.pdf', 'page': 223}),\n",
       " Document(page_content=\"Steve NouriQ7 What is the significance of Residual Networks? \\nThe main thing that residual connections did was allow for direct feature access from previous \\nlayers. This makes information propagation throughout the network much easier. One very \\ninteresting paper about this shows how using local skip connections gives the network a type of \\nensemble multi -path structure, giving features multiple paths to propagate throughout the \\nnetwork.\\nQ8 What is batch normalization and why does it work? \\nTraining Deep Neural Networks is complicated by the fact that the distribution of each layer's \\ninputs changes during training, as the parameters of the previous layers change. The idea is then \\nto normalize the inputs of each layer in such a way that they have a mean output activation of \\nzero and a standard deviation of one. This is done for each individual mini -batch at each layer i.e \\ncompute the mean and variance of that mini -batch alone, then normalize. This is analogous to\\nhow the inputs to networks are standardized. How does this help? We know that normalizing the \\ninputs to a network helps it learn. But a network is just a series of layers, where the output of one \\nlayer becomes the input to the next. That means we can think of any layer in a neural network as \\nthe first layer of a smaller subsequent network. Thought of as a series of neural networks feeding \\ninto each other, we normalize the output of one layer before applying the activation function and \\nthen feed it into the following layer (sub -network).\\nQ9 Why would you use many small convolutional kernels such as 3x3 rather than \\na few large ones? \\nThis is very well explained in the VGGNet paper . There are 2 reasons: First, you can use several \\nsmaller kernels rather than few large ones to get the same receptive field and capture more spatial \\ncontext, but with the smaller kernels you are using less parameters and computations. Secondly, \\nbecause with smaller kernels you will be using more filters, you'll be able to use more activation \\nfunctions and thus have a more discriminative mapping function being learned by your CNN.\\nQ10 What is Precision?\\nPrecision (also called positive predictive value) is the fraction of relevant instances among the \\nretrieved instances\\nPrecision = true positive / (true positive + false positive)\\nQ11 What is Recall?\\nRecall (also known as sensitivity) is the fraction of relevant instances that have been retrieved \\nover the total amount of relevant instances. Recall = true positive / (true positive + false negative)\\nQ12 Define F1 -score. \\nIt is the weighted average of precision and recall. It considers both false positive and false \\nnegatives into account. It is used to measure the model’s performance.\", metadata={'source': 'ML questions dump.pdf', 'page': 224}),\n",
       " Document(page_content='Steve NouriF1-Score = 2 * (precision * recall) / (precision + recall)\\nQ13 What is cost function? \\nThe cost function is a scalar function that Quantifies the error factor of the Neural Network. Lower \\nthe cost function better than the Neural network. Eg: MNIST Data set to classify the image, the \\ninput image is digit 2 and the Neural network wrongly predicts it to be 3\\nQ14 List different activation neurons or functions\\n●Linear Neuron\\n●Binary Threshold Neuron\\n●Stochastic Binary Neuron\\n●Sigmoid Neuron\\n●Tanh function\\n●Rectified Linear Unit (ReLU)\\nQ15 Define Learning rate.\\nThe learning rate is a hyper -parameter that controls how much we are adjusting the weights of \\nour network with respect to the loss gradient. \\nQ16 What is Momentum (w.r.t NN optimization)?\\nMomentum lets the optimization algorithm remembers its last step, and adds some proportion of \\nit to the current step. This way, even if the algorithm is stuck in a flat region, or a small local \\nminimum, it can get out and continue towards the true minimum.\\nQ17 What is the difference between Batch Gradient Descent and Stochastic \\nGradient Descent?\\nBatch gradient descent computes the gradient using the whole dataset. This is great for convex \\nor relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum \\nsolution, either local or global. Additionally, batch gradient descent, given an annealed learning \\nrate, will eventually find the minimum located in its basin of attraction.\\nStochastic gradient descent (SGD) computes the gradient using a single sample. SGD works well \\n(Not well, I suppose, but better than batch gradient descent) for error manifolds that have lots of \\nlocal maxima/minima. In this case, the somewhat noisier gradient calculated using the reduced \\nnumber of samples tends to jerk the model out of local minima into a region that hopefully is more \\noptimal.\\nQ18 Epoch vs Batch vs Iteration.\\nEpoch: one forward pass and one backward pass of all the training examples\\nBatch: examples processed together in one pass (forward and backward)\\nIteration: number of training examples / Batch size', metadata={'source': 'ML questions dump.pdf', 'page': 225}),\n",
       " Document(page_content='Steve NouriQ19 What is the vanishing gradient? \\nAs we add more and more hidden layers, backpropagation becomes less and less useful in \\npassing information to the lower layers. In effect, as information is passed back, the gradients \\nbegin to vanish and become small relative to the weights of the networks.\\nQ20 What are dropouts? \\nDropout is a simple way to prevent a neural network from overfitting. It is the dropping out of some \\nof the units in a neural network. It is similar to the natural reproduction process, where nature \\nproduces offsprings by combining distinct genes (dropping out others) rather than strengthening \\nthe co-adapting of them.\\nQ21 Can you explain the differences between supervised, unsupervised, and \\nreinforcement learning? \\nIn supervised learning, we train a model to learn the relationship between input data and output \\ndata. We need to have labeled data to be able to do supervised learning.\\nWith unsupervised learning, we only have unlabeled data. The model learns a representation of \\nthe data. Unsupervised learning is frequently used to initialize the parameters of the model when \\nwe have a lot of unlabeled data and a small fraction of labeled data. We first train an unsupervised \\nmodel and, after that, we use the weights of the model to train a supervised model. In \\nreinforcement learning, the model has some input data and a reward depending on the output of \\nthe model. The model learns a policy that maximizes the reward. Reinforcement learning has \\nbeen applied successfully to strategic games such as Go and even classic Atari video games.\\nQ22 What is data augmentation? Can you give some examples? \\nData augmentation is a technique for synthesizing new data by modifying existing data in such a \\nway that the target is not changed, or it is changed in a known way. Computer vision is one of the \\nfields where data augmentation is very useful. There are many modifications that we can do to \\nimages:\\n●Resize\\n●Horizontal or vertical flip\\n●Rotate, Add noise, Deform\\n●Modify colors Each problem needs a customized data augmentation pipeline. For \\nexample, on OCR, doing flips will change the text and won’t be beneficial; h owever, \\nresizes and small rotations may help.\\nQ23 What are the components of GAN? \\n●Generator\\n●Discriminator\\nQ24 What’s the difference between a generative and discriminative model?\\nA generative model will learn categories of data while a discriminative model will simply learn the \\ndistinction between different categories of data. Discriminative models will generally outperform \\ngenerative models on classification tasks .', metadata={'source': 'ML questions dump.pdf', 'page': 226}),\n",
       " Document(page_content='Steve NouriQ25 What is Linear Filtering?\\nLinear filtering is a neighborhood operation, which means that the output of a pixel’s value is \\ndecided by the weighted sum of the values of the input pixels.\\nQ26 How can you achieve Blurring through Gaussian Filter?\\nThis is the most common technique for blurring or smoothing an image. This filter improves the \\nresulting pixel found at the center and slowly minimizes the effects as pixels move away from the \\ncenter. This filter can also help in removing noise in an image\\nQ27 What is Non -Linear Filtering? How it is used?\\nLinear filtering is easy to use and implement. In some cases, this method is enough to get the \\nnecessary output. However, an increase in performance can be obtained through non -linear \\nfiltering. Through non -linear filtering, we can have more control and achieve better results when\\nwe encounter a more complex computer vision task.\\nQ28 Explain Median Filtering.\\nThe median filter is an example of a non -linear filtering technique. This technique is commonly \\nused for minimizing the noise in an image. It operates by inspecting the image pixel by pixel and \\ntaking the place of each pixel’s value with the value of the neighboring pixel median.\\nSome techniques in detecting and matching features are:\\n●Lucas-Kanade\\n●Harris\\n●Shi-Tomasi\\n●SUSAN (smallest uni value segment assimilating nucleus)\\n●MSER (maximally stable extremal regions)\\n●SIFT (scale -invariant feature transform)\\n●HOG (histogram of oriented gradients)\\n●FAST (features from accelerated segment test)\\n●SURF (speeded -up robust features)\\nQ29 Describe the Scale Invariant Feature Transform (SIFT) algorithm\\nSIFT solves the problem of detecting the corners of an object even if it is scaled. Steps to \\nimplement this algorithm:\\n●Scale-space extrema detection –This step will identify the locations and scales that can \\nstill be recognized from different angles or views of the same object in an image.\\n●Keypoint localization –When possible key points are located, they would be refined to get \\naccurate results. This would result in the elimination of points that are low in contrast or \\npoints that have edges that are deficiently localized.\\n●Orientation assignment –In this step, a consistent orientation is assigned to each key \\npoint to attain invariance when the image is being rotated.\\n●Keypoint matching –In this step, the key points between images are now linked to \\nrecognizing their nearest neighbors.', metadata={'source': 'ML questions dump.pdf', 'page': 227}),\n",
       " Document(page_content='Steve NouriQ30 Why Speeded -Up Robust Features (SURF) came into existence?\\nSURF was introduced to as a speed -up version of SIFT. Though SIFT can detect and describe \\nkey points of an object in an image, still this algorithm is slow.\\nQ31 What is Oriented FAST and rotated BRIEF (ORB)?\\nThis algorithm is a great possible substitute for SIFT and SURF, mainly because it performs better \\nin computation and matching. It is a combination of fast keypoint detector and brief descriptor, \\nwhich contains a lot of alterations to improve performance. It is also a great alternative in terms \\nof cost because the SIFT and SURF algorithms are patented, which means that you need to buy \\nthem for their utilization.\\nQ32 What is image segmentation?\\nIn computer vision, segmentation is the process of extracting pixels in an image that is related. \\nSegmentation algorithms usually take an image and produce a group of contours (the boundary \\nof an object that has well -defined edges in an image) or a mask where a set of related pixels are \\nassigned to a unique color value to identify it.\\nPopular image segmentation techniques:\\n●Active contours\\n●Level sets\\n●Graph-based merging\\n●Mean Shift\\n●Texture and intervening contour -based normalized cuts\\nQ33 What is the purpose of semantic segmentation?\\nThepurpose of semantic segmentation is to categorize every pixel of an image to a certain class \\nor label. In semantic segmentation, we can see what is the class of a pixel by simply looking \\ndirectly at the color, but one downside of this is that we cannot identify if two colored masks belong \\nto a certain object.\\nQ34 Explain instance segmentation.\\nIn semantic segmentation, the only thing that matters to us is the class of each pixel. This would \\nsomehow lead to a problem that we cannot identify if that class belongs to the same object or not. \\nSemantic segmentation cannot identify if two objects in an image are separate entities. So to \\nsolve this problem, instance segmentation was created. This segmentation can identify two \\ndifferent objects of the same class. For example, if an image has two sheep in it, the sheep will \\nbe detected and masked with different colors to differentiate what instance of a class they belong \\nto. \\nQ35 How is panoptic segmentation different from semantic/instance \\nsegmentation?\\nPanoptic segmentation is basically a union of semantic and instance segmentation. In panoptic \\nsegmentation , every pixel is classified by a certain class and those pixels that have several \\ninstances of a class are also determined. For example, if an image has two cars, these cars will ', metadata={'source': 'ML questions dump.pdf', 'page': 228}),\n",
       " Document(page_content='Steve Nouribe masked with different colors. These colors represent the same class —car —but point to \\ndifferent instances of a certain class.\\nQ36 Explain the problem of recognition in computer vision.\\nRecognition is one of the toughest challenges in the concepts in computer vision. Why is \\nrecognition hard? For the human eyes, recognizing an object’s features or attributes would be \\nvery easy. Humans can recognize multiple objects with very small effort. However, this does not \\napply to a machine. It would be very hard for a machine to recognize or detect an object because \\nthese objects vary. They vary in terms of viewpoints, sizes, or scales. Though these things are \\nstill challenges faced by most computer vision systems, they are still making advancements or \\napproaches for solving these daunting tasks.\\nQ37 What is Object Recognition?\\nObject recognition is used for indicating an object in an image or video. This is a product of \\nmachine learning and deep learning algorithms. Object recognition tries to acquire this innate \\nhuman ability, which is to understand certain features or visual detail of an image.\\nQ38 What is Object Detection and it’s real -life use cases?\\nObject detection in computer vision refers to the ability of machines to pinpoint the location of an \\nobject in an image or video. A lot of companies have been using object detection techniques in \\ntheir system. They use it for face detection, web images, and security purposes.\\nQ39 Describe Optical Flow, its uses, and assumptions.\\nOptical flow is the pattern of apparent motion of image objects between two consecutive frames \\ncaused by the movement of object or camera. It is a 2D vector field where each vector is a \\ndisplacement vector showing the movement of points from the first frame to the second\\nOptical flow has many applications in areas like :\\n●Structure from Motion\\n●Video Compression\\n● Video Stabilization …\\nOptical flow works on several assumptions:\\n1. The pixel intensities of an object do not change between consecutive frames.\\n2. Neighboring pixels have similar motion.\\nQ40 What is Histogram of Oriented Gradients (HOG)?\\nHOG stands for Histograms of Oriented Gradients. HOG is a type of “feature descriptor”. The \\nintent of a feature descriptor is to generalize the object in such a way that the same object (in this \\ncase a person) produces as close as possible to the same feature descriptor when viewed under \\ndifferent conditions. This makes the classification task easier.', metadata={'source': 'ML questions dump.pdf', 'page': 229}),\n",
       " Document(page_content='Steve NouriQ41 What is BOV: Bag -of-visual-words (BOV)?\\nBOV also called the bag of keypoints, is based on vector quantization. Similar to HOG features, \\nBOV features are histograms that count the number of occurrences of certain patterns within a \\npatch of the image.\\nQ42 What is Poselets? Where are poselets used?\\nPoselets rely on manually added extra keypoints such as “right shoulder”, “left shoulder”, “right \\nknee” and “left knee”. They were originally used for human pose estimation\\nQ43 Explain Textons in context of CNNs\\nA texton is the minimal building block of vision. The computer vision literature does not give a \\nstrict definition for textons, but edge detectors could be one example. One might argue that deep \\nlearning techniques with Convolution Neuronal Networks (CNNs) learn textons in the first filters.\\nQ44 What is Markov Random Fields (MRFs)?\\nMRFs are undirected probabilistic graphical models which are a wide -spread model in computer \\nvision. The overall idea of MRFs is to assign a random variable for each feature and a random \\nvariable for each pixel\\nQ45 Explain the concept of superpixel?\\nA superpixel is an image patch that is better aligned with intensity edges than a rectangular patch. \\nSuperpixels can be extracted with any segmentation algorithm, however, most of them produce \\nhighly irregular superpixels, with widely varying sizes and shapes. A more regular space \\ntessellation may be desired.\\nQ46 What is Non -maximum suppression(NMS) and where is it used?\\nNMS is often used along with edge detection algorithms. The image is scanned along the image \\ngradient direction, and if pixels are not part of the local maxima they are set to zero. It is widely \\nused in object detection algorithms.\\nQ47 Describe the use of Computer Vision in Healthcare.\\nComputer vision has also been an important part of advances in health -tech. Computer vision \\nalgorithms can help automate tasks such as detecting cancerous moles in skin images or finding \\nsymptoms in x -ray and MRI scans.\\nQ48 Describe the use of Computer Vision in Augmented Reality & Mixed Reality\\nComputer vision also plays an important role in augmented and mixed reality, the technology that \\nenables computing devices such as smartphones, tablets, and smart glasses to overlay and \\nembed virtual objects on real -world imagery. Using computer vision, AR gear detects objects in \\nthe real world in order to determine the locations on a device’s display to place a virtual object. \\nFor instance, computer vision algorithms can help AR applications detect planes such as ', metadata={'source': 'ML questions dump.pdf', 'page': 230}),\n",
       " Document(page_content='Steve Nouritabletops, walls, and floors, a very important part of establishing depth and dimensions and \\nplacing virtual objects in the physical world.\\nQ49 Describe the use of Computer Vision in Facial Recognition\\nComputer vision also plays an important role in facial recognition applications, the technology that \\nenables computers to match images of people’s faces to their identities. Computer vision \\nalgorithms detect facial features in images and compare them with databases of face profiles. \\nConsumer devices use facial recognition to authenticate the identities of their owners. Social \\nmedia apps use facial recognition to detect and tag users. Law enforcement agencies also rely \\non facial recognition technology to identify criminals in video feeds.\\nQ50 Describe the use of Computer Vision in Self -Driving Cars\\nComputer vision enables self -driving cars to make sense of their surroundings. Cameras capture \\nvideo from different angles around the car and feed it to computer vision software, which then \\nprocesses the images in real -time to find the extremities of roads, read traffic signs, detect other \\ncars, objects, and pedestrians. The self -driving car can then steer its way on streets and \\nhighways, avoid hitting obstacles, and (hopefully) safely drive its passengers to their destination.\\nQ51 Explain famous Computer Vision tasks using a single image example.\\nMany popular computer vision applications involve trying to recognize things in photographs; for \\nexample:\\nObject Classification: What broad category of object is in this photograph?\\nObject Identification: Which type of a given object is in this photograph?\\nObject Verification: Is the object in the photograph?\\nObject Detection: Where are the objects in the photograph?\\nObject Landmark Detection: What are the key points for the object in the photograph?\\nObject Segmentation: What pixels belong to the object in the image?\\nObject Recognition: What objects are in this photograph and where are they?\\nQ52 Explain the distinction between Computer Vision and Image Processing.\\nComputer vision is distinct from image processing.\\nImage processing is the process of creating a new image from an existing image, typically \\nsimplifying or enhancing the content in some way. It is a type of digital signal processing and is \\nnot concerned with understanding the content of an image.\\nA given computer vision system may require image processing to be applied to raw input, e.g. \\npre-processing images.\\nExamples of image processing include:\\n●Normalizing photometric properties of the image, such as brightness or color.\\n●Cropping the bounds of the image, such as centering an object in a photograph.\\n●Removing digital noise from an image, such as digital artifacts from low light levels.', metadata={'source': 'ML questions dump.pdf', 'page': 231}),\n",
       " Document(page_content='Steve NouriQ53 Explain business use cases in computer vision.\\n●Optical character recognition (OCR)\\n●Machine inspection\\n●Retail (e.g. automated checkouts)\\n●3D model building (photogrammetry)\\n●Medical imaging\\n●Automotive safety\\n●Match move (e.g. merging CGI with live actors in movies)\\n●Motion capture (mocap)\\n●Surveillance\\n●Fingerprint recognition and biometrics\\nQ54 What is the Boltzmann Machine?\\nOne of the most basic Deep Learning models is a Boltzmann Machine, resembling a simplified \\nversion of the Multi -Layer Perceptron. This model features a visible input layer and a hidden layer \\n-- just a two -layer neural net that makes stochastic decisions as to whether a neuron should be \\non or off. Nodes are connected across layers, but no two nodes of the same layer are connected.\\nQ56 What Is the Role of Activation Functions in a Neural Network?\\nAt the most basic level, an activation function decides whether a neuron should be fired or not. It \\naccepts the weighted sum of the inputs and bias as input to any activation function. Step function, \\nSigmoid, ReLU, Tanh, and Softmax are examples of activation functions.\\nQ57 What Is the Difference Between a Feedforward Neural Network and Recurrent \\nNeural Network?\\nA Feedforward Neural Network signals travel in one direction from input to output. There are no \\nfeedback loops; the network considers only the current input. It cannot memorize previous inputs \\n(e.g., CNN).\\nQ58 What Are the Applications of a Recurrent Neural Network (RNN)?\\nTheRNNcan be used for sentiment analysis, text mining, and image captioning. Recurrent Neural \\nNetworks can also address time series problems such as predicting the prices of stocks in a \\nmonth or quarter.\\nQ59 What Are the Softmax and ReLU Functions?\\nSoftmax is an activation function that generates the output between zero and one. It divides each \\noutput, such that the total sum of the outputs is equal to one. Softmax is often used for output \\nlayers.', metadata={'source': 'ML questions dump.pdf', 'page': 232}),\n",
       " Document(page_content='Steve NouriQ60 What Are Hyperparameters?\\nWith neural networks, you’re usually working with hyperparameters once the data is formatted \\ncorrectly. A hyperparameter is a parameter whose value is set before the learning process begins. \\nIt determines how a network is trained and the structure of the network (such as the number of \\nhidden units, the learning rate, epochs, etc.).\\nQ61 What Will Happen If the Learning Rate Is Set Too Low or Too High?\\nWhen your learning rate is too low, training of the model will progress very slowly as we are \\nmaking minimal updates to the weights. It will take many updates before reaching the minimum \\npoint. If the learning rate is set too high, this causes undesirable divergent behavior to the loss \\nfunction due to drastic updates in weights. It may fail to converge (model can give a good output) \\nor even diverge (data is too chaotic for the network to train).\\nQ62 How Are Weights Initialized in a Network?\\nThere are two methods here: we can either initialize the weights to zero or assign them randomly.\\nInitializing all weights to 0: This makes your model similar to a linear model. All the neurons and \\nevery layer perform the same operation, giving the same output and making the deep net useless.\\nInitializing all weights randomly: Here, the weights are assigned randomly by initializing them very \\nclose to 0. It gives better accuracy to the model since every neuron performs different \\ncomputations. This is the most commonly used method.\\nQ63 What Are the Different Layers on CNN?\\nThere are four layers in CNN:\\n1.Convolutional Layer -the layer that performs a convolutional operation, creating several \\nsmaller picture windows to go over the data.\\n2. ReLU Layer -it brings non -linearity to the network and converts all the negative pixels to \\nzero. The output is a rectified feature map.\\n3. Pooling Layer -pooling is a down -sampling operation that reduces the dimensionality of \\nthe feature map.\\n4. Fully Connected Layer -this layer recognizes and classifies the objects in the image.\\nQ64 What is Pooling on CNN, and How Does It Work?\\nPooling is used to reduce the spatial dimensions of a CNN. It performs down -sampling operations \\nto reduce the dimensionality and creates a pooled feature map by sliding a filter matrix over the \\ninput matrix.\\nQ65 How Does an LSTM Network Work?\\nLong-Short-Term Memory (LSTM) is a special kind of recurrent neural network capable of learning \\nlong-term dependencies, remembering information for long periods as its default behavior. There \\nare three steps in an LSTM network:', metadata={'source': 'ML questions dump.pdf', 'page': 233}),\n",
       " Document(page_content=\"Steve Nouri●Step 1: The network decides what to forget and what to remember.\\n●Step 2: It selectively updates cell state values.\\n●Step 3: The network decides what part of the current state makes it to the output.\\nQ66 What Is the Difference Between Epoch, Batch, and Iteration in Deep Learning?\\n●Epoch -Represents one iteration over the entire dataset (everything put into the training \\nmodel).\\n●Batch -Refers to when we cannot pass the entire dataset into the neural network at once, \\nso we divide the dataset into several batches.\\n●Iteration -if we have 10,000 images as data and a batch size of 200. then an epoch should \\nrun 50 iterations (10,000 divided by 50).\\nQ67 Why Is Tensorflow the Most Preferred Library in Deep Learning?\\nTensorflow provides both C++ and Python APIs, making it easier to work on and has a faster \\ncompilation time compared to other Deep Learning libraries like Keras and Torch. Tensorflow \\nsupports both CPU and GPU computing devices.\\nQ68 What Do You Mean by Tensor in Tensorflow?\\nA tensor is a mathematical object represented as arrays of higher dimensions. These arrays of \\ndata with different dimensions and ranks fed as input to the neural network are called “Tensors.”\\nQ69 Explain a Computational Graph.\\nEverything in TensorFlow is based on creating a computational graph. It has a network of nodes \\nwhere each node operates, Nodes represent mathematical operations, and edges represent \\ntensors. Since data flows in the form of a graph, it is also called a “DataFlow Graph.”\\nQ70 What Is an Auto -encoder?\\nThis Neural Network has three layers in which the input neurons are equal to the output neurons. \\nThe network's target outside is the same as the input. It uses dimensionality reduction to \\nrestructure the input. It works by compressing the image input to a latent space representation \\nthen reconstructing the output from this representation.\\nQ71 Can we have the same bias for all neurons of a hidden layer?\\nEssentially, you can have a different bias value at each layer or at each neuron as well. However, \\nit is best if we have a bias matrix for all the neurons in the hidden layers as well.\\nA point to note is that both these strategies would give you very different results.\\nQ72 In a neural network, what if all the weights are initialized with the same value?\\nIn simplest terms, if all the neurons have the same value of weights, each hidden unit will get \\nexactly the same signal. While this might work during forward propagation, the derivative of the \\ncost function during backward propagation would be the same every time.\", metadata={'source': 'ML questions dump.pdf', 'page': 234}),\n",
       " Document(page_content='Steve NouriIn short, there is no learning happening by the network! What do you call the phenomenon of the \\nmodel being unable to learn any patterns from the data? Yes, underfitting .\\nTherefore, if all weights have the same initial value, this would lead to underfitting.\\nQ73 What is the role of weights and bias in a neural network?\\nThis is a question best explained with a real -life example. Consider that you want to go out today \\nto play a cricket match with your friends. Now, a number of factors can affect your decision -\\nmaking, like:\\n●How many of your friends can make it to the game?\\n●How much equipment can all of you bring?\\n●What is the temperature outside?\\nAnd so on. These factors can change your decision greatly or not too much. For example, if it is \\nraining outside, then you cannot go out to play at all. Or if you have only one bat, you can share \\nit while playing as well. The magnitude by which these factors can affect the game is called the \\nweight of that factor.\\nFactors like the weather or temperature might have a higher weight, and other factors like \\nequipment would have a lower weight.\\nQ74 Why does a Convolutional Neural Network (CNN) work better with image data?\\nThe key to this question lies in the Convolution operation. Unlike humans, the machine sees the \\nimage as a matrix of pixel values. Instead of interpreting a shape like a petal or an ear, it just \\nidentifies curves and edges.\\nThus, instead of looking at the entire image, it helps to just read the image in parts. Doing this for \\na 300 x 300 -pixel image would mean dividing the matrix into smaller 3 x 3 matrices and dealing \\nwith them one by one. This is convolution.\\nQ75 Why do RNNs work better with text data?\\nThe main component that differentiates Recurrent Neural Networks (RNN) from the other models \\nis the addition of a loop at each node. This loop brings the recurrence mechanism in RNNs. In a \\nbasic Artificial Neural Network (ANN), each input is given the same weight and fed to the network \\nat the same time. So, for a sentence like “I saw the movie and hated it”, it would be diffi cult to \\ncapture the information which associates “it” with the “movie”.\\nQ76 In a CNN, if the input size 5 X 5 and the filter size is 7 X 7, then what would be \\nthe size of the output?\\nThis is a pretty intuitive answer. As we saw above, we perform the convo lution on ‘x’ one step at \\na time, to the right, and in the end, we got Z with dimensions 2 X 2, for X with dimensions 3 X 3.\\nThus, to make the input size similar to the filter size, we make use of padding –adding 0s to the \\ninput matrix such that its new size becomes at least 7 X 7. Thus, the output size would be using \\nthe formula:\\nDimension of image = (n, n) = 5 X 5\\nDimension of filter = (f,f)  = 7 X 7\\nPadding = 1 (adding 1 pixel with value 0 all around the edges)\\nDimension of output will be (n+2p -f+1) X (n+2p -f+1) = 1 X 1', metadata={'source': 'ML questions dump.pdf', 'page': 235}),\n",
       " Document(page_content='Steve NouriQ77 What’s the difference between valid and same padding in a CNN?\\nThis question has more chances of being a follow -up question to the previous one. Or if you have \\nexplained how you used CNNs in a computer vision task, the interviewer might ask this question \\nalong with the details of the padding parameters.\\n●Valid Padding: When we do not use any padding. The resultant matrix after convolution \\nwill have dimensions (n –f + 1) X (n –f + 1)\\n●Same padding: Adding padded elements all around the edges such that the output matrix \\nwill have the same dimensions as that of the input matrix\\nQ78 What are the applications of transfer learning in Deep Learning?\\nI am sure you would have a doubt as to why a relatively simple question was included in the \\nIntermediate Level. The reason is the sheer volume of subsequent questions it can generate! The\\nuse oftransfer learning has been one of the key milestones in deep learning. Training a large \\nmodel on a huge dataset, and then using the final parameters on smaller simpler datasets has \\nled to defining breakthroughs in the form of Pretrained Models. Be it Computer Vision or NLP, \\npretrained models have become the norm in research and in the industry. Some popular \\nexamples include BERT, ResNet, GPT -2, VGG-16, etc, and many more.\\nQ79 Why is GRU faster as compared to LSTM?\\nAs you can see, the LSTM model can become quite complex. In order to still retain the \\nfunctionality of retaining information across time and yet not make a too complex model, we need \\nGRUs. Basically, in GRUs, instead of having an additional Forget gate, we combine the input and \\nForget gates into a single Update Gate:\\nQ80 How is the transformer architecture better than RNN?\\nAdvancements in deep learning have made it possible to solve many tasks in Natural Language \\nProcessing. Networks/Sequence models like RNNs, LSTMs, etc. are specifically used for this \\npurpose –so as to capture all possible information from a given sentence, or a paragraph. \\nHowever, sequential processing comes with its caveats:\\n●It requires high processing power\\n●It is difficult to execute in parallel because of its sequential nature\\nQ81 How Can We Scale GANs Beyond Image Synthesis?\\nAside from applications like image -to-image translation and domain -adaptation most GAN \\nsuccesses have been in image synthesis. Attempts to use GANs beyond images have focused \\non three domains: Text, Structured Data and Audio \\nQ82 How Should we Evaluate GANs and When Should We Use Them?\\nWhen it comes to evaluating GANs, there are many proposals but little consensus. Suggestions \\ninclude:', metadata={'source': 'ML questions dump.pdf', 'page': 236}),\n",
       " Document(page_content=\"Steve Nouri●Inception Score and FID -Both these scores use a pre -trained image classifier and both \\nhave known issues. A common criticism is that these scores measure ‘sample quality’ and \\ndon’t really capture ‘sample diversity’.\\n●MS-SSIM -propose using MS -SSIM to separately evaluate diversity, but this technique \\nhas some issues and hasn’t really caught on.\\n●AIS - propose putting a Gaussian observation model on the outputs of a GAN and using \\nannealed importance sampling to estimate the log -likelihood under this model, but show \\nthat estimates computed this way are inaccurate in the case where the GAN generator is \\nalso a flow model The generator being a flow model allows for the computation of exact \\nlog-likelihoods in this case.\\n●Geometry Score -suggest computing geometric properties of the generated data manifold \\nand comparing those properties to the real data.\\n●Precision and Recall -attempt to measure both the ‘precision’ and ‘recall’ of GANs.\\n●Skill Rating -have shown that trained GAN discriminators can contain useful information \\nwith which evaluation can be performed.\\nQ83 What should we use GANs for? \\nIf you want an actual density model, GANs probably isn’t the best choice. There is now good \\nexperimental evidence that GANs learn a ‘low support’ representation of the target dataset, which \\nmeans there may be substantial parts of the test set to which a GAN (implicitly) assigns zero \\nlikelihood. \\nQ84 How should we evaluate GANs on these perceptual tasks?\\nIdeally, we would just use a human judge, but this is expensive. A cheap proxy is to see if a \\nclassifier can distinguish between real and fake examples. This is called a classifier two -sample \\ntest (C2STs). The main issue with C2STs is that if the Generator has even a minor defect that’s \\nsystematic across samples (e.g., ) this will dominate the evaluation. \\nQ85 Explain the problem of Vanishing Gradients in GANs\\nResearch has suggested that if your discriminator is too good, then generator training can fail due \\ntovanishing gradients . In effect, an optimal discriminator doesn't provide enough information for \\nthe generator to make progress.\\nAttempts to Remedy\\n●Wasserstein loss: The Wasserstein loss is designed to prevent vanishing gradients even \\nwhen you train the discriminator to optimality.\\n●Modified minimax loss: The original GAN paper proposed a modification to minimax loss\\nto deal with vanishing gradients.\\nQ86 What is Mode Collapse and why it is a big issue?\\nUsually, you want your GAN to produce a wide variety of outputs. You want, for example, a \\ndifferent face for every random input to your face generator.\\nHowever, if a generator produces an especially plausible output, the generator may learn to \\nproduce only that output. In fact, the generator is always trying to find the one output that seems \\nmost plausible to the discriminator.\", metadata={'source': 'ML questions dump.pdf', 'page': 237}),\n",
       " Document(page_content=\"Steve NouriIf the generator starts producing the same output (or a small set of outputs) over and over again, \\nthe discriminator's best strategy is to learn to always reject that output. But if the next generation \\nof discriminator gets stuck in a local minimum and doesn't find the best strategy, then it's too easy \\nfor the next generator iteration to find the most plausible output for the current discriminator.\\nEach iteration of generator over -optimizes for a particular discriminator and the discriminator \\nnever manages to learn its way out of the trap. As a result, the generators rotate through a small \\nset of output types. This form of GAN failure is called mode collapse.\\nQ87 ExplainProgressive GANs\\nIn a progressive GAN, the generator's first layers produce very low resolution images, and \\nsubsequent layers add details. This technique allows the GAN to train more quickly than \\ncomparable non -progressive GANs, and produces higher resolution images.\\nQ88 Explain Conditional GANs\\nConditional GANs train on a labeled data set and let you specify the label for each generated \\ninstance. For example, an unconditional MNIST GAN would produce random digits, while a \\nconditional MNIST GAN would let you specify which digit the GAN should generate.\\nInstead of modeling the joint probability P(X, Y), conditional GANs model the conditional \\nprobability P(X | Y).\\nFor more information about conditional GANs, see Mirza et al, 2014 .\\nQ89 Explain Image -to-Image Translation\\nImage-to-Image translation GANs take an image as input and map it to a generated output image \\nwith different properties. For example, we can take a mask image with blob of color in the shape \\nof a car, and the GAN can fill in the shape with photorealistic car details.\\nQ90 Explain CycleGAN\\nCycleGANs learn to transform images from one set into images that could plausibly belong to \\nanother set. For example, a CycleGAN produced the righthand image below when given the \\nlefthand image as input. It took an image of a horse and turned it into an image of a zebra.\\nQ91 What is Super -resolution?\\nSuper-resolution GANs increase the resolution of images, adding detail where necessary to fill in \\nblurry areas. For example, the blurry middle image below is a downsampled version of the original \\nimage on the left. Given the blurry image, a GAN produced the sharper image on the right:\\nQ92 Explain different problems in GANs\\nMany GAN models suffer the following major problems:\\n●Non-convergence: the model parameters oscillate, destabilize and never converge,\\n●Mode collapse: the generator collapses which produces limited varieties of samples,\", metadata={'source': 'ML questions dump.pdf', 'page': 238}),\n",
       " Document(page_content='Steve Nouri●Diminished gradient: the discriminator gets too successful that the generator gradient \\nvanishes and learns nothing,\\n●Unbalance between the generator and discriminator causing overfitting, &\\n●Highly sensitive to the hyperparameter selections.\\nQ93 Describe Cost v.s. image quality in GANS?\\nIn a discriminative model, the loss measures the accuracy of the prediction and we use it to \\nmonitor the progress of the training. However, the loss in GAN measures how well we are doing \\ncompared with our opponent. Often, the generator cost increases but the image quality is actually \\nimproving. We fall back to examine the generated images manually to verify the progress. This \\nmakes model comparison harder which leads to difficulties in picking the best model in a single \\nrun. It also complicates the tuning process.\\nQ94 Why Singular Value Decomposition (SVD) is used in Computer Vision?\\nThe singular value decomposition is the most common and useful decomposition in computer \\nvision. The goal of computer vision is to explain the three -dimensional world through two -\\ndimensional pictures.\\nQ95 What Is Image Transform?\\nAn image can be expanded in terms of a discrete set of basis arrays called basis images. Hence, \\nthese basis images can be generated by unitary matrices. An NxN image can be viewed as an \\nN^2×1 vector. It provides a set of coordinates or basis vectors for vector space.\\nQ96 List The Hardware Oriented Color Models?\\nThey are as follows.\\n–RGB model\\n–CMY model\\n–YIQ model\\n–HSI model\\nQ96 What Is The Need For Transform?\\nAnswer: The need for transform is most of the signals or images are time -domain signal (ie) \\nsignals can be measured with a function of time. This representation is not always best. Any \\nperson of the mathematical transformations is applied to the signal or images to obtain further \\ninformation from that signal. Particularly, for image processing.\\nQ97 What is FPN?\\nFeature Pyramid Network (FPN) is a feature extractor designed with a feature pyramid concept \\nto improve accuracy and speed. Images are first to pass through the CNN pathway, yielding \\nsemantically rich final layers. Then to regain better resolution, it creates a top -down pathway by \\nupsampling this feature map.', metadata={'source': 'ML questions dump.pdf', 'page': 239}),\n",
       " Document(page_content='Top  100 Interview Questions on Cloud\\nComputing Services\\nBySteve Nouri\\nQ1 Which are the different layers that define cloudarchitecture?\\nAns. Below mentioned are the different layers thatare used by cloud architecture:\\n●Cluster Controller\\n●SC or Storage Controller\\n●NC or Node Controller\\n●CLC or Cloud Controller\\n●Walrus\\nQ2 Explain Cloud Service Models?\\nAns. There are three types of Cloud Service Models:\\n●Infrastructure as a service (IaaS)\\n●Platform as a service (PaaS)\\n●Software as a service (SaaS)\\nQ3 What are Hybrid clouds?\\nAns.Hybridcloudsaremadeupofbothpubliccloudsandprivateclouds.Itispreferredover\\nboththecloudsbecauseitappliesthemostrobustapproachtoimplementcloudarchitecture.\\nThehybridcloudhasfeaturesandperformanceofbothprivateandpubliccloud.Ithasan\\nimportantfeaturewherethecloudcanbecreatedbyanorganizationandthecontrolofitcanbe\\ngiven to some other organization.\\nQ4 Explain Platform as a Service (Paas)?\\nAns.Itisalsoalayerincloudarchitecture.PlatformasaServiceisresponsibletoprovide\\ncompletevirtualizationoftheinfrastructurelayer,makeitlooklikeasingleserverandinvisible\\nfor the outside world.\\nQ5 What is the difference in cloud computing and MobileCloud computing?\\nAns.Mobilecloudcomputingandcloudcomputinghasthesameconcept.Thecloudcomputing\\nbecomesactivewhenswitchedfromthemobile.Moreover,mostofthetaskscanbeperformed\\nwiththehelpofmobile.Theseapplicationsrunonthemobileserverandproviderightstothe\\nuser to access and manage storage.\\nSteve Nouri\\n', metadata={'source': 'ML questions dump.pdf', 'page': 240}),\n",
       " Document(page_content='Q6 What are the security aspects provided with thecloud?\\nAns. There are 3 types ofCloud Computing Security:\\n●Identity Management: It authorizes the applicationservices.\\n●AccessControl:Theuserneedspermissionsothattheycancontroltheaccessof\\nanother user who is entering into the cloud environment.\\n●AuthenticationandAuthorization:Allowsonlytheauthorizedandauthenticatedtheuser\\nonly to access the data and applications\\nQ7 What are system integrators in cloud computing?\\nSystemIntegratorsemergedintothescenein2006.Systemintegrationisthepracticeof\\nbringingtogethercomponentsofasystemintoawholeandmakingsurethatthesystem\\nperforms smoothly.\\nA person or a company which specializes in systemintegration is called as a system integrator.\\nQ8 What is the usage of utility computing?\\nUtilitycomputing,orTheComputerUtility,isaserviceprovisioningmodelinwhichaservice\\nprovidermakescomputingresourcesandinfrastructuremanagementavailabletothecustomer\\nas needed and charges them for specific usage ratherthan a flat rate\\nUtilitycomputingisaplug-inmanagedbyanorganizationwhichdecideswhattypeofservices\\nhas to be deployed from the cloud. It facilitatesusers to pay only for what they use.\\nQ9 What are some large cloud providers and databases?\\nFollowing are the most used large cloud providersand databases:\\n– Google BigTable\\n– Amazon SimpleDB\\n– Cloud-based SQL\\nQ10 Explain the difference between cloud and traditionaldata centers.\\nInatraditionaldatacenter,themajordrawbackistheexpenditure.Atraditionaldatacenteris\\ncomparativelyexpensiveduetoheating,hardware,andsoftwareissues.So,notonlyisthe\\ninitial cost higher, but the maintenance cost is alsoa problem.\\nCloudbeingscaledwhenthereisanincreaseindemand.Mostlytheexpenditureisonthe\\nmaintenance of the data centers, while these issuesare not faced in cloud computing.\\nQ11 What do you mean by CaaS?\\nCaaSisaterminologyusedinthetelecomindustryasCommunicationAsaService.CaaS\\nofferstotheenterpriseuserfeaturessuchasdesktopcallcontrol,unifiedmessaging,and\\ndesktop faxing.\\nSteve Nouri\\n', metadata={'source': 'ML questions dump.pdf', 'page': 241}),\n",
       " Document(page_content='Q12 What is hypervisor in Cloud Computing?\\nAns:Itisavirtualmachinescreenthatcanlogicallymanageresourcesforvirtualmachines.It\\nallocates,partition,isolateorchangewiththeprogramgivenasvirtualizationhypervisor.\\nHardwarehypervisorallowshavingmultipleguestOperatingSystemsrunningonasinglehost\\nsystem at the same time.\\nQ13 Define what MultiCloud is?\\nMulticloudcomputingmaybedefinedasthedeliberateuseofthesametypeofcloudservices\\nfrom multiple public cloud providers.\\nQ14 What is a multi-cloud strategy?\\nThewaymostorganizationsadoptthecloudisthattheytypicallystartwithoneprovider.They\\nthencontinuedownthatpathandeventuallybegintogetalittleconcernedaboutbeingtoo\\ndependentononevendor.Sotheywillstartentertainingtheuseofanotherprovideroratleast\\nallowing people to use another provider.\\nTheymayevenuseafunctionality-basedapproach.Forexample,theymayuseAmazonas\\ntheirprimarycloudinfrastructureprovider,buttheymaydecidetouseGoogleforanalytics,\\nmachinelearning,andbigdata.Sothistypeofmulti-cloudstrategyisdrivenbysourcingor\\nprocurement(andperhapsonspecificcapabilities),butitdoesn’tfocusonanythingintermsof\\ntechnology and architecture.\\nQ15 What is meant by Edge Computing, and how is itrelated to the cloud?\\nUnlikecloudcomputing,edgecomputingisallaboutthephysicallocationandissuesrelatedto\\nlatency.Cloudandedgearecomplementaryconceptscombiningthestrengthsofacentralized\\nsystemwiththeadvantagesofdistributedoperationsatthephysicallocationwherethingsand\\npeople connect.\\nDisadvantages of SaaS cloud computing layer\\n1) Security\\nActually,dataisstoredinthecloud,sosecuritymaybeanissueforsomeusers.However,cloud\\ncomputing is not more secure than in-house deployment.\\n2) Latency issue\\nSincedataandapplicationsarestoredinthecloudatavariabledistancefromtheend-user,\\nthereisapossibilitythattheremaybegreaterlatencywheninteractingwiththeapplication\\ncomparedtolocaldeployment.Therefore,theSaaSmodelisnotsuitableforapplicationswhose\\ndemand response time is in milliseconds.\\n3) Total Dependency on Internet\\nWithout an internet connection, most SaaS applicationsare not usable.\\n4) Switching between SaaS vendors is difficult\\nSwitchingSaaSvendorsinvolvesthedifficultandslowtaskoftransferringtheverylargedata\\nfiles over the internet and then converting and importingthem into another SaaS also.\\nSteve Nouri', metadata={'source': 'ML questions dump.pdf', 'page': 242}),\n",
       " Document(page_content='Q16 What is IaaS in Cloud Computing?\\nAns:IaaSi.e.InfrastructureasaServicewhichisalsoknownasHardwareasaService.Inthis\\ntypeofmodel,organizationsusuallygivestheirITinfrastructuresuchasservers,processing,\\nstorage,virtualmachinesandotherresources.Customerscanaccesstheresourcesveryeasily\\non internet using on-demand pay model.\\nQ17 Explain what is the use of “EUCALYPTUS” in cloudcomputing?\\nAns.EUCALYPTUShasanopensourcesoftwareinfrastructureincloudcomputing.Itisusedto\\naddclustersinthecloudcomputingplatform.WiththehelpofEUCALYPTUSpublic,private,\\nandhybridcloudcanbebuilt.Itcanproduceitsowndatacenters.Moreover,itcanallowyouto\\nuse its functionality to many other organizations.\\nQ18Whenyouaddasoftwarestack,likeanoperatingsystemandapplicationstothe\\nservice, the model shifts to 1 / 4 model.\\nSoftwareasaservice.ThisisoftenbecauseMicrosoft’sWindowsAzurePlatformisbest\\nrepresented as presently using a SaaS model.\\nQ19 Name the foremost refined and restrictive servicemodel?\\nThemostrefinedandrestrictiveservicemodelisPaaS.Oncetheservicerequirestheconsumer\\ntouseanentirehardware/software/applicationstack,itisusingtheforemostrefinedand\\nrestrictive service model.\\nQ20 To what is, a pay-as-you-go model matches resourcesto need on an ongoing basis.\\nUtility.Thiseliminateswasteandhastheadditionaladvantageofshiftingriskfromthe\\nconsumer.\\nQ21Youutilizethefeaturepermitsyoutooptimizeyoursystemandcaptureallpossible\\ntransactions.\\nElasticity. You have the ability to modify the resourcesas required.\\nQ22 Name all the kind of virtualization is also characteristicof cloud computing?\\nStorage,Application,CPU.Tomodifythesecharacteristics,resourcesshouldbeextremely\\nconfigurable and versatile.\\nQ23 What Are Main Features Of Cloud Services?\\nSome important features of the cloud service are givenas follows:\\n• Accessing and managing the commercial software.\\n• Centralizing the activities of management of softwarein the Web environment.\\n• Developing applications that are capable of managingseveral clients.\\n•Centralizingtheupdatingfeatureofsoftwarethateliminatestheneedofdownloadingthe\\nupgrades.\\nSteve Nouri', metadata={'source': 'ML questions dump.pdf', 'page': 243}),\n",
       " Document(page_content='Q24 Which Services Are Provided By Window Azure OperatingSystem?\\nWindows Azure provides three core services which aregiven as follows:\\n• Compute\\n• Storage\\n• Management\\nQ25 What Are The Advantages Of Cloud Services?\\nSome of the advantages of cloud service are givenas follows:\\n• Helps in the utilization of investment in the corporatesector; and therefore, is cost saving.\\n•Helpsinthedevelopingscalableandrobustapplications.Previously,thescalingtookmonths,\\nbut now, scaling takes less time.\\n• Helps in saving time in terms of deployment andmaintenance.\\nQ26 Mention The Basic Components Of A Server ComputerIn Cloud Computing?\\nThecomponentsusedinlessexpensiveclientcomputersmatcheswiththehardware\\ncomponentsofservercomputerincloudcomputing.Althoughservercomputersareusuallybuilt\\nfromhigher-gradecomponentsthanclientcomputers.BasiccomponentsincludeMotherboard,\\nMemory, Processor, Network connection, Hard drives,Video, Power supply etc.\\nQ27 Explain what S3 is?\\nS3standsforSimpleStorageService.YoucanuseS3interfacetostoreandretrieveany\\namountofdata,atanytimeandfromanywhereontheweb.ForS3,thepaymentmodelis“pay\\nas you go.”\\nQ28 What is AMI?\\nAMIstandsforAmazonMachineImage.It’satemplatethatprovidestheinformation(an\\noperatingsystem,anapplicationserver,andapplications)requiredtolaunchaninstance,which\\nisacopyoftheAMIrunningasavirtualserverinthecloud.Youcanlaunchinstancesfromas\\nmany different AMIs as you need.\\nQ29 Mention what the relationship between an instanceand AMI is?\\nFromasingleAMI,youcanlaunchmultipletypesofinstances.Aninstancetypedefinesthe\\nhardwareofthehostcomputerusedforyourinstance.Eachinstancetypeprovidesdifferent\\ncomputerandmemorycapabilities.Onceyoulaunchaninstance,itlookslikeatraditionalhost,\\nand we can interact with it as we would with any computer.\\nQ30 How many buckets can you create in AWS by default?\\nBy default, you can create up to 100 buckets in eachof your AWS accounts.\\nSteve Nouri\\n', metadata={'source': 'ML questions dump.pdf', 'page': 244}),\n",
       " Document(page_content='Q31 Explain can you vertically scale an Amazon instance?How?\\nYes, you can vertically scale on Amazon instance.For that\\n●Spin up a new larger instance than the one you arecurrently running\\n●Pause that instance and detach the root webs volumefrom the server and discard\\n●Then stop your live instance and detach its root volume\\n●Note the unique device ID and attach that root volumeto your new server\\n●And start it again\\nQ32 Explain what T2 instances is?\\nT2instancesaredesignedtoprovidemoderatebaselineperformanceandthecapabilitytoburst\\nto higher performance as required by the workload.\\nQ33InVPCwithprivateandpublicsubnets,databaseserversshouldideallybelaunched\\ninto which subnet?\\nWithprivateandpublicsubnetsinVPC,databaseserversshouldideallylaunchintoprivate\\nsubnets.\\nQ34 Mention what the security best practices for AmazonEC2 are?\\nFor secure Amazon EC2 best practices, follow the followingsteps\\n●Use AWS identity and access management to controlaccess to your AWS resources\\n●Restrictaccessbyallowingonlytrustedhostsornetworkstoaccessportsonyour\\ninstance\\n●Review the rules in your security groups regularly\\n●Only open up permissions that you require\\n●Disable password-based login, for example, launchedfrom your AMI\\nQ35 Is the property of broadcast or multicast supportedby Amazon VPC?\\nNo, currently Amazon VPI not provide support for broadcastor multicast.\\nQ36 How many Elastic IPs is allows you to create byAWS?\\n5 VPC Elastic IP addresses are allowed for each AWSaccount.\\nQ37 Explain default storage class in S3\\nThe default storage class is a Standard frequentlyaccessed.\\nQ38 What are the Roles in AWS?\\nRolesareusedtoprovidepermissionstoentitieswhichyoucantrustwithinyourAWSaccount.\\nRolesareverysimilartousers.However,withroles,youdonotrequiretocreateanyusername\\nand password to work with the resources.\\nQ39 What are the edge locations?\\nEdgelocationistheareawherethecontentswillbecached.So,whenauseristryingto\\naccessing any content, the content will automaticallybe searched in the edge location.\\nSteve Nouri', metadata={'source': 'ML questions dump.pdf', 'page': 245}),\n",
       " Document(page_content='Q40 Explain snowball\\nSnowballisadatatransportoption.Itusedsourceappliancestoalargeamountofdataintoand\\noutofAWS.Withthehelpofsnowball,youcantransferamassiveamountofdatafromone\\nplace to another. It helps you to reduce networkingcosts.\\nQ41 What is a redshift?\\nRedshiftisabigdatawarehouseproduct.Itisfastandpowerful,fullymanageddatawarehouse\\nservice in the cloud.\\nQ42 What are the advantages of auto-scaling?\\nFollowing are the advantages of autoscaling\\n●Offers fault tolerance\\n●Better availability\\n●Better cost management\\nQ43 What is meant by subnet?\\nA large section of IP Address divided into chunksis known as subnets.\\nQ44 Can you establish a Peering connection to a VPCin a different region?\\nYes,wecanestablishapeeringconnectiontoaVPCinadifferentregion.Itiscalled\\ninter-region VPC peering connection.\\nQ45 What is SQS?\\nSimpleQueueServicealsoknownasSQS.Itisdistributedqueuingservicewhichactsasa\\nmediator for two controllers.\\nQ46 How many subnets can you have per VPC?\\nYou can have 200 subnets per VPC.\\nQ47 What is Amazon EMR?\\nEMRisasurvivedclusterstagewhichhelpsyoutointerprettheworkingofdatastructures\\nbeforetheintimation.ApacheHadoopandApacheSparkontheAmazonWebServiceshelps\\nyoutoinvestigatealargeamountofdata.Youcanpreparedatafortheanalyticsgoalsand\\nmarketing intellect workloads using ApacheHiveandusing other relevant open source designs.\\nQ48 What is boot time taken for the instance storedbacked AMI?\\nThe boot time for an Amazon instance store-backendAMI is less than 5 minutes.\\nQ49 Do you need an internet gateway to use peeringconnections?\\nYes, the Internet gateway is needed to use VPC (virtualprivate cloud peering) connections.\\nQ50 How to connect EBS volume to multiple instances?\\nSteve Nouri\\n', metadata={'source': 'ML questions dump.pdf', 'page': 246}),\n",
       " Document(page_content='Wecan’tbeabletoconnectEBSvolumetomultipleinstances.Although,youcanconnect\\nvarious EBS Volumes to a single instance.\\nQ51 List different types of cloud services\\nVarious types of cloud services are:\\n●Software as a Service (SaaS),\\n●Data as a Service (DaaS)\\n●Platform as a Service (PaaS)\\n●Infrastructure as a Service (IaaS).\\nQ52 What are the different types of Load Balancerin AWS services?\\nTwo types of Load balancer are:\\n1.Application Load Balancer\\n2.Classic Load Balancer\\nQ53 In which situation you will select provisionedIOPS over standard RDS storage?\\nYoushouldselectprovisionedIOPSstorageoverstandardRDSstorageifyouwanttoperform\\nbatch-related workloads.\\nQ54 What are the important features of Amazon cloudsearch?\\nImportant features of the Amazon cloud are:\\n●Boolean searches\\n●Prefix Searches\\n●Range searches\\n●Entire text search\\n●AutoComplete advice\\nQ55 What are the various components of the GoogleCloud Platform?\\nGoogleCloudPlatform(GCP)iscomposedofasetofelementsthathelpspeopleindifferent\\nways. The various GCP elements are\\n●Google Compute Engine\\n●Google Cloud Container Engine\\n●Google Cloud App Engine\\n●Google Cloud Storage\\n●Google Cloud Dataflow\\n●Google BigQuery Service\\n●Google Cloud Job Discovery\\n●Google Cloud Endpoints\\n●Google Cloud Test Lab\\n●Google Cloud Machine Learning Engine\\nSteve Nouri', metadata={'source': 'ML questions dump.pdf', 'page': 247}),\n",
       " Document(page_content='Q56 What are the main advantages of using Google CloudPlatform?\\nGoogleCloudPlatformisamediumthatprovidesitsusersaccesstothebestcloudservices\\nandfeatures.Itisgainingpopularityamongthecloudprofessionalsaswellasusersforthe\\nadvantages if offer.\\nHere are the main advantages of using Google CloudPlatform over others –\\n●GCP offers much better pricing deals as compared tothe other cloud service providers\\n●GoogleCloudserversallowyoutoworkfromanywheretohaveaccesstoyour\\ninformation and data.\\n●Consideringhostingcloudservices,GCPhasanoverallincreasedperformanceand\\nservice\\n●GoogleCloudisveryfastinprovidingupdatesaboutserverandsecurityinabetterand\\nmore efficient manner\\n●ThesecuritylevelofGoogleCloudPlatformisexemplary;thecloudplatformand\\nnetworks are secured and encrypted with various securitymeasures.\\nIfyouaregoingfortheGoogleCloudinterview,youshouldprepareyourselfwithenough\\nknowledgeofGoogleCloudPlatform.TheadvantagesofGCPisamongfrequentlyasked\\nGoogle Cloud interview questions, so you need to beprepared to answer it.\\nQ57 Why should you opt to Google Cloud Hosting?\\nAnswer:ThereasonforoptingGoogleCloudHostingistheadvantagesitoffers.Herearethe\\nadvantages of choosing Google Cloud Hosting:\\n●Availability of better pricing plans\\n●Benefits of live migration of the machines\\n●Enhanced performance and execution\\n●Commitment to Constant development and expansion\\n●The private network provides efficiency and maximumtime\\n●Strong control and security of the cloud platform\\n●Inbuilt redundant backups ensure data integrity andreliability\\nTheinterviewermayaskthisquestiontocheckyourknowledgeandexplanationskillsabout\\nGoogleCloud.ThistypeofquestionsarebasicallycategorizedundertheGoogleCloud\\nconsultant interview questions and may be asked inthe Google Cloud interview.\\nQ58 What are the libraries and tools for cloud storageon GCP?\\nAnswer:Atthecorelevel,XMLAPIandJSONAPIarethereforthecloudstorageonGoogle\\nCloudPlatform.Butalongwiththese,therearefollowingoptionsprovidedbyGoogletointeract\\nwith the cloud storage.\\n●GoogleCloudPlatformConsole,whichperformsbasicoperationsonobjectsand\\nbuckets\\n●CloudStorageClientLibraries,whichprovideprogrammingsupportforvarious\\nlanguages including Java, Ruby, and Python\\n●GustilCommand-line Tool, which provides a commandline interface for the cloud storage\\nSteve Nouri\\n', metadata={'source': 'ML questions dump.pdf', 'page': 248}),\n",
       " Document(page_content='TherearemanythirdpartylibrariesandtoolssuchasBotoLibrary.Thisisthetechnical\\nquestionthatyoumaycomeacrossifyouaregoingfortheGoogleCloudEngineerinterview.\\nYou need to prepare yourself with the basic knowledgeof GCP tools and libraries.\\nQ59 What do you know about Google Compute Engine?\\nGoogleCloudEngineisthebasiccomponentoftheGoogleCloudPlatform.So,itbecomesa\\ncommonquestionthatliesundertheGoogleCloudEngineerinterviewquestionsaswellas\\nGoogle Cloud Architect interview questions.\\nGoogleComputeEngineisanIaaSproductthatoffersself-managedandflexiblevirtual\\nmachinesthatarehostedontheinfrastructureofGoogle.ItincludesWindowsandLinuxbased\\nvirtual machines that may run on local, KVM, and durablestorage options.\\nItalsoincludesREST-basedAPIforthecontrolandconfigurationpurposes.GoogleCompute\\nEngineintegrateswithGCPtechnologiessuchasGoogleAppEngine,GoogleCloudStorage,\\nandGoogleBigQueryinordertoextenditscomputationalabilityandthuscreatesmore\\nsophisticated and complex applications.\\nQ60 How are the Google Compute Engine and Google AppEngine related?\\nThistypicalandstraightforwardquestionisapartofthefrequentlyaskedGoogleCloudPlatform\\ninterviewquestionsandanswers,andcanbeansweredlikethis.GoogleComputeEngineand\\nGoogleAppEnginearecomplementarytoeachother.GoogleComputeEngineistheIaaS\\nproduct whereas Google App Engine is a PaaS productof Google.\\nGoogleAppEngineisgenerallyusedtorunweb-basedapplications,mobilebackends,andline\\nofbusiness.Ifyouwanttokeeptheunderlyinginfrastructureinmoreofyourcontrol,then\\nComputeEngineisaperfectchoice.Forinstance,youcanuseComputeEngineforthe\\nimplementationofcustomizedbusinesslogicorincase,youneedtorunyourownstorage\\nsystem.\\nQ61 How does the pricing model work in GCP cloud?\\nAnswer:WhileworkingonGoogleCloudPlatform,theuserischargedonthebasisofcompute\\ninstance,networkuse,andstoragebyGoogleComputeEngine.GoogleCloudchargesvirtual\\nmachinesonthebasisofpersecondwiththelimitofminimumof1minute.Then,thecostof\\nstorage is charged on the basis of the amount of datathat you store.\\nThecostofthenetworkiscalculatedaspertheamountofdatathathasbeentransferred\\nbetweenthevirtualmachineinstancescommunicatingwitheachotheroverthenetwork.You\\nshouldprepareyourselfwiththequestionsonGoogleCloudPlatformpricingmodelsasthese\\nare among the most common Google Cloud interview questions.\\nQ62WhatarethedifferentmethodsfortheauthenticationofGoogleComputeEngine\\nAPI?\\nAnswer:ThisisoneofthepopularGoogleCloudarchitectinterviewquestionswhichcanbe\\nansweredasfollows.TherearedifferentmethodsfortheauthenticationofGoogleCompute\\nEngine API:\\n●Using OAuth 2.0\\n●Through client library\\nSteve Nouri', metadata={'source': 'ML questions dump.pdf', 'page': 249}),\n",
       " Document(page_content=\"●Directly with an access token\\nQ63 List some Database services by GCP.\\nTherearemanyGoogleclouddatabaseserviceswhichhelpsmanyenterprisestomanagetheir\\ndata.\\n●BareMetalSolutionisarelationaldatabasetypeandallowtomigrateorliftandshift\\nspecialized workloads to Google cloud.\\n●CloudSQLisafullymanaged,reliableandintegratedrelationaldatabaseservicesfor\\nMySQL,MSSQLServerandPostgreSQLknownasPostgres.Itreducemaintenance\\ncost and ensure business continuity.\\n●Cloud Spanner\\n●Cloud Bigtable\\n●Firestore\\n●Firebase Realtime Database\\n●Memorystore\\n●Google Cloud Partner Services\\n●For more database products you can referGoogle CloudDatabases\\n●For more data base solutions you can referGooglecloud Database solutions\\nQ64 What are the Google Cloud storage services?\\nGoogleCloudstorageprovidesmanyservicesforyourbusinesswhicharerunningonGoogle\\nCloud's infrastructure.\\n●Cloud Storage (Object storage)\\n●Persistent disk\\n●Local SSD\\n●Cloud Storage (Archival storage)\\n●Filestore\\n●Data Transfer Services\\n●Transfer Appliance\\n●Cloud Storage for Firebase\\n●Google Workspace\\n●For more Google cloud storage referGoogle Cloud Storage\\nQ65 What are the different Network services by GCP?\\nGoogleCloudprovidesmanyNetworkingservicesandtechnologiesthatmakeeasytoscale\\nand manage your network.\\n●Hybrid connectivity helps to connect your infrastructureto Google Cloud\\n●Virtual Private Cloud (VPC) manage networking foryour resources\\n●Cloud DNS is a highly available global domain namingsystem (DNS) network.\\n●Service Directory provides a service-centric networksolution.\\nSteve Nouri\\n\", metadata={'source': 'ML questions dump.pdf', 'page': 250}),\n",
       " Document(page_content=\"●Cloud Load Balancing\\n●Cloud CDN\\n●Cloud Armor\\n●Cloud NAT\\n●Network Telemetry\\n●VPC Service Controls\\n●Network Intelligence Center\\n●Network Service Tiers\\n●For more about Networking products referGoogle CloudNetworking\\nQ66 List some Data Analytics service by GCP.\\nGoogle Cloud offers various Data Analytics services.\\n●BigQueryisanmulti-clouddatawarehouseforbusinessagilitythatishighscalable,\\nserverless, and cost effective.\\n●Looker\\n●DataProcisaserviceforrunningApaceSparkandApaceHadoopClusters.Itmakes\\nopen-source data and analytics processing easy, fastand more secure inCloud.\\n●Dataflow\\n●Pub/Sub\\n●Cloud Data Fusion\\n●Data Catalog\\n●Cloud Composer\\n●Google Data Studio\\n●Dataprep\\n●CloudLifeSciencesenableslifesciencescommunitytomanage,processandtransform\\nbiomedical data at scale.\\n●GoogleMarketingPlatformisamarketingplatformthatcombinesyouradvertisingand\\nanalyticstohelpyoumakebettermarketingresults,deeperinsightsandquality\\ncustomerconnections.It'snotanGoogleofficialcloudproduct,comesunderseparate\\nterms of services.\\n●For Google Cloud analytics services visitData Analytics\\nQ67 Explain Google BigQuery in Google Cloud Platform\\nFortraditionaldatawarehouse,hardwaresetupreplacementisrequired.Insuchcase,Google\\nBigQueryservestobethereplacement.Inaddition,BigQueryhelpsinorganizingthetabledata\\ninto unit called as datasets.\\nQ68 Explain Auto-scaling in Google cloud computing\\nWithouthumanintervention,youcanmechanicallyprovisionandinitiatenewinstancesinAWS.\\nDepending on various metrics and load, Auto-scalingis triggered.\\nSteve Nouri\\n\", metadata={'source': 'ML questions dump.pdf', 'page': 251}),\n",
       " Document(page_content='Q69 Describe Hypervisor in Google Cloud Platform\\nHypervisorisotherwisecalledasVMM(VirtualMachineMonitor).Hypervisorissaidtobea\\ncomputerhardware/softwareusedtocreateandrunvirtualmachines(virtualmachinesisalso\\ncalled as Guest machine). Hypervisor is the one thatruns on a host machine.\\nQ70 Define VPC in the Google cloud platform\\nVPCisGooglecloudplatformishelpfulisprovidingconnectivityfromthepremiseandtoanyof\\ntheregionwithoutinternet.VPCConnectivityisforcomputingAppEngineFlexinstances,\\nKubernetesEngineclusters,virtualmachineinstanceandfewotherresourcesdependingonthe\\nprojects. Multiple VPC can also be used in numerousprojects.\\nQ71. Which service in Azure is used to manage resourcesin Azure?\\nAzureResourceManagerisusedto“manage”infrastructureswhichinvolveano.ofazure\\nservices.Itcanbeusedtodeploy,manageanddeletealltheresourcestogetherusingasimple\\nJSON script.\\nQ72 Which of the following web applications can bedeployed with Azure?\\nMicrosoftalsohasreleasedSDKsforbothJavaandRubytoallowapplicationswritteninthose\\nlanguages to place calls to the Azure Service PlatformAPI to the AppFabric Service.\\nQ73 What are Roles in Azure and why do we use them?\\nRolesarenothingserversinlaymanterms.Theseserversaremanaged,loadbalanced,\\nPlatform as a Service virtual machines that work togetherto achieve a common goal.\\nThere are 3 types of roles in Microsoft Azure:\\n●Web Role\\n●Worker Role\\n●VM  Role\\nLet’s discuss each of these roles in detail:\\n●WebRole–Awebroleisbasicallyusedtodeployawebsite,usinglanguagessupported\\nbytheIISplatformlike,PHP,.NETetc.Itisconfiguredandcustomizedtorunweb\\napplications.\\n●WorkerRole–AworkerroleismorelikeanhelptotheWebrole,itusedtoexecute\\nbackground processes unlike the Web Role which isused to deploy the website.\\n●VMRole–TheVMroleisusedbyausertoscheduletasksandotherwindowsservices.\\nThisrolecanbeusedtocustomizethemachinesonwhichthewebandworkerroleis\\nrunning.\\nQ74 What is Azure as PaaS?\\nPaaSisacomputingplatformthatincludesanoperatingsystem,programminglanguage\\nexecutionenvironment,database,orwebservices.Developersandapplicationprovidersuse\\nthis type of Azure services.\\nSteve Nouri', metadata={'source': 'ML questions dump.pdf', 'page': 252}),\n",
       " Document(page_content='Q75 What are Break-fix issues in Microsoft Azure?\\nIn,MicrosoftAzure,allthetechnicalproblemiscalledbreak-fixissues.Thistermuseswhen\\n\"work involved in support a technology when it failsin the normal course of its function.\\nQ76 Explain Diagnostics in Windows Azure\\nWindowsAzureDiagnosticoffersthefacilitytostorediagnosticdata.InAzure,somediagnostics\\ndataisstoredinthetable,whilesomearestoredinablob.Thediagnosticmonitorrunsin\\nWindows Azure as well as in the computer\\'s emulatorfor collecting data for a role instance.\\nQ77 State the difference between repetitive and minimalmonitoring.\\nVerbosemonitoringcollectsmetricsbasedonperformance.Itallowsacloseanalysisofdatafed\\nduring the process of application.\\nOntheotherhand,minimalmonitoringisadefaultconfigurationmethod.Itmakestheuserof\\nperformance counters gathered from the operating systemof the host.\\nQ78 What is the main difference between the repositoryand the powerhouse server?\\nThemaindifferencebetweenthemisthatrepositoryserversareinsteadoftheintegrity,\\nconsistency,anduniformitywhilepowerhouseservergovernstheintegrationofdifferentaspects\\nof the database repository.\\nQ79 Explain command task in Microsoft Azure\\nCommandtaskisanoperationalwindowwhichsetofftheflowofeithersingleormultiple\\ncommon whiles when the system is running.\\nQ80 What is the difference between Azure Service BusQueues and Storage Queues?\\nTwotypesofqueuemechanismsaresupportedbyAzure:StoragequeuesandServiceBus\\nqueues.\\nStoragequeues:ThesearethepartoftheAzurestorageinfrastructure,featuresasimple\\nREST-basedGET/PUT/PEEKinterface.Providespersistentandreliablemessagingwithinand\\nbetween services.\\nServiceBusqueues:ThesearethepartofabroaderAzuremessaginginfrastructurethathelps\\nto queue as well as publish/subscribe, and more advancedintegration patterns.\\nQ81 Explain Azure Service Fabric.\\nAzureServiceFabricisadistributedplatformdesignedbyMicrosofttofacilitatethe\\ndevelopment,deploymentandmanagementofhighlyscalableandcustomizableapplications.\\nTheapplicationscreatedinthisenvironmentconsistsofdetachedmicroservicesthat\\ncommunicate with each other through service applicationprogramming interfaces.\\nQ82 Define the Azure Redis Cache.\\nAzureRedisCacheisanopen-sourceandin-memoryRediscachethathelpswebapplications\\ntofetchdatafromabackenddatasourceintocacheandserverwebpagesfromthecacheto\\nenhancetheapplicationperformance.Itprovidesapowerfulandsecurewaytocachethe\\napplication’s data in the Azure cloud.\\nSteve Nouri', metadata={'source': 'ML questions dump.pdf', 'page': 253}),\n",
       " Document(page_content='Q83HowmanyinstancesofaRoleshouldbedeployedtosatisfyAzureSLA(service\\nlevel agreement)? And what’s the benefit of AzureSLA?\\nTWO. And if we do so, the role would have externalconnectivity at least 99.95% of the time.\\nQ84 What are the options to manage session state inWindows Azure?\\n●Windows Azure Caching\\n●SQL Azure\\n●Azure Table\\nQ85 What is cspack?\\nItisacommand-linetoolthatgeneratesaservicepackagefile(.cspkg)andpreparesan\\napplication for deployment, either to Windows Azureor to the compute emulator.\\nQ86 What is csrun?\\nItisacommand-linetoolthatdeploysapackagedapplicationtotheWindowsAzurecompute\\nemulator and manages the running service.\\nQ87 How to design applications to handle connectionfailure in Windows Azure?\\nTheTransientFaultHandlingApplicationBlocksupportsvariousstandardwaysofgenerating\\ntheretrydelaytimeinterval,includingfixedinterval,incrementalinterval(theintervalincreases\\nbyastandardamount),andexponentialback-off(theintervaldoubleswithsomerandom\\nvariation).\\nQ88 What is Windows Azure Diagnostics?\\nWindowsAzureDiagnosticsenablesyoutocollectdiagnosticdatafromanapplicationrunning\\ninWindowsAzure.Youcanusediagnosticdatafordebuggingandtroubleshooting,measuring\\nperformance, monitoring resource usage, traffic analysisand capacity planning, and auditing.\\nQ89WhatisthedifferencebetweenWindowsAzureQueuesandWindowsAzureService\\nBus Queues?\\nWindowsAzuresupportstwotypesofqueuemechanisms:WindowsAzureQueuesandService\\nBusQueues.\\nWindowsAzureQueues,whicharepartoftheWindowsAzurestorageinfrastructure,featurea\\nsimpleREST-basedGet/Put/Peekinterface,providingreliable,persistentmessagingwithinand\\nbetween services.\\nServiceBusQueuesarepartofabroaderWindowsAzuremessaginginfrastructuredead-letters\\nqueuing as well as publish/subscribe, Web serviceremoting, and integration patterns.\\nQ90 What is the use of Azure Active Directory?\\nAzureActiveDirectoryisanidentifyandaccessmanagementsystem.Itisverymuchsimilarto\\ntheactivedirectories.Itallowsyoutograntyouremployeeinaccessingspecificproductsand\\nservices within the network.\\nSteve Nouri\\n', metadata={'source': 'ML questions dump.pdf', 'page': 254}),\n",
       " Document(page_content='Q91IsitpossibletocreateaVirtualMachineusingAzureResourceManagerinaVirtual\\nNetwork that was created using classic deployment?\\nThisisnotsupported.YoucannotuseAzureResourceManagertodeployavirtualmachineinto\\na virtual network that was created using classic deployment.\\nQ92 What are virtual machine scale sets in Azure?\\nExplanation:VirtualmachinescalesetsareAzurecomputeresourcethatyoucanusetodeploy\\nandmanageasetofidenticalVMs.WithalltheVMsconfiguredthesame,scalesetsare\\ndesignedtosupporttrueautoscale,andnopre-provisioningofVMsisrequired.Soit’seasierto\\nbuild large-scale services that target big compute,big data, and containerized workloads.\\nQ93 Are data disks supported within scale sets?\\nExplanation:Yes.Ascalesetcandefineanattacheddatadiskconfigurationthatappliestoall\\nVMs in the set. Other options for storing data include:\\n●Azure files (SMB shared drives)\\n●OS drive\\n●Temp drive (local, not backed by Azure Storage)\\n●Azure data service (for example, Azure tables, Azureblobs)\\n●External data service (for example, remote database)\\nQ95 What is the difference between the Windows AzurePlatform and Windows Azure?\\nTheformerisMicrosoft’sPaaSofferingincludingWindowsAzure,SQLAzure,andAppFabric;\\nwhile the latter is part of the offering and Microsoft’scloud OS.\\nQ96 What are the three main components of the WindowsAzure Platform?\\nCompute, Storage and AppFabric.\\nQ97 Can you move a resource from one group to another?\\nYes, you can. A resource can be moved among resourcegroups.\\nQ98 How many resource groups a subscription can have?\\nAsubscriptioncanhaveupto800resourcegroups.Also,aresourcegroupcanhaveupto800\\nresources of the same type and up to 15 tags.\\nQ99 Explain the fault domain.\\nAnswer:ThisisoneofthecommonAzureinterviewquestionswhichshouldbeansweredthatit\\nisalogicalworkingdomaininwhichtheunderlyinghardwareissharingacommonpower\\nsourceandswitchnetwork.ThismeansthatwhenVMsiscreatedtheAzuredistributestheVM\\nacrossthefaultdomainthatlimitsthepotentialimpactofhardwarefailure,powerinterruptionor\\noutages of the network.\\nQ100 Differentiate between the repository and thepowerhouse server?\\nAnswer:Repositoryserversarethosewhichareinlieuoftheintegrity,consistency,and\\nuniformitywhereasthepowerhouseservergovernstheintegrationofdifferentaspectsofthe\\ndatabase repository.\\nSteve Nouri', metadata={'source': 'ML questions dump.pdf', 'page': 255}),\n",
       " Document(page_content='800+ Data Science Q&A \\nOrgnized by Steve Nouri\\nSeptemeber 2021\\nwww.Hackmakers.com\\nwww.AI4Diversity.org\\nA I 4 D i v e r s i t y', metadata={'source': 'ML questions dump.pdf', 'page': 256})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8c3b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\devsh\\anaconda3\\lib\\site-packages (1.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c165a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\vectorstores\\faiss.py:47\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[1;34m(no_avx2)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[0;32m      4\u001b[0m instructor_embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceInstructEmbeddings()\n\u001b[1;32m----> 5\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstructor_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\vectorstores\\base.py:417\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    415\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    416\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\vectorstores\\faiss.py:603\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \n\u001b[0;32m    586\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    602\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m--> 603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m    604\u001b[0m     texts,\n\u001b[0;32m    605\u001b[0m     embeddings,\n\u001b[0;32m    606\u001b[0m     embedding,\n\u001b[0;32m    607\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    608\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\vectorstores\\faiss.py:557\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__from\u001b[39m(\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    556\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m--> 557\u001b[0m     faiss \u001b[38;5;241m=\u001b[39m \u001b[43mdependable_faiss_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance_strategy \u001b[38;5;241m==\u001b[39m DistanceStrategy\u001b[38;5;241m.\u001b[39mMAX_INNER_PRODUCT:\n\u001b[0;32m    559\u001b[0m         index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\vectorstores\\faiss.py:49\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[1;34m(no_avx2)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import faiss python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m faiss\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings()\n",
    "vectordb = FAISS.from_documents(documents=data, embedding=instructor_embeddings)\n",
    "\n",
    "# embeddings = HuggingFaceInstructEmbeddings(\n",
    "#     query_instruction=\"Represent the query for retrieval: \"\n",
    "# )\n",
    "# e = embeddings.embed_query(\"What is gradient descent?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca49277",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "rdocs = retriever.get_relevant_documents(\"What is gradient boosting?\")\n",
    "rdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6102a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951c30f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "e[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9a63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584f7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
